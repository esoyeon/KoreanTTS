{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kss+유인나_batch_8+16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqacD7svMaa4"
      },
      "source": [
        "## Environment Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iA4VSJ_MwnU",
        "outputId": "014eb272-69b0-4c40-c358-fc3f149cece9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "# !pip uninstall tensorflow\n",
        "# !pip install tensorflow==1.12.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
            "Collecting tensorflow==1.12.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/3f/d4ece3c10789214ff5633c038ee10152bb4daaaf68d402f1cd1942b4549c/tensorflow-1.12.2-cp36-cp36m-manylinux1_x86_64.whl (83.1MB)\n",
            "\u001b[K     |████████████████████████████████| 83.1MB 34kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.2) (0.3.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.2) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.2) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.2) (0.35.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.2) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.2) (0.10.0)\n",
            "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.2) (1.12.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.2) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.2) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.2) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.2) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12.2) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12.2) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.2) (3.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12.2) (50.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.2) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.2) (3.2.0)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-1.12.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSVY76zZi1yF",
        "outputId": "e4aa7c3a-c62c-4b5e-d712-db3db081526d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "!apt -qq -y install fonts-nanum\n",
        "import matplotlib.font_manager as fm\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=9)\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "mpl.font_manager._rebuild()\n",
        "\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 11 not upgraded.\n",
            "Need to get 9,604 kB of archives.\n",
            "After this operation, 29.5 MB of additional disk space will be used.\n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 144628 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20170925-1_all.deb ...\n",
            "Unpacking fonts-nanum (20170925-1) ...\n",
            "Setting up fonts-nanum (20170925-1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fonts-nanum is already the newest version (20170925-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 11 not upgraded.\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQjJxzaJM9x0",
        "outputId": "813b745f-507e-45c4-d2f4-75ad6edb2cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install jamo\n",
        "!pip install unidecode"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jamo\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/cc/49812faae67f9a24be6ddaf58a2cf7e8c3cbfcf5b762d9414f7103d2ea2c/jamo-0.4.1-py3-none-any.whl\n",
            "Installing collected packages: jamo\n",
            "Successfully installed jamo-0.4.1\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 9.5MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPz2twU0k9Vn",
        "outputId": "449f200e-482c-4e8b-83e1-a8305fc3d056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# tensorflow 1.xx 에서 gpu 사용\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow-gpu==1.12.2\n",
        "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda=9.0.176-1\n",
        "\n",
        "!nvcc --version"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.3.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.3.0\n",
            "Collecting tensorflow-gpu==1.12.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/24/52fc6ba729b53ebdb9b4db45e1abae0cd80c93929937c3bcc1435b359732/tensorflow_gpu-1.12.2-cp36-cp36m-manylinux1_x86_64.whl (127.8MB)\n",
            "\u001b[K     |████████████████████████████████| 127.8MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.2) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.2) (1.33.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.2) (1.18.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.2) (0.8.1)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.2) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.2) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.2) (0.35.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.2) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.2) (0.10.0)\n",
            "Collecting tensorboard<1.13.0,>=1.12.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.12.2) (3.12.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.12.2) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow-gpu==1.12.2) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow-gpu==1.12.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.12.2) (50.3.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow-gpu==1.12.2) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow-gpu==1.12.2) (3.4.0)\n",
            "Installing collected packages: keras-applications, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.12.2 tensorflow-gpu-1.12.2\n",
            "--2020-11-05 16:13:42--  https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 152.199.16.29\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|152.199.16.29|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb?DaFJTJUFl1T4bZY1N_Uu28Q8nLVKWhzyURf3dJxdtGId9ZsiAGElZfT2bB3whM879VRa3d6Wln63NyolHeDHci8CEbKDITBnwVPFWUfKX8ih6-0oao-R3Y94OZlPl-LGF0dxR8CfRU25g5o8Pn26yuiUDW4soCS3XkxuzzYGJllIH87ICbV-XazchngZpd5p_qOHjh4Dlz69ypPIfzBZ [following]\n",
            "--2020-11-05 16:13:43--  https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb?DaFJTJUFl1T4bZY1N_Uu28Q8nLVKWhzyURf3dJxdtGId9ZsiAGElZfT2bB3whM879VRa3d6Wln63NyolHeDHci8CEbKDITBnwVPFWUfKX8ih6-0oao-R3Y94OZlPl-LGF0dxR8CfRU25g5o8Pn26yuiUDW4soCS3XkxuzzYGJllIH87ICbV-XazchngZpd5p_qOHjh4Dlz69ypPIfzBZ\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.20.126\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.20.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1212738714 (1.1G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.13G   249MB/s    in 4.6s    \n",
            "\n",
            "2020-11-05 16:13:48 (252 MB/s) - ‘cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb’ saved [1212738714/1212738714]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu1604-9-0-local.\n",
            "(Reading database ... 144646 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb ...\n",
            "Unpacking cuda-repo-ubuntu1604-9-0-local (9.0.176-1) ...\n",
            "Setting up cuda-repo-ubuntu1604-9-0-local (9.0.176-1) ...\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-9-0-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-9-0-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-9-0-local  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-9-0-local  Release [574 B]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  Release.gpg [819 B]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  Release.gpg [819 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:11 file:/var/cuda-repo-9-0-local  Packages [15.4 kB]\n",
            "Ign:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:13 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,687 kB]\n",
            "Ign:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:16 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [864 kB]\n",
            "Get:19 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [40.1 kB]\n",
            "Get:20 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [48.9 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,167 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,118 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,353 kB]\n",
            "Get:25 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,750 kB]\n",
            "Ign:26 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:26 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [405 kB]\n",
            "Fetched 10.7 MB in 2s (4,371 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cuda-9-0 cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0\n",
            "  cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "The following NEW packages will be installed:\n",
            "  cuda cuda-9-0 cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0\n",
            "  cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "0 upgraded, 34 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 0 B/1,097 MB of archives.\n",
            "After this operation, 2,315 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-9-0-local  cuda-license-9-0 9.0.176-1 [22.0 kB]\n",
            "Get:2 file:/var/cuda-repo-9-0-local  cuda-misc-headers-9-0 9.0.176-1 [684 kB]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  cuda-core-9-0 9.0.176-1 [16.9 MB]\n",
            "Get:4 file:/var/cuda-repo-9-0-local  cuda-cudart-9-0 9.0.176-1 [106 kB]\n",
            "Get:5 file:/var/cuda-repo-9-0-local  cuda-driver-dev-9-0 9.0.176-1 [10.9 kB]\n",
            "Get:6 file:/var/cuda-repo-9-0-local  cuda-cudart-dev-9-0 9.0.176-1 [767 kB]\n",
            "Get:7 file:/var/cuda-repo-9-0-local  cuda-command-line-tools-9-0 9.0.176-1 [25.4 MB]\n",
            "Get:8 file:/var/cuda-repo-9-0-local  cuda-nvrtc-9-0 9.0.176-1 [6,348 kB]\n",
            "Get:9 file:/var/cuda-repo-9-0-local  cuda-nvrtc-dev-9-0 9.0.176-1 [9,334 B]\n",
            "Get:10 file:/var/cuda-repo-9-0-local  cuda-cusolver-9-0 9.0.176-1 [26.2 MB]\n",
            "Get:11 file:/var/cuda-repo-9-0-local  cuda-cusolver-dev-9-0 9.0.176-1 [5,317 kB]\n",
            "Get:12 file:/var/cuda-repo-9-0-local  cuda-cublas-9-0 9.0.176-1 [25.0 MB]\n",
            "Get:13 file:/var/cuda-repo-9-0-local  cuda-cublas-dev-9-0 9.0.176-1 [49.4 MB]\n",
            "Get:14 file:/var/cuda-repo-9-0-local  cuda-cufft-9-0 9.0.176-1 [84.1 MB]\n",
            "Get:15 file:/var/cuda-repo-9-0-local  cuda-cufft-dev-9-0 9.0.176-1 [73.7 MB]\n",
            "Get:16 file:/var/cuda-repo-9-0-local  cuda-curand-9-0 9.0.176-1 [38.8 MB]\n",
            "Get:17 file:/var/cuda-repo-9-0-local  cuda-curand-dev-9-0 9.0.176-1 [57.9 MB]\n",
            "Get:18 file:/var/cuda-repo-9-0-local  cuda-cusparse-9-0 9.0.176-1 [25.2 MB]\n",
            "Get:19 file:/var/cuda-repo-9-0-local  cuda-cusparse-dev-9-0 9.0.176-1 [25.3 MB]\n",
            "Get:20 file:/var/cuda-repo-9-0-local  cuda-npp-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:21 file:/var/cuda-repo-9-0-local  cuda-npp-dev-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:22 file:/var/cuda-repo-9-0-local  cuda-nvgraph-9-0 9.0.176-1 [6,081 kB]\n",
            "Get:23 file:/var/cuda-repo-9-0-local  cuda-nvgraph-dev-9-0 9.0.176-1 [5,658 kB]\n",
            "Get:24 file:/var/cuda-repo-9-0-local  cuda-samples-9-0 9.0.176-1 [75.9 MB]\n",
            "Get:25 file:/var/cuda-repo-9-0-local  cuda-documentation-9-0 9.0.176-1 [53.1 MB]\n",
            "Get:26 file:/var/cuda-repo-9-0-local  cuda-libraries-dev-9-0 9.0.176-1 [2,596 B]\n",
            "Get:27 file:/var/cuda-repo-9-0-local  cuda-nvml-dev-9-0 9.0.176-1 [47.6 kB]\n",
            "Get:28 file:/var/cuda-repo-9-0-local  cuda-visual-tools-9-0 9.0.176-1 [398 MB]\n",
            "Get:29 file:/var/cuda-repo-9-0-local  cuda-toolkit-9-0 9.0.176-1 [2,836 B]\n",
            "Get:30 file:/var/cuda-repo-9-0-local  cuda-libraries-9-0 9.0.176-1 [2,566 B]\n",
            "Get:31 file:/var/cuda-repo-9-0-local  cuda-runtime-9-0 9.0.176-1 [2,526 B]\n",
            "Get:32 file:/var/cuda-repo-9-0-local  cuda-demo-suite-9-0 9.0.176-1 [3,880 kB]\n",
            "Get:33 file:/var/cuda-repo-9-0-local  cuda-9-0 9.0.176-1 [2,552 B]\n",
            "Get:34 file:/var/cuda-repo-9-0-local  cuda 9.0.176-1 [2,504 B]\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package cuda-license-9-0.\n",
            "(Reading database ... 144705 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-license-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-license-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-9-0.\n",
            "Preparing to unpack .../01-cuda-misc-headers-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-core-9-0.\n",
            "Preparing to unpack .../02-cuda-core-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-core-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-9-0.\n",
            "Preparing to unpack .../03-cuda-cudart-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-9-0.\n",
            "Preparing to unpack .../04-cuda-driver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-9-0.\n",
            "Preparing to unpack .../05-cuda-cudart-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-9-0.\n",
            "Preparing to unpack .../06-cuda-command-line-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-9-0.\n",
            "Preparing to unpack .../07-cuda-nvrtc-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-9-0.\n",
            "Preparing to unpack .../08-cuda-nvrtc-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-9-0.\n",
            "Preparing to unpack .../09-cuda-cusolver-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-9-0.\n",
            "Preparing to unpack .../10-cuda-cusolver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-9-0.\n",
            "Preparing to unpack .../11-cuda-cublas-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-dev-9-0.\n",
            "Preparing to unpack .../12-cuda-cublas-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-9-0.\n",
            "Preparing to unpack .../13-cuda-cufft-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-9-0.\n",
            "Preparing to unpack .../14-cuda-cufft-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-9-0.\n",
            "Preparing to unpack .../15-cuda-curand-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-9-0.\n",
            "Preparing to unpack .../16-cuda-curand-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-9-0.\n",
            "Preparing to unpack .../17-cuda-cusparse-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-9-0.\n",
            "Preparing to unpack .../18-cuda-cusparse-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-9-0.\n",
            "Preparing to unpack .../19-cuda-npp-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-9-0.\n",
            "Preparing to unpack .../20-cuda-npp-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-9-0.\n",
            "Preparing to unpack .../21-cuda-nvgraph-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-9-0.\n",
            "Preparing to unpack .../22-cuda-nvgraph-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-samples-9-0.\n",
            "Preparing to unpack .../23-cuda-samples-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-samples-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-documentation-9-0.\n",
            "Preparing to unpack .../24-cuda-documentation-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-9-0.\n",
            "Preparing to unpack .../25-cuda-libraries-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-9-0.\n",
            "Preparing to unpack .../26-cuda-nvml-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-9-0.\n",
            "Preparing to unpack .../27-cuda-visual-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-9-0.\n",
            "Preparing to unpack .../28-cuda-toolkit-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-9-0.\n",
            "Preparing to unpack .../29-cuda-libraries-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-runtime-9-0.\n",
            "Preparing to unpack .../30-cuda-runtime-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-9-0.\n",
            "Preparing to unpack .../31-cuda-demo-suite-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-9-0.\n",
            "Preparing to unpack .../32-cuda-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda.\n",
            "Preparing to unpack .../33-cuda_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda (9.0.176-1) ...\n",
            "Setting up cuda-license-9-0 (9.0.176-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-9.0/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cublas-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-core-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cublas-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-samples-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-9-0 (9.0.176-1) ...\n",
            "Setting up cuda (9.0.176-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2017 NVIDIA Corporation\n",
            "Built on Fri_Sep__1_21:08:03_CDT_2017\n",
            "Cuda compilation tools, release 9.0, V9.0.176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVYM3NlXM5Zh",
        "outputId": "9bb6ff1b-fa18-4e42-a8eb-fb133d886b49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "# tensorflow 버전 확인\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.12.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCw3Lsb_JbjL"
      },
      "source": [
        "# For TF 1.X\n",
        "# limit gpu memory growth\n",
        "\n",
        "config = tf.ConfigProto() \n",
        "config.gpu_options.allow_growth = True \n",
        "session = tf.Session(config=config)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpUJWY0F3V2d",
        "outputId": "0bb3f066-a924-41bd-bcc6-470bd7f9b8da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 사양 확인\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 10180150049327702875, name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 15825695308988743746\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 1986031717649069388\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15672259380\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 12666885327594432223\n",
              " physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvREKPyiM_t4",
        "outputId": "02ddf578-7cfd-4ac4-bb2e-e3cb8e211a57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8js-FzSbNFyx"
      },
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "os.chdir('/content/drive/My Drive/stt플젝')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTPMaaCoSeEa",
        "outputId": "5433d82d-bb47-4f24-c16d-ad6bd143389b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 -c \"import nltk; nltk.download('punkt')\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4se9gUwOpqs",
        "outputId": "ccd52932-c0ce-45b3-8918-31534df87f13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2vz-RrwNNAU",
        "outputId": "47333bec-f639-4ed9-fe40-e04a903e6f05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python ./Tacotron2-Wavenet-Korean-TTS/preprocess.py --num_workers 10 --name kss --in_dir ./datasets/kss --out_dir ./data/kss"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Hyperparameters:\n",
            "  adam_beta1: 0.9\n",
            "  adam_beta2: 0.999\n",
            "  allow_clipping_in_normalization: True\n",
            "  attention_dim: 128\n",
            "  attention_filters: 32\n",
            "  attention_kernel: (31,)\n",
            "  attention_size: 128\n",
            "  attention_type: bah_mon_norm\n",
            "  attention_win_size: 7\n",
            "  cleaners: korean_cleaners\n",
            "  clip_mels_length: True\n",
            "  cumulative_weights: True\n",
            "  dec_prenet_sizes: [256, 256]\n",
            "  decoder_layers: 2\n",
            "  decoder_lstm_units: 1024\n",
            "  dilation_channels: 256\n",
            "  dilations: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
            "  dropout_prob: 0.5\n",
            "  embedding_size: 512\n",
            "  enc_conv_channels: 512\n",
            "  enc_conv_kernel_size: 5\n",
            "  enc_conv_num_layers: 3\n",
            "  encoder_lstm_units: 256\n",
            "  fft_size: 2048\n",
            "  filter_width: 3\n",
            "  gc_channels: 32\n",
            "  griffin_lim_iters: 60\n",
            "  hop_size: 300\n",
            "  inference_prenet_dropout: True\n",
            "  initial_data_greedy: True\n",
            "  initial_phase_step: 8000\n",
            "  input_type: raw\n",
            "  l2_regularization_strength: 0\n",
            "  legacy: True\n",
            "  main_data: ['']\n",
            "  main_data_greedy_factor: 0\n",
            "  mask_encoder: True\n",
            "  max_abs_value: 4.0\n",
            "  max_checkpoints: 3\n",
            "  max_mel_frames: 1000\n",
            "  max_n_frame: 1000\n",
            "  min_level_db: -100\n",
            "  min_n_frame: 150\n",
            "  min_tokens: 30\n",
            "  model_type: multi-speaker\n",
            "  momentum: 0.9\n",
            "  name: Tacotron-2\n",
            "  num_mels: 80\n",
            "  num_steps: 1000000\n",
            "  optimizer: adam\n",
            "  out_channels: 30\n",
            "  post_bank_channel_size: 128\n",
            "  post_bank_size: 8\n",
            "  post_highway_depth: 4\n",
            "  post_maxpool_width: 2\n",
            "  post_proj_sizes: [256, 80]\n",
            "  post_proj_width: 3\n",
            "  post_rnn_size: 128\n",
            "  postnet_channels: 512\n",
            "  postnet_kernel_size: (5,)\n",
            "  postnet_num_layers: 5\n",
            "  power: 1.5\n",
            "  preemphasis: 0.97\n",
            "  preemphasize: True\n",
            "  prenet_layers: [256, 256]\n",
            "  prioritize_loss: False\n",
            "  quantization_channels: 256\n",
            "  reduction_factor: 2\n",
            "  ref_level_db: 20\n",
            "  rescaling: True\n",
            "  rescaling_max: 0.999\n",
            "  residual_channels: 128\n",
            "  residual_legacy: True\n",
            "  sample_rate: 24000\n",
            "  sample_size: 9000\n",
            "  scalar_input: True\n",
            "  signal_normalization: True\n",
            "  silence_threshold: 0\n",
            "  skip_channels: 128\n",
            "  skip_inadequate: False\n",
            "  skip_path_filter: False\n",
            "  smoothing: False\n",
            "  speaker_embedding_size: 16\n",
            "  store_metadata: False\n",
            "  symmetric_mels: True\n",
            "  synthesis_constraint: False\n",
            "  synthesis_constraint_type: window\n",
            "  tacotron_decay_learning_rate: True\n",
            "  tacotron_decay_rate: 0.5\n",
            "  tacotron_decay_steps: 18000\n",
            "  tacotron_final_learning_rate: 0.0001\n",
            "  tacotron_initial_learning_rate: 0.001\n",
            "  tacotron_reg_weight: 1e-06\n",
            "  tacotron_start_decay: 40000\n",
            "  tacotron_zoneout_rate: 0.1\n",
            "  trim_fft_size: 512\n",
            "  trim_hop_size: 128\n",
            "  trim_silence: True\n",
            "  trim_top_db: 23\n",
            "  upsample_factor: [12, 25]\n",
            "  upsample_type: SubPixel\n",
            "  use_biases: True\n",
            "  use_lws: False\n",
            "  wavenet_batch_size: 2\n",
            "  wavenet_clip_gradients: True\n",
            "  wavenet_decay_rate: 0.5\n",
            "  wavenet_decay_steps: 300000\n",
            "  wavenet_dropout: 0.05\n",
            "  wavenet_learning_rate: 0.001\n",
            "  win_size: 1200\n",
            "Sampling frequency: 24000\n",
            " 52% 6639/12854 [33:48<57:57,  1.79it/s]concurrent.futures.process._RemoteTraceback: \n",
            "\"\"\"\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/concurrent/futures/process.py\", line 175, in _process_worker\n",
            "    r = call_item.fn(*call_item.args, **call_item.kwargs)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/datasets/kss.py\", line 167, in _process_utterance\n",
            "    'tokens': text_to_sequence(text),   # eos(~)에 해당하는 \"1\"이 끝에 붙는다.\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/__init__.py\", line 42, in text_to_sequence\n",
            "    return _text_to_sequence(text, cleaner_names, as_token)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/__init__.py\", line 63, in _text_to_sequence\n",
            "    sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/__init__.py\", line 110, in _clean_text\n",
            "    text = cleaner(text) # '존경하는' --> ['ᄌ', 'ᅩ', 'ᆫ', 'ᄀ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᄂ', 'ᅳ', 'ᆫ', '~']\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/cleaners.py\", line 29, in korean_cleaners\n",
            "    text = ko_tokenize(text) # '존경하는' --> ['ᄌ', 'ᅩ', 'ᆫ', 'ᄀ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᄂ', 'ᅳ', 'ᆫ', '~']\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 142, in tokenize\n",
            "    text = normalize(text)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 164, in normalize\n",
            "    text = normalize_number(text)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 215, in normalize_number\n",
            "    lambda x: number_to_korean(x, False), text)\n",
            "  File \"/usr/lib/python3.6/re.py\", line 191, in sub\n",
            "    return _compile(pattern, flags).sub(repl, string, count)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 215, in <lambda>\n",
            "    lambda x: number_to_korean(x, False), text)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 246, in number_to_korean\n",
            "    num = ast.literal_eval(num_str)\n",
            "  File \"/usr/lib/python3.6/ast.py\", line 48, in literal_eval\n",
            "    node_or_string = parse(node_or_string, mode='eval')\n",
            "  File \"/usr/lib/python3.6/ast.py\", line 35, in parse\n",
            "    return compile(source, filename, mode, PyCF_ONLY_AST)\n",
            "  File \"<unknown>\", line 1\n",
            "    010\n",
            "      ^\n",
            "SyntaxError: invalid token\n",
            "\"\"\"\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"./Tacotron2-Wavenet-Korean-TTS/preprocess.py\", line 62, in <module>\n",
            "    preprocess(mod, in_dir, out_dir, num_workers)\n",
            "  File \"./Tacotron2-Wavenet-Korean-TTS/preprocess.py\", line 21, in preprocess\n",
            "    metadata = mod.build_from_path(hparams, in_dir, out_dir,num_workers=num_workers, tqdm=tqdm)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/datasets/kss.py\", line 44, in build_from_path\n",
            "    return [future.result() for future in tqdm(futures) if future.result() is not None]\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/datasets/kss.py\", line 44, in <listcomp>\n",
            "    return [future.result() for future in tqdm(futures) if future.result() is not None]\n",
            "  File \"/usr/lib/python3.6/concurrent/futures/_base.py\", line 425, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.6/concurrent/futures/_base.py\", line 384, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/lib/python3.6/concurrent/futures/process.py\", line 175, in _process_worker\n",
            "    r = call_item.fn(*call_item.args, **call_item.kwargs)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/datasets/kss.py\", line 167, in _process_utterance\n",
            "    'tokens': text_to_sequence(text),   # eos(~)에 해당하는 \"1\"이 끝에 붙는다.\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/__init__.py\", line 42, in text_to_sequence\n",
            "    return _text_to_sequence(text, cleaner_names, as_token)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/__init__.py\", line 63, in _text_to_sequence\n",
            "    sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/__init__.py\", line 110, in _clean_text\n",
            "    text = cleaner(text) # '존경하는' --> ['ᄌ', 'ᅩ', 'ᆫ', 'ᄀ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᄂ', 'ᅳ', 'ᆫ', '~']\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/cleaners.py\", line 29, in korean_cleaners\n",
            "    text = ko_tokenize(text) # '존경하는' --> ['ᄌ', 'ᅩ', 'ᆫ', 'ᄀ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᄂ', 'ᅳ', 'ᆫ', '~']\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 142, in tokenize\n",
            "    text = normalize(text)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 164, in normalize\n",
            "    text = normalize_number(text)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 215, in normalize_number\n",
            "    lambda x: number_to_korean(x, False), text)\n",
            "  File \"/usr/lib/python3.6/re.py\", line 191, in sub\n",
            "    return _compile(pattern, flags).sub(repl, string, count)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 215, in <lambda>\n",
            "    lambda x: number_to_korean(x, False), text)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 246, in number_to_korean\n",
            "    num = ast.literal_eval(num_str)\n",
            "  File \"/usr/lib/python3.6/ast.py\", line 48, in literal_eval\n",
            "    node_or_string = parse(node_or_string, mode='eval')\n",
            "  File \"/usr/lib/python3.6/ast.py\", line 35, in parse\n",
            "    return compile(source, filename, mode, PyCF_ONLY_AST)\n",
            "  File \"<unknown>\", line 1\n",
            "    010\n",
            "      ^\n",
            "SyntaxError: invalid token\n",
            " 52% 6639/12854 [1:06:20<1:02:05,  1.67it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asPccdo8SACe",
        "outputId": "03f798e1-16a2-4342-879f-05edfba85ac8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python ./Tacotron2-Wavenet-Korean-TTS/preprocess.py --num_workers 10 --name inna --in_dir ./datasets/inna --out_dir ./data/inna"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Hyperparameters:\n",
            "  adam_beta1: 0.9\n",
            "  adam_beta2: 0.999\n",
            "  allow_clipping_in_normalization: True\n",
            "  attention_dim: 128\n",
            "  attention_filters: 32\n",
            "  attention_kernel: (31,)\n",
            "  attention_size: 128\n",
            "  attention_type: bah_mon_norm\n",
            "  attention_win_size: 7\n",
            "  cleaners: korean_cleaners\n",
            "  clip_mels_length: True\n",
            "  cumulative_weights: True\n",
            "  dec_prenet_sizes: [256, 256]\n",
            "  decoder_layers: 2\n",
            "  decoder_lstm_units: 1024\n",
            "  dilation_channels: 256\n",
            "  dilations: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
            "  dropout_prob: 0.5\n",
            "  embedding_size: 512\n",
            "  enc_conv_channels: 512\n",
            "  enc_conv_kernel_size: 5\n",
            "  enc_conv_num_layers: 3\n",
            "  encoder_lstm_units: 256\n",
            "  fft_size: 2048\n",
            "  filter_width: 3\n",
            "  gc_channels: 32\n",
            "  griffin_lim_iters: 60\n",
            "  hop_size: 300\n",
            "  inference_prenet_dropout: True\n",
            "  initial_data_greedy: True\n",
            "  initial_phase_step: 8000\n",
            "  input_type: raw\n",
            "  l2_regularization_strength: 0\n",
            "  legacy: True\n",
            "  main_data: ['']\n",
            "  main_data_greedy_factor: 0\n",
            "  mask_encoder: True\n",
            "  max_abs_value: 4.0\n",
            "  max_checkpoints: 3\n",
            "  max_mel_frames: 1000\n",
            "  max_n_frame: 1000\n",
            "  min_level_db: -100\n",
            "  min_n_frame: 150\n",
            "  min_tokens: 30\n",
            "  model_type: multi-speaker\n",
            "  momentum: 0.9\n",
            "  name: Tacotron-2\n",
            "  num_mels: 80\n",
            "  num_steps: 1000000\n",
            "  optimizer: adam\n",
            "  out_channels: 30\n",
            "  post_bank_channel_size: 128\n",
            "  post_bank_size: 8\n",
            "  post_highway_depth: 4\n",
            "  post_maxpool_width: 2\n",
            "  post_proj_sizes: [256, 80]\n",
            "  post_proj_width: 3\n",
            "  post_rnn_size: 128\n",
            "  postnet_channels: 512\n",
            "  postnet_kernel_size: (5,)\n",
            "  postnet_num_layers: 5\n",
            "  power: 1.5\n",
            "  preemphasis: 0.97\n",
            "  preemphasize: True\n",
            "  prenet_layers: [256, 256]\n",
            "  prioritize_loss: False\n",
            "  quantization_channels: 256\n",
            "  reduction_factor: 2\n",
            "  ref_level_db: 20\n",
            "  rescaling: True\n",
            "  rescaling_max: 0.999\n",
            "  residual_channels: 128\n",
            "  residual_legacy: True\n",
            "  sample_rate: 24000\n",
            "  sample_size: 9000\n",
            "  scalar_input: True\n",
            "  signal_normalization: True\n",
            "  silence_threshold: 0\n",
            "  skip_channels: 128\n",
            "  skip_inadequate: False\n",
            "  skip_path_filter: False\n",
            "  smoothing: False\n",
            "  speaker_embedding_size: 16\n",
            "  store_metadata: False\n",
            "  symmetric_mels: True\n",
            "  synthesis_constraint: False\n",
            "  synthesis_constraint_type: window\n",
            "  tacotron_decay_learning_rate: True\n",
            "  tacotron_decay_rate: 0.5\n",
            "  tacotron_decay_steps: 18000\n",
            "  tacotron_final_learning_rate: 0.0001\n",
            "  tacotron_initial_learning_rate: 0.001\n",
            "  tacotron_reg_weight: 1e-06\n",
            "  tacotron_start_decay: 40000\n",
            "  tacotron_zoneout_rate: 0.1\n",
            "  trim_fft_size: 512\n",
            "  trim_hop_size: 128\n",
            "  trim_silence: True\n",
            "  trim_top_db: 23\n",
            "  upsample_factor: [12, 25]\n",
            "  upsample_type: SubPixel\n",
            "  use_biases: True\n",
            "  use_lws: False\n",
            "  wavenet_batch_size: 2\n",
            "  wavenet_clip_gradients: True\n",
            "  wavenet_decay_rate: 0.5\n",
            "  wavenet_decay_steps: 300000\n",
            "  wavenet_dropout: 0.05\n",
            "  wavenet_learning_rate: 0.001\n",
            "  win_size: 1200\n",
            "Sampling frequency: 24000\n",
            " 16% 486/2983 [01:47<20:25,  2.04it/s]concurrent.futures.process._RemoteTraceback: \n",
            "\"\"\"\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/concurrent/futures/process.py\", line 175, in _process_worker\n",
            "    r = call_item.fn(*call_item.args, **call_item.kwargs)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/datasets/inna.py\", line 163, in _process_utterance\n",
            "    'tokens': text_to_sequence(text),   # eos(~)에 해당하는 \"1\"이 끝에 붙는다.\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/__init__.py\", line 42, in text_to_sequence\n",
            "    return _text_to_sequence(text, cleaner_names, as_token)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/__init__.py\", line 63, in _text_to_sequence\n",
            "    sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/__init__.py\", line 110, in _clean_text\n",
            "    text = cleaner(text) # '존경하는' --> ['ᄌ', 'ᅩ', 'ᆫ', 'ᄀ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᄂ', 'ᅳ', 'ᆫ', '~']\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/cleaners.py\", line 29, in korean_cleaners\n",
            "    text = ko_tokenize(text) # '존경하는' --> ['ᄌ', 'ᅩ', 'ᆫ', 'ᄀ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᄂ', 'ᅳ', 'ᆫ', '~']\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 142, in tokenize\n",
            "    text = normalize(text)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 164, in normalize\n",
            "    text = normalize_number(text)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 215, in normalize_number\n",
            "    lambda x: number_to_korean(x, False), text)\n",
            "  File \"/usr/lib/python3.6/re.py\", line 191, in sub\n",
            "    return _compile(pattern, flags).sub(repl, string, count)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 215, in <lambda>\n",
            "    lambda x: number_to_korean(x, False), text)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 246, in number_to_korean\n",
            "    num = ast.literal_eval(num_str)\n",
            "  File \"/usr/lib/python3.6/ast.py\", line 48, in literal_eval\n",
            "    node_or_string = parse(node_or_string, mode='eval')\n",
            "  File \"/usr/lib/python3.6/ast.py\", line 35, in parse\n",
            "    return compile(source, filename, mode, PyCF_ONLY_AST)\n",
            "  File \"<unknown>\", line 1\n",
            "    017\n",
            "      ^\n",
            "SyntaxError: invalid token\n",
            "\"\"\"\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"./Tacotron2-Wavenet-Korean-TTS/preprocess.py\", line 62, in <module>\n",
            "    preprocess(mod, in_dir, out_dir, num_workers)\n",
            "  File \"./Tacotron2-Wavenet-Korean-TTS/preprocess.py\", line 21, in preprocess\n",
            "    metadata = mod.build_from_path(hparams, in_dir, out_dir,num_workers=num_workers, tqdm=tqdm)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/datasets/inna.py\", line 46, in build_from_path\n",
            "    return [future.result() for future in tqdm(futures) if future.result() is not None]\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/datasets/inna.py\", line 46, in <listcomp>\n",
            "    return [future.result() for future in tqdm(futures) if future.result() is not None]\n",
            "  File \"/usr/lib/python3.6/concurrent/futures/_base.py\", line 425, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.6/concurrent/futures/_base.py\", line 384, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/lib/python3.6/concurrent/futures/process.py\", line 175, in _process_worker\n",
            "    r = call_item.fn(*call_item.args, **call_item.kwargs)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/datasets/inna.py\", line 163, in _process_utterance\n",
            "    'tokens': text_to_sequence(text),   # eos(~)에 해당하는 \"1\"이 끝에 붙는다.\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/__init__.py\", line 42, in text_to_sequence\n",
            "    return _text_to_sequence(text, cleaner_names, as_token)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/__init__.py\", line 63, in _text_to_sequence\n",
            "    sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/__init__.py\", line 110, in _clean_text\n",
            "    text = cleaner(text) # '존경하는' --> ['ᄌ', 'ᅩ', 'ᆫ', 'ᄀ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᄂ', 'ᅳ', 'ᆫ', '~']\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/cleaners.py\", line 29, in korean_cleaners\n",
            "    text = ko_tokenize(text) # '존경하는' --> ['ᄌ', 'ᅩ', 'ᆫ', 'ᄀ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᄂ', 'ᅳ', 'ᆫ', '~']\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 142, in tokenize\n",
            "    text = normalize(text)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 164, in normalize\n",
            "    text = normalize_number(text)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 215, in normalize_number\n",
            "    lambda x: number_to_korean(x, False), text)\n",
            "  File \"/usr/lib/python3.6/re.py\", line 191, in sub\n",
            "    return _compile(pattern, flags).sub(repl, string, count)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 215, in <lambda>\n",
            "    lambda x: number_to_korean(x, False), text)\n",
            "  File \"/content/drive/My Drive/stt플젝/Tacotron2-Wavenet-Korean-TTS/text/korean.py\", line 246, in number_to_korean\n",
            "    num = ast.literal_eval(num_str)\n",
            "  File \"/usr/lib/python3.6/ast.py\", line 48, in literal_eval\n",
            "    node_or_string = parse(node_or_string, mode='eval')\n",
            "  File \"/usr/lib/python3.6/ast.py\", line 35, in parse\n",
            "    return compile(source, filename, mode, PyCF_ONLY_AST)\n",
            "  File \"<unknown>\", line 1\n",
            "    017\n",
            "      ^\n",
            "SyntaxError: invalid token\n",
            " 16% 486/2983 [11:47<1:00:35,  1.46s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nxwpMsoPIQ6"
      },
      "source": [
        "# train_tacotron2.py     \n",
        "# parser.add_argument('--data_paths', default='/content/drive/My Drive/stt플젝/data/kss,/content/drive/My Drive/stt플젝/data/inna')\n",
        "# test_interval 생성빈도 5000으로 낮춤 --> 1000으로 높임\n",
        "# checkpoint_interval 체크포인트 빈도수 1000으로 높임\n",
        "# parser.add_argument('--batch_size', type=int, default=8)  # 배치사이즈 조절 --> 32로 다시 --> 16으로 --> 다음 셀에서 8로\n",
        "# parser.add_argument('--load_path', default='/content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45')\n",
        "\n",
        "\n",
        "# utils/__init__.py \n",
        "# NanumBarunGothic.ttf 경로 설정\n",
        "# copy_file hparams.py 경로 설정\n",
        "\n",
        "# datasets/datafeeder_tacotron2.py\n",
        "# data_dirs 경로 변경  # '/content/drive/My Drive/stt플젝/data/inna'\n",
        "# mydatafeed = DataFeederTacotron2(coord, data_dirs, hparams, config, 8,data_type='train', batch_size=config.batch_size)\n",
        "# --> 다시 32로 --> 16으로 --> 다음 셀에서 8로\n",
        "\n",
        "\n",
        "#!python ./Tacotron2-Wavenet-Korean-TTS/train_tacotron2.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FeEFr7hE73N"
      },
      "source": [
        "# batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WuZXYBur0zG",
        "outputId": "f9a8068e-362b-46e9-9439-8cf029136e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# batch_size=8\n",
        "\n",
        "!python ./Tacotron2-Wavenet-Korean-TTS/train_tacotron2.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Step 22252   [1.616 sec/step, loss=0.77559, avg_loss=0.77973]\n",
            "Step 22253   [1.614 sec/step, loss=0.79117, avg_loss=0.77935]\n",
            "Step 22254   [1.611 sec/step, loss=0.71691, avg_loss=0.77835]\n",
            "Step 22255   [1.609 sec/step, loss=0.80876, avg_loss=0.77835]\n",
            "Step 22256   [1.614 sec/step, loss=0.69889, avg_loss=0.77750]\n",
            "Step 22257   [1.607 sec/step, loss=0.76202, avg_loss=0.77771]\n",
            "Step 22258   [1.609 sec/step, loss=0.84939, avg_loss=0.77803]\n",
            "Step 22259   [1.570 sec/step, loss=0.78662, avg_loss=0.77852]\n",
            "Step 22260   [1.587 sec/step, loss=0.83903, avg_loss=0.77923]\n",
            "Generated 32 batches of size 8 in 7.783 sec\n",
            "Step 22261   [1.597 sec/step, loss=0.78672, avg_loss=0.77890]\n",
            "Step 22262   [1.596 sec/step, loss=0.78222, avg_loss=0.77888]\n",
            "Step 22263   [1.598 sec/step, loss=0.88566, avg_loss=0.77965]\n",
            "Step 22264   [1.589 sec/step, loss=0.77501, avg_loss=0.77925]\n",
            "Step 22265   [1.576 sec/step, loss=0.79831, avg_loss=0.78013]\n",
            "Step 22266   [1.575 sec/step, loss=0.77854, avg_loss=0.77979]\n",
            "Step 22267   [1.570 sec/step, loss=0.77421, avg_loss=0.77994]\n",
            "Step 22268   [1.569 sec/step, loss=0.73529, avg_loss=0.77960]\n",
            "Step 22269   [1.552 sec/step, loss=0.79836, avg_loss=0.77966]\n",
            "Step 22270   [1.554 sec/step, loss=0.77283, avg_loss=0.77933]\n",
            "Step 22271   [1.561 sec/step, loss=0.75443, avg_loss=0.77954]\n",
            "Step 22272   [1.533 sec/step, loss=0.76812, avg_loss=0.77975]\n",
            "Step 22273   [1.536 sec/step, loss=0.75838, avg_loss=0.77954]\n",
            "Step 22274   [1.529 sec/step, loss=0.73097, avg_loss=0.77866]\n",
            "Step 22275   [1.538 sec/step, loss=0.75433, avg_loss=0.77867]\n",
            "Step 22276   [1.523 sec/step, loss=0.80523, avg_loss=0.77824]\n",
            "Step 22277   [1.521 sec/step, loss=0.77817, avg_loss=0.77828]\n",
            "Step 22278   [1.519 sec/step, loss=0.79183, avg_loss=0.77866]\n",
            "Step 22279   [1.517 sec/step, loss=0.81806, avg_loss=0.77892]\n",
            "Step 22280   [1.521 sec/step, loss=0.81126, avg_loss=0.77873]\n",
            "Step 22281   [1.535 sec/step, loss=0.75599, avg_loss=0.77808]\n",
            "Step 22282   [1.534 sec/step, loss=0.78984, avg_loss=0.77791]\n",
            "Step 22283   [1.533 sec/step, loss=0.77877, avg_loss=0.77823]\n",
            "Step 22284   [1.533 sec/step, loss=0.82207, avg_loss=0.77847]\n",
            "Step 22285   [1.535 sec/step, loss=0.83472, avg_loss=0.77872]\n",
            "Step 22286   [1.571 sec/step, loss=0.67158, avg_loss=0.77770]\n",
            "Step 22287   [1.568 sec/step, loss=0.77616, avg_loss=0.77741]\n",
            "Step 22288   [1.567 sec/step, loss=0.78939, avg_loss=0.77736]\n",
            "Step 22289   [1.566 sec/step, loss=0.81644, avg_loss=0.77747]\n",
            "Step 22290   [1.576 sec/step, loss=0.74205, avg_loss=0.77753]\n",
            "Step 22291   [1.581 sec/step, loss=0.79617, avg_loss=0.77752]\n",
            "Step 22292   [1.581 sec/step, loss=0.80353, avg_loss=0.77824]\n",
            "Step 22293   [1.588 sec/step, loss=0.85748, avg_loss=0.77919]\n",
            "Generated 32 batches of size 8 in 8.013 sec\n",
            "Step 22294   [1.588 sec/step, loss=0.82217, avg_loss=0.77981]\n",
            "Step 22295   [1.585 sec/step, loss=0.74666, avg_loss=0.77909]\n",
            "Step 22296   [1.592 sec/step, loss=0.74006, avg_loss=0.77831]\n",
            "Step 22297   [1.589 sec/step, loss=0.88440, avg_loss=0.77943]\n",
            "Step 22298   [1.601 sec/step, loss=0.76040, avg_loss=0.77941]\n",
            "Step 22299   [1.601 sec/step, loss=0.77986, avg_loss=0.78023]\n",
            "Step 22300   [1.580 sec/step, loss=0.82068, avg_loss=0.78040]\n",
            "Writing summary at step: 22300\n",
            "Step 22301   [1.573 sec/step, loss=0.80566, avg_loss=0.78115]\n",
            "Step 22302   [1.580 sec/step, loss=0.78961, avg_loss=0.78148]\n",
            "Step 22303   [1.577 sec/step, loss=0.80052, avg_loss=0.78186]\n",
            "Step 22304   [1.578 sec/step, loss=0.78415, avg_loss=0.78174]\n",
            "Step 22305   [1.565 sec/step, loss=0.72132, avg_loss=0.78101]\n",
            "Step 22306   [1.566 sec/step, loss=0.79858, avg_loss=0.78112]\n",
            "Step 22307   [1.573 sec/step, loss=0.84112, avg_loss=0.78174]\n",
            "Step 22308   [1.575 sec/step, loss=0.75926, avg_loss=0.78080]\n",
            "Step 22309   [1.576 sec/step, loss=0.76829, avg_loss=0.78098]\n",
            "Step 22310   [1.581 sec/step, loss=0.79966, avg_loss=0.78139]\n",
            "Step 22311   [1.579 sec/step, loss=0.79809, avg_loss=0.78122]\n",
            "Step 22312   [1.571 sec/step, loss=0.80969, avg_loss=0.78173]\n",
            "Step 22313   [1.584 sec/step, loss=0.76417, avg_loss=0.78188]\n",
            "Step 22314   [1.579 sec/step, loss=0.75611, avg_loss=0.78182]\n",
            "Step 22315   [1.579 sec/step, loss=0.77219, avg_loss=0.78238]\n",
            "Step 22316   [1.592 sec/step, loss=0.81635, avg_loss=0.78193]\n",
            "Step 22317   [1.596 sec/step, loss=0.76661, avg_loss=0.78142]\n",
            "Step 22318   [1.601 sec/step, loss=0.83045, avg_loss=0.78208]\n",
            "Step 22319   [1.606 sec/step, loss=0.77118, avg_loss=0.78177]\n",
            "Step 22320   [1.620 sec/step, loss=0.78059, avg_loss=0.78203]\n",
            "Step 22321   [1.621 sec/step, loss=0.77265, avg_loss=0.78246]\n",
            "Step 22322   [1.623 sec/step, loss=0.82145, avg_loss=0.78254]\n",
            "Step 22323   [1.616 sec/step, loss=0.80160, avg_loss=0.78322]\n",
            "Step 22324   [1.612 sec/step, loss=0.76443, avg_loss=0.78272]\n",
            "Step 22325   [1.584 sec/step, loss=0.79585, avg_loss=0.78355]\n",
            "Generated 32 batches of size 8 in 9.450 sec\n",
            "Step 22326   [1.574 sec/step, loss=0.78876, avg_loss=0.78410]\n",
            "Step 22327   [1.564 sec/step, loss=0.79553, avg_loss=0.78494]\n",
            "Step 22328   [1.571 sec/step, loss=0.84469, avg_loss=0.78578]\n",
            "Step 22329   [1.568 sec/step, loss=0.83477, avg_loss=0.78618]\n",
            "Step 22330   [1.570 sec/step, loss=0.76157, avg_loss=0.78593]\n",
            "Step 22331   [1.562 sec/step, loss=0.76514, avg_loss=0.78645]\n",
            "Step 22332   [1.556 sec/step, loss=0.77988, avg_loss=0.78648]\n",
            "Step 22333   [1.559 sec/step, loss=0.80270, avg_loss=0.78659]\n",
            "Step 22334   [1.564 sec/step, loss=0.77730, avg_loss=0.78702]\n",
            "Step 22335   [1.581 sec/step, loss=0.80108, avg_loss=0.78697]\n",
            "Step 22336   [1.581 sec/step, loss=0.72667, avg_loss=0.78613]\n",
            "Step 22337   [1.575 sec/step, loss=0.79415, avg_loss=0.78638]\n",
            "Step 22338   [1.577 sec/step, loss=0.82132, avg_loss=0.78687]\n",
            "Step 22339   [1.577 sec/step, loss=0.76312, avg_loss=0.78623]\n",
            "Step 22340   [1.579 sec/step, loss=0.82511, avg_loss=0.78649]\n",
            "Step 22341   [1.580 sec/step, loss=0.79641, avg_loss=0.78612]\n",
            "Step 22342   [1.585 sec/step, loss=0.76968, avg_loss=0.78628]\n",
            "Step 22343   [1.592 sec/step, loss=0.76875, avg_loss=0.78611]\n",
            "Step 22344   [1.589 sec/step, loss=0.81359, avg_loss=0.78679]\n",
            "Step 22345   [1.554 sec/step, loss=0.78038, avg_loss=0.78732]\n",
            "Step 22346   [1.538 sec/step, loss=0.75964, avg_loss=0.78746]\n",
            "Step 22347   [1.531 sec/step, loss=0.74718, avg_loss=0.78671]\n",
            "Step 22348   [1.551 sec/step, loss=0.83458, avg_loss=0.78728]\n",
            "Step 22349   [1.555 sec/step, loss=0.85078, avg_loss=0.78768]\n",
            "Step 22350   [1.557 sec/step, loss=0.75388, avg_loss=0.78745]\n",
            "Step 22351   [1.569 sec/step, loss=0.80964, avg_loss=0.78731]\n",
            "Step 22352   [1.571 sec/step, loss=0.85903, avg_loss=0.78814]\n",
            "Step 22353   [1.580 sec/step, loss=0.79600, avg_loss=0.78819]\n",
            "Step 22354   [1.579 sec/step, loss=0.70505, avg_loss=0.78807]\n",
            "Step 22355   [1.581 sec/step, loss=0.80191, avg_loss=0.78800]\n",
            "Step 22356   [1.579 sec/step, loss=0.77498, avg_loss=0.78876]\n",
            "Generated 32 batches of size 8 in 10.229 sec\n",
            "Step 22357   [1.621 sec/step, loss=0.76700, avg_loss=0.78881]\n",
            "Step 22358   [1.632 sec/step, loss=0.82416, avg_loss=0.78856]\n",
            "Step 22359   [1.640 sec/step, loss=0.74969, avg_loss=0.78819]\n",
            "Step 22360   [1.624 sec/step, loss=0.81080, avg_loss=0.78791]\n",
            "Step 22361   [1.615 sec/step, loss=0.78767, avg_loss=0.78792]\n",
            "Step 22362   [1.621 sec/step, loss=0.79947, avg_loss=0.78809]\n",
            "Step 22363   [1.622 sec/step, loss=0.79991, avg_loss=0.78723]\n",
            "Step 22364   [1.616 sec/step, loss=0.80421, avg_loss=0.78752]\n",
            "Step 22365   [1.621 sec/step, loss=0.78490, avg_loss=0.78739]\n",
            "Step 22366   [1.622 sec/step, loss=0.78121, avg_loss=0.78742]\n",
            "Step 22367   [1.665 sec/step, loss=0.71753, avg_loss=0.78685]\n",
            "Step 22368   [1.681 sec/step, loss=0.76668, avg_loss=0.78716]\n",
            "Step 22369   [1.676 sec/step, loss=0.80412, avg_loss=0.78722]\n",
            "Step 22370   [1.674 sec/step, loss=0.74300, avg_loss=0.78692]\n",
            "Step 22371   [1.669 sec/step, loss=0.76440, avg_loss=0.78702]\n",
            "Step 22372   [1.677 sec/step, loss=0.74026, avg_loss=0.78674]\n",
            "Step 22373   [1.670 sec/step, loss=0.82837, avg_loss=0.78744]\n",
            "Step 22374   [1.676 sec/step, loss=0.74484, avg_loss=0.78758]\n",
            "Step 22375   [1.666 sec/step, loss=0.75939, avg_loss=0.78763]\n",
            "Step 22376   [1.665 sec/step, loss=0.80064, avg_loss=0.78759]\n",
            "Step 22377   [1.666 sec/step, loss=0.74622, avg_loss=0.78727]\n",
            "Step 22378   [1.666 sec/step, loss=0.79865, avg_loss=0.78734]\n",
            "Step 22379   [1.667 sec/step, loss=0.79567, avg_loss=0.78711]\n",
            "Step 22380   [1.663 sec/step, loss=0.78928, avg_loss=0.78689]\n",
            "Step 22381   [1.649 sec/step, loss=0.75894, avg_loss=0.78692]\n",
            "Step 22382   [1.652 sec/step, loss=0.82446, avg_loss=0.78727]\n",
            "Step 22383   [1.652 sec/step, loss=0.72939, avg_loss=0.78678]\n",
            "Step 22384   [1.662 sec/step, loss=0.78481, avg_loss=0.78640]\n",
            "Step 22385   [1.667 sec/step, loss=0.75573, avg_loss=0.78561]\n",
            "Step 22386   [1.647 sec/step, loss=0.83524, avg_loss=0.78725]\n",
            "Step 22387   [1.655 sec/step, loss=0.76882, avg_loss=0.78718]\n",
            "Step 22388   [1.656 sec/step, loss=0.78524, avg_loss=0.78713]\n",
            "Generated 32 batches of size 8 in 10.306 sec\n",
            "Step 22389   [1.658 sec/step, loss=0.77274, avg_loss=0.78670]\n",
            "Step 22390   [1.651 sec/step, loss=0.79918, avg_loss=0.78727]\n",
            "Step 22391   [1.649 sec/step, loss=0.80631, avg_loss=0.78737]\n",
            "Step 22392   [1.656 sec/step, loss=0.78882, avg_loss=0.78722]\n",
            "Step 22393   [1.646 sec/step, loss=0.78501, avg_loss=0.78650]\n",
            "Step 22394   [1.644 sec/step, loss=0.77157, avg_loss=0.78599]\n",
            "Step 22395   [1.648 sec/step, loss=0.76162, avg_loss=0.78614]\n",
            "Step 22396   [1.644 sec/step, loss=0.77812, avg_loss=0.78652]\n",
            "Step 22397   [1.652 sec/step, loss=0.82832, avg_loss=0.78596]\n",
            "Step 22398   [1.632 sec/step, loss=0.79795, avg_loss=0.78634]\n",
            "Step 22399   [1.627 sec/step, loss=0.76867, avg_loss=0.78623]\n",
            "Step 22400   [1.640 sec/step, loss=0.76735, avg_loss=0.78569]\n",
            "Writing summary at step: 22400\n",
            "Step 22401   [1.648 sec/step, loss=0.80422, avg_loss=0.78568]\n",
            "Step 22402   [1.648 sec/step, loss=0.73464, avg_loss=0.78513]\n",
            "Step 22403   [1.650 sec/step, loss=0.77678, avg_loss=0.78489]\n",
            "Step 22404   [1.651 sec/step, loss=0.80294, avg_loss=0.78508]\n",
            "Step 22405   [1.652 sec/step, loss=0.80970, avg_loss=0.78596]\n",
            "Step 22406   [1.654 sec/step, loss=0.76055, avg_loss=0.78558]\n",
            "Step 22407   [1.650 sec/step, loss=0.78324, avg_loss=0.78500]\n",
            "Step 22408   [1.650 sec/step, loss=0.79743, avg_loss=0.78538]\n",
            "Step 22409   [1.657 sec/step, loss=0.75960, avg_loss=0.78530]\n",
            "Step 22410   [1.649 sec/step, loss=0.72540, avg_loss=0.78456]\n",
            "Step 22411   [1.654 sec/step, loss=0.77453, avg_loss=0.78432]\n",
            "Step 22412   [1.658 sec/step, loss=0.78346, avg_loss=0.78406]\n",
            "Step 22413   [1.643 sec/step, loss=0.78754, avg_loss=0.78429]\n",
            "Step 22414   [1.642 sec/step, loss=0.73564, avg_loss=0.78409]\n",
            "Step 22415   [1.694 sec/step, loss=0.74384, avg_loss=0.78380]\n",
            "Step 22416   [1.687 sec/step, loss=0.77610, avg_loss=0.78340]\n",
            "Step 22417   [1.691 sec/step, loss=0.82650, avg_loss=0.78400]\n",
            "Generated 32 batches of size 8 in 10.103 sec\n",
            "Step 22418   [1.687 sec/step, loss=0.82401, avg_loss=0.78393]\n",
            "Step 22419   [1.686 sec/step, loss=0.80639, avg_loss=0.78429]\n",
            "Step 22420   [1.672 sec/step, loss=0.81185, avg_loss=0.78460]\n",
            "Step 22421   [1.677 sec/step, loss=0.83043, avg_loss=0.78518]\n",
            "Step 22422   [1.677 sec/step, loss=0.79477, avg_loss=0.78491]\n",
            "Step 22423   [1.679 sec/step, loss=0.79639, avg_loss=0.78486]\n",
            "Step 22424   [1.682 sec/step, loss=0.76299, avg_loss=0.78484]\n",
            "Step 22425   [1.692 sec/step, loss=0.81428, avg_loss=0.78503]\n",
            "Step 22426   [1.690 sec/step, loss=0.80243, avg_loss=0.78517]\n",
            "Step 22427   [1.670 sec/step, loss=0.81796, avg_loss=0.78539]\n",
            "Step 22428   [1.684 sec/step, loss=0.78730, avg_loss=0.78482]\n",
            "Step 22429   [1.680 sec/step, loss=0.77715, avg_loss=0.78424]\n",
            "Step 22430   [1.684 sec/step, loss=0.79180, avg_loss=0.78454]\n",
            "Step 22431   [1.692 sec/step, loss=0.81331, avg_loss=0.78502]\n",
            "Step 22432   [1.729 sec/step, loss=0.64594, avg_loss=0.78368]\n",
            "Step 22433   [1.736 sec/step, loss=0.75214, avg_loss=0.78318]\n",
            "Step 22434   [1.731 sec/step, loss=0.78466, avg_loss=0.78325]\n",
            "Step 22435   [1.714 sec/step, loss=0.80274, avg_loss=0.78327]\n",
            "Step 22436   [1.710 sec/step, loss=0.82537, avg_loss=0.78426]\n",
            "Step 22437   [1.715 sec/step, loss=0.78357, avg_loss=0.78415]\n",
            "Step 22438   [1.713 sec/step, loss=0.84643, avg_loss=0.78440]\n",
            "Step 22439   [1.712 sec/step, loss=0.80254, avg_loss=0.78480]\n",
            "Step 22440   [1.710 sec/step, loss=0.83502, avg_loss=0.78489]\n",
            "Step 22441   [1.711 sec/step, loss=0.78000, avg_loss=0.78473]\n",
            "Step 22442   [1.711 sec/step, loss=0.75484, avg_loss=0.78458]\n",
            "Step 22443   [1.714 sec/step, loss=0.79377, avg_loss=0.78483]\n",
            "Step 22444   [1.713 sec/step, loss=0.77560, avg_loss=0.78445]\n",
            "Step 22445   [1.714 sec/step, loss=0.84245, avg_loss=0.78507]\n",
            "Step 22446   [1.716 sec/step, loss=0.82970, avg_loss=0.78577]\n",
            "Step 22447   [1.722 sec/step, loss=0.81035, avg_loss=0.78641]\n",
            "Step 22448   [1.706 sec/step, loss=0.80464, avg_loss=0.78611]\n",
            "Step 22449   [1.704 sec/step, loss=0.79662, avg_loss=0.78556]\n",
            "Step 22450   [1.703 sec/step, loss=0.77359, avg_loss=0.78576]\n",
            "Step 22451   [1.697 sec/step, loss=0.78213, avg_loss=0.78549]\n",
            "Step 22452   [1.696 sec/step, loss=0.76102, avg_loss=0.78451]\n",
            "Generated 32 batches of size 8 in 10.028 sec\n",
            "Step 22453   [1.691 sec/step, loss=0.80137, avg_loss=0.78456]\n",
            "Step 22454   [1.693 sec/step, loss=0.83948, avg_loss=0.78590]\n",
            "Step 22455   [1.695 sec/step, loss=0.74361, avg_loss=0.78532]\n",
            "Step 22456   [1.694 sec/step, loss=0.81536, avg_loss=0.78572]\n",
            "Step 22457   [1.651 sec/step, loss=0.79566, avg_loss=0.78601]\n",
            "Step 22458   [1.653 sec/step, loss=0.74868, avg_loss=0.78526]\n",
            "Step 22459   [1.646 sec/step, loss=0.79259, avg_loss=0.78569]\n",
            "Step 22460   [1.645 sec/step, loss=0.78131, avg_loss=0.78539]\n",
            "Step 22461   [1.644 sec/step, loss=0.82339, avg_loss=0.78575]\n",
            "Step 22462   [1.645 sec/step, loss=0.79078, avg_loss=0.78566]\n",
            "Step 22463   [1.639 sec/step, loss=0.81331, avg_loss=0.78579]\n",
            "Step 22464   [1.639 sec/step, loss=0.74537, avg_loss=0.78521]\n",
            "Step 22465   [1.640 sec/step, loss=0.79277, avg_loss=0.78529]\n",
            "Step 22466   [1.645 sec/step, loss=0.83215, avg_loss=0.78579]\n",
            "Step 22467   [1.604 sec/step, loss=0.88450, avg_loss=0.78746]\n",
            "Step 22468   [1.595 sec/step, loss=0.75407, avg_loss=0.78734]\n",
            "Step 22469   [1.594 sec/step, loss=0.73272, avg_loss=0.78662]\n",
            "Step 22470   [1.595 sec/step, loss=0.83460, avg_loss=0.78754]\n",
            "Step 22471   [1.600 sec/step, loss=0.77904, avg_loss=0.78769]\n",
            "Step 22472   [1.582 sec/step, loss=0.78435, avg_loss=0.78813]\n",
            "Step 22473   [1.584 sec/step, loss=0.78558, avg_loss=0.78770]\n",
            "Step 22474   [1.588 sec/step, loss=0.74244, avg_loss=0.78768]\n",
            "Step 22475   [1.591 sec/step, loss=0.79676, avg_loss=0.78805]\n",
            "Step 22476   [1.624 sec/step, loss=0.81556, avg_loss=0.78820]\n",
            "Step 22477   [1.620 sec/step, loss=0.77923, avg_loss=0.78853]\n",
            "Step 22478   [1.623 sec/step, loss=0.83447, avg_loss=0.78889]\n",
            "Step 22479   [1.618 sec/step, loss=0.77764, avg_loss=0.78871]\n",
            "Step 22480   [1.613 sec/step, loss=0.75655, avg_loss=0.78838]\n",
            "Step 22481   [1.620 sec/step, loss=0.80030, avg_loss=0.78879]\n",
            "Step 22482   [1.618 sec/step, loss=0.76552, avg_loss=0.78820]\n",
            "Step 22483   [1.627 sec/step, loss=0.79992, avg_loss=0.78891]\n",
            "Step 22484   [1.620 sec/step, loss=0.77905, avg_loss=0.78885]\n",
            "Generated 32 batches of size 8 in 10.152 sec\n",
            "Step 22485   [1.625 sec/step, loss=0.73482, avg_loss=0.78864]\n",
            "Step 22486   [1.630 sec/step, loss=0.81602, avg_loss=0.78845]\n",
            "Step 22487   [1.623 sec/step, loss=0.82294, avg_loss=0.78899]\n",
            "Step 22488   [1.623 sec/step, loss=0.76842, avg_loss=0.78882]\n",
            "Step 22489   [1.622 sec/step, loss=0.79319, avg_loss=0.78903]\n",
            "Step 22490   [1.629 sec/step, loss=0.73719, avg_loss=0.78841]\n",
            "Step 22491   [1.628 sec/step, loss=0.81022, avg_loss=0.78845]\n",
            "Step 22492   [1.620 sec/step, loss=0.84663, avg_loss=0.78902]\n",
            "Step 22493   [1.620 sec/step, loss=0.78741, avg_loss=0.78905]\n",
            "Step 22494   [1.621 sec/step, loss=0.83144, avg_loss=0.78965]\n",
            "Step 22495   [1.618 sec/step, loss=0.78612, avg_loss=0.78989]\n",
            "Step 22496   [1.598 sec/step, loss=0.82992, avg_loss=0.79041]\n",
            "Step 22497   [1.593 sec/step, loss=0.74842, avg_loss=0.78961]\n",
            "Step 22498   [1.635 sec/step, loss=0.65068, avg_loss=0.78814]\n",
            "Step 22499   [1.646 sec/step, loss=0.73796, avg_loss=0.78783]\n",
            "Step 22500   [1.633 sec/step, loss=0.77397, avg_loss=0.78790]\n",
            "Writing summary at step: 22500\n",
            "Step 22501   [1.623 sec/step, loss=0.75339, avg_loss=0.78739]\n",
            "Step 22502   [1.636 sec/step, loss=0.81259, avg_loss=0.78817]\n",
            "Step 22503   [1.637 sec/step, loss=0.77381, avg_loss=0.78814]\n",
            "Step 22504   [1.642 sec/step, loss=0.74879, avg_loss=0.78760]\n",
            "Step 22505   [1.639 sec/step, loss=0.82725, avg_loss=0.78777]\n",
            "Step 22506   [1.643 sec/step, loss=0.80224, avg_loss=0.78819]\n",
            "Step 22507   [1.649 sec/step, loss=0.81251, avg_loss=0.78848]\n",
            "Step 22508   [1.665 sec/step, loss=0.71313, avg_loss=0.78764]\n",
            "Step 22509   [1.654 sec/step, loss=0.75613, avg_loss=0.78761]\n",
            "Step 22510   [1.659 sec/step, loss=0.80414, avg_loss=0.78839]\n",
            "Step 22511   [1.665 sec/step, loss=0.74767, avg_loss=0.78812]\n",
            "Step 22512   [1.664 sec/step, loss=0.85093, avg_loss=0.78880]\n",
            "Step 22513   [1.665 sec/step, loss=0.76877, avg_loss=0.78861]\n",
            "Step 22514   [1.672 sec/step, loss=0.77296, avg_loss=0.78898]\n",
            "Step 22515   [1.622 sec/step, loss=0.79885, avg_loss=0.78953]\n",
            "Step 22516   [1.617 sec/step, loss=0.74586, avg_loss=0.78923]\n",
            "Generated 32 batches of size 8 in 10.305 sec\n",
            "Step 22517   [1.622 sec/step, loss=0.75918, avg_loss=0.78856]\n",
            "Step 22518   [1.621 sec/step, loss=0.78418, avg_loss=0.78816]\n",
            "Step 22519   [1.617 sec/step, loss=0.79953, avg_loss=0.78809]\n",
            "Step 22520   [1.621 sec/step, loss=0.80111, avg_loss=0.78798]\n",
            "Step 22521   [1.611 sec/step, loss=0.81309, avg_loss=0.78781]\n",
            "Step 22522   [1.612 sec/step, loss=0.83329, avg_loss=0.78820]\n",
            "Step 22523   [1.607 sec/step, loss=0.76233, avg_loss=0.78786]\n",
            "Step 22524   [1.609 sec/step, loss=0.80885, avg_loss=0.78831]\n",
            "Step 22525   [1.600 sec/step, loss=0.82155, avg_loss=0.78839]\n",
            "Step 22526   [1.608 sec/step, loss=0.76712, avg_loss=0.78803]\n",
            "Step 22527   [1.638 sec/step, loss=0.77304, avg_loss=0.78758]\n",
            "Step 22528   [1.622 sec/step, loss=0.80702, avg_loss=0.78778]\n",
            "Step 22529   [1.637 sec/step, loss=0.79384, avg_loss=0.78795]\n",
            "Step 22530   [1.632 sec/step, loss=0.78965, avg_loss=0.78793]\n",
            "Step 22531   [1.642 sec/step, loss=0.75105, avg_loss=0.78730]\n",
            "Step 22532   [1.605 sec/step, loss=0.81184, avg_loss=0.78896]\n",
            "Step 22533   [1.591 sec/step, loss=0.79144, avg_loss=0.78936]\n",
            "Step 22534   [1.587 sec/step, loss=0.80900, avg_loss=0.78960]\n",
            "Step 22535   [1.589 sec/step, loss=0.80104, avg_loss=0.78958]\n",
            "Step 22536   [1.590 sec/step, loss=0.79603, avg_loss=0.78929]\n",
            "Step 22537   [1.587 sec/step, loss=0.79116, avg_loss=0.78937]\n",
            "Step 22538   [1.589 sec/step, loss=0.75576, avg_loss=0.78846]\n",
            "Step 22539   [1.593 sec/step, loss=0.78994, avg_loss=0.78833]\n",
            "Step 22540   [1.599 sec/step, loss=0.84299, avg_loss=0.78841]\n",
            "Step 22541   [1.596 sec/step, loss=0.76382, avg_loss=0.78825]\n",
            "Step 22542   [1.600 sec/step, loss=0.75710, avg_loss=0.78827]\n",
            "Step 22543   [1.609 sec/step, loss=0.82428, avg_loss=0.78858]\n",
            "Step 22544   [1.640 sec/step, loss=0.88164, avg_loss=0.78964]\n",
            "Generated 32 batches of size 8 in 9.480 sec\n",
            "Step 22545   [1.650 sec/step, loss=0.74217, avg_loss=0.78864]\n",
            "Step 22546   [1.644 sec/step, loss=0.79173, avg_loss=0.78826]\n",
            "Step 22547   [1.645 sec/step, loss=0.70972, avg_loss=0.78725]\n",
            "Step 22548   [1.640 sec/step, loss=0.76982, avg_loss=0.78690]\n",
            "Step 22549   [1.639 sec/step, loss=0.79516, avg_loss=0.78689]\n",
            "Step 22550   [1.640 sec/step, loss=0.79453, avg_loss=0.78710]\n",
            "Step 22551   [1.633 sec/step, loss=0.85071, avg_loss=0.78778]\n",
            "Step 22552   [1.626 sec/step, loss=0.78755, avg_loss=0.78805]\n",
            "Step 22553   [1.625 sec/step, loss=0.78347, avg_loss=0.78787]\n",
            "Step 22554   [1.620 sec/step, loss=0.78646, avg_loss=0.78734]\n",
            "Step 22555   [1.617 sec/step, loss=0.76688, avg_loss=0.78757]\n",
            "Step 22556   [1.622 sec/step, loss=0.79515, avg_loss=0.78737]\n",
            "Step 22557   [1.620 sec/step, loss=0.82689, avg_loss=0.78768]\n",
            "Step 22558   [1.608 sec/step, loss=0.79970, avg_loss=0.78819]\n",
            "Step 22559   [1.606 sec/step, loss=0.77521, avg_loss=0.78802]\n",
            "Step 22560   [1.602 sec/step, loss=0.79298, avg_loss=0.78813]\n",
            "Step 22561   [1.609 sec/step, loss=0.75366, avg_loss=0.78744]\n",
            "Step 22562   [1.616 sec/step, loss=0.81687, avg_loss=0.78770]\n",
            "Step 22563   [1.610 sec/step, loss=0.80001, avg_loss=0.78757]\n",
            "Step 22564   [1.619 sec/step, loss=0.79152, avg_loss=0.78803]\n",
            "Step 22565   [1.616 sec/step, loss=0.78997, avg_loss=0.78800]\n",
            "Step 22566   [1.610 sec/step, loss=0.78966, avg_loss=0.78757]\n",
            "Step 22567   [1.607 sec/step, loss=0.78065, avg_loss=0.78654]\n",
            "Step 22568   [1.603 sec/step, loss=0.81327, avg_loss=0.78713]\n",
            "Step 22569   [1.605 sec/step, loss=0.80408, avg_loss=0.78784]\n",
            "Step 22570   [1.605 sec/step, loss=0.80037, avg_loss=0.78750]\n",
            "Step 22571   [1.608 sec/step, loss=0.75773, avg_loss=0.78729]\n",
            "Step 22572   [1.622 sec/step, loss=0.75527, avg_loss=0.78699]\n",
            "Step 22573   [1.664 sec/step, loss=0.63210, avg_loss=0.78546]\n",
            "Step 22574   [1.683 sec/step, loss=0.76980, avg_loss=0.78573]\n",
            "Step 22575   [1.697 sec/step, loss=0.75052, avg_loss=0.78527]\n",
            "Generated 32 batches of size 8 in 6.929 sec\n",
            "Step 22576   [1.668 sec/step, loss=0.78727, avg_loss=0.78499]\n",
            "Step 22577   [1.674 sec/step, loss=0.73867, avg_loss=0.78458]\n",
            "Step 22578   [1.686 sec/step, loss=0.85640, avg_loss=0.78480]\n",
            "Step 22579   [1.690 sec/step, loss=0.73157, avg_loss=0.78434]\n",
            "Step 22580   [1.692 sec/step, loss=0.80484, avg_loss=0.78482]\n",
            "Step 22581   [1.689 sec/step, loss=0.75746, avg_loss=0.78440]\n",
            "Step 22582   [1.690 sec/step, loss=0.79710, avg_loss=0.78471]\n",
            "Step 22583   [1.683 sec/step, loss=0.77719, avg_loss=0.78448]\n",
            "Step 22584   [1.683 sec/step, loss=0.72808, avg_loss=0.78397]\n",
            "Step 22585   [1.667 sec/step, loss=0.76876, avg_loss=0.78431]\n",
            "Step 22586   [1.655 sec/step, loss=0.76848, avg_loss=0.78384]\n",
            "Step 22587   [1.655 sec/step, loss=0.79148, avg_loss=0.78352]\n",
            "Step 22588   [1.653 sec/step, loss=0.77994, avg_loss=0.78364]\n",
            "Step 22589   [1.654 sec/step, loss=0.80697, avg_loss=0.78378]\n",
            "Step 22590   [1.647 sec/step, loss=0.76978, avg_loss=0.78410]\n",
            "Step 22591   [1.652 sec/step, loss=0.80265, avg_loss=0.78403]\n",
            "Step 22592   [1.665 sec/step, loss=0.78324, avg_loss=0.78339]\n",
            "Step 22593   [1.680 sec/step, loss=0.78799, avg_loss=0.78340]\n",
            "Step 22594   [1.682 sec/step, loss=0.74272, avg_loss=0.78251]\n",
            "Step 22595   [1.679 sec/step, loss=0.75870, avg_loss=0.78224]\n",
            "Step 22596   [1.690 sec/step, loss=0.79257, avg_loss=0.78186]\n",
            "Step 22597   [1.687 sec/step, loss=0.78095, avg_loss=0.78219]\n",
            "Step 22598   [1.655 sec/step, loss=0.77907, avg_loss=0.78347]\n",
            "Step 22599   [1.647 sec/step, loss=0.76804, avg_loss=0.78377]\n",
            "Step 22600   [1.646 sec/step, loss=0.77808, avg_loss=0.78381]\n",
            "Writing summary at step: 22600\n",
            "Step 22601   [1.665 sec/step, loss=0.81195, avg_loss=0.78440]\n",
            "Step 22602   [1.650 sec/step, loss=0.79403, avg_loss=0.78421]\n",
            "Step 22603   [1.646 sec/step, loss=0.76721, avg_loss=0.78415]\n",
            "Step 22604   [1.645 sec/step, loss=0.81685, avg_loss=0.78483]\n",
            "Step 22605   [1.653 sec/step, loss=0.82305, avg_loss=0.78479]\n",
            "Step 22606   [1.645 sec/step, loss=0.76589, avg_loss=0.78442]\n",
            "Generated 32 batches of size 8 in 7.008 sec\n",
            "Step 22607   [1.685 sec/step, loss=0.75110, avg_loss=0.78381]\n",
            "Step 22608   [1.671 sec/step, loss=0.80964, avg_loss=0.78478]\n",
            "Step 22609   [1.672 sec/step, loss=0.77434, avg_loss=0.78496]\n",
            "Step 22610   [1.675 sec/step, loss=0.78718, avg_loss=0.78479]\n",
            "Step 22611   [1.664 sec/step, loss=0.82408, avg_loss=0.78555]\n",
            "Step 22612   [1.668 sec/step, loss=0.70938, avg_loss=0.78414]\n",
            "Step 22613   [1.678 sec/step, loss=0.76004, avg_loss=0.78405]\n",
            "Step 22614   [1.672 sec/step, loss=0.81960, avg_loss=0.78452]\n",
            "Step 22615   [1.671 sec/step, loss=0.78590, avg_loss=0.78439]\n",
            "Step 22616   [1.669 sec/step, loss=0.79633, avg_loss=0.78489]\n",
            "Step 22617   [1.667 sec/step, loss=0.74014, avg_loss=0.78470]\n",
            "Step 22618   [1.675 sec/step, loss=0.78567, avg_loss=0.78471]\n",
            "Step 22619   [1.685 sec/step, loss=0.79381, avg_loss=0.78466]\n",
            "Step 22620   [1.687 sec/step, loss=0.75970, avg_loss=0.78424]\n",
            "Step 22621   [1.698 sec/step, loss=0.78501, avg_loss=0.78396]\n",
            "Step 22622   [1.700 sec/step, loss=0.79262, avg_loss=0.78356]\n",
            "Step 22623   [1.704 sec/step, loss=0.80211, avg_loss=0.78395]\n",
            "Step 22624   [1.699 sec/step, loss=0.76893, avg_loss=0.78355]\n",
            "Step 22625   [1.702 sec/step, loss=0.81353, avg_loss=0.78347]\n",
            "Step 22626   [1.696 sec/step, loss=0.74942, avg_loss=0.78330]\n",
            "Step 22627   [1.672 sec/step, loss=0.71533, avg_loss=0.78272]\n",
            "Step 22628   [1.678 sec/step, loss=0.75622, avg_loss=0.78221]\n",
            "Step 22629   [1.671 sec/step, loss=0.76440, avg_loss=0.78192]\n",
            "Step 22630   [1.677 sec/step, loss=0.76427, avg_loss=0.78166]\n",
            "Step 22631   [1.661 sec/step, loss=0.73930, avg_loss=0.78155]\n",
            "Step 22632   [1.681 sec/step, loss=0.78178, avg_loss=0.78125]\n",
            "Step 22633   [1.688 sec/step, loss=0.81940, avg_loss=0.78153]\n",
            "Step 22634   [1.687 sec/step, loss=0.74857, avg_loss=0.78092]\n",
            "Step 22635   [1.685 sec/step, loss=0.81133, avg_loss=0.78102]\n",
            "Step 22636   [1.684 sec/step, loss=0.80879, avg_loss=0.78115]\n",
            "Step 22637   [1.684 sec/step, loss=0.75075, avg_loss=0.78075]\n",
            "Step 22638   [1.684 sec/step, loss=0.77748, avg_loss=0.78097]\n",
            "Step 22639   [1.689 sec/step, loss=0.75159, avg_loss=0.78058]\n",
            "Step 22640   [1.687 sec/step, loss=0.78926, avg_loss=0.78004]\n",
            "Generated 32 batches of size 8 in 7.927 sec\n",
            "Step 22641   [1.734 sec/step, loss=0.74848, avg_loss=0.77989]\n",
            "Step 22642   [1.726 sec/step, loss=0.75231, avg_loss=0.77984]\n",
            "Step 22643   [1.711 sec/step, loss=0.79657, avg_loss=0.77957]\n",
            "Step 22644   [1.680 sec/step, loss=0.80548, avg_loss=0.77880]\n",
            "Step 22645   [1.672 sec/step, loss=0.76771, avg_loss=0.77906]\n",
            "Step 22646   [1.678 sec/step, loss=0.74506, avg_loss=0.77859]\n",
            "Step 22647   [1.682 sec/step, loss=0.73820, avg_loss=0.77888]\n",
            "Step 22648   [1.684 sec/step, loss=0.83001, avg_loss=0.77948]\n",
            "Step 22649   [1.695 sec/step, loss=0.86402, avg_loss=0.78017]\n",
            "Step 22650   [1.694 sec/step, loss=0.81633, avg_loss=0.78039]\n",
            "Step 22651   [1.694 sec/step, loss=0.78243, avg_loss=0.77970]\n",
            "Step 22652   [1.693 sec/step, loss=0.77283, avg_loss=0.77956]\n",
            "Step 22653   [1.691 sec/step, loss=0.80242, avg_loss=0.77975]\n",
            "Step 22654   [1.698 sec/step, loss=0.82147, avg_loss=0.78010]\n",
            "Step 22655   [1.696 sec/step, loss=0.80531, avg_loss=0.78048]\n",
            "Step 22656   [1.693 sec/step, loss=0.78558, avg_loss=0.78038]\n",
            "Step 22657   [1.698 sec/step, loss=0.81560, avg_loss=0.78027]\n",
            "Step 22658   [1.698 sec/step, loss=0.75130, avg_loss=0.77979]\n",
            "Step 22659   [1.710 sec/step, loss=0.77205, avg_loss=0.77976]\n",
            "Step 22660   [1.752 sec/step, loss=0.64660, avg_loss=0.77829]\n",
            "Step 22661   [1.756 sec/step, loss=0.76886, avg_loss=0.77844]\n",
            "Step 22662   [1.743 sec/step, loss=0.80612, avg_loss=0.77834]\n",
            "Step 22663   [1.748 sec/step, loss=0.75673, avg_loss=0.77790]\n",
            "Step 22664   [1.741 sec/step, loss=0.77136, avg_loss=0.77770]\n",
            "Step 22665   [1.740 sec/step, loss=0.79133, avg_loss=0.77772]\n",
            "Step 22666   [1.743 sec/step, loss=0.82639, avg_loss=0.77808]\n",
            "Step 22667   [1.747 sec/step, loss=0.74843, avg_loss=0.77776]\n",
            "Step 22668   [1.746 sec/step, loss=0.80227, avg_loss=0.77765]\n",
            "Step 22669   [1.751 sec/step, loss=0.81000, avg_loss=0.77771]\n",
            "Step 22670   [1.769 sec/step, loss=0.73503, avg_loss=0.77706]\n",
            "Step 22671   [1.770 sec/step, loss=0.80461, avg_loss=0.77753]\n",
            "Generated 32 batches of size 8 in 7.726 sec\n",
            "Step 22672   [1.757 sec/step, loss=0.81220, avg_loss=0.77809]\n",
            "Step 22673   [1.712 sec/step, loss=0.72329, avg_loss=0.77901]\n",
            "Step 22674   [1.683 sec/step, loss=0.74517, avg_loss=0.77876]\n",
            "Step 22675   [1.672 sec/step, loss=0.73153, avg_loss=0.77857]\n",
            "Step 22676   [1.674 sec/step, loss=0.81060, avg_loss=0.77880]\n",
            "Step 22677   [1.675 sec/step, loss=0.78475, avg_loss=0.77926]\n",
            "Step 22678   [1.663 sec/step, loss=0.81396, avg_loss=0.77884]\n",
            "Step 22679   [1.656 sec/step, loss=0.80457, avg_loss=0.77957]\n",
            "Step 22680   [1.657 sec/step, loss=0.76417, avg_loss=0.77916]\n",
            "Step 22681   [1.654 sec/step, loss=0.78902, avg_loss=0.77948]\n",
            "Step 22682   [1.652 sec/step, loss=0.79599, avg_loss=0.77947]\n",
            "Step 22683   [1.648 sec/step, loss=0.82104, avg_loss=0.77991]\n",
            "Step 22684   [1.648 sec/step, loss=0.80168, avg_loss=0.78064]\n",
            "Step 22685   [1.661 sec/step, loss=0.79600, avg_loss=0.78092]\n",
            "Step 22686   [1.660 sec/step, loss=0.81798, avg_loss=0.78141]\n",
            "Step 22687   [1.669 sec/step, loss=0.78740, avg_loss=0.78137]\n",
            "Step 22688   [1.677 sec/step, loss=0.79409, avg_loss=0.78151]\n",
            "Step 22689   [1.687 sec/step, loss=0.81090, avg_loss=0.78155]\n",
            "Step 22690   [1.690 sec/step, loss=0.80975, avg_loss=0.78195]\n",
            "Step 22691   [1.685 sec/step, loss=0.77406, avg_loss=0.78166]\n",
            "Step 22692   [1.668 sec/step, loss=0.72474, avg_loss=0.78108]\n",
            "Step 22693   [1.666 sec/step, loss=0.77345, avg_loss=0.78093]\n",
            "Step 22694   [1.662 sec/step, loss=0.79729, avg_loss=0.78148]\n",
            "Step 22695   [1.667 sec/step, loss=0.78431, avg_loss=0.78174]\n",
            "Step 22696   [1.659 sec/step, loss=0.76477, avg_loss=0.78146]\n",
            "Step 22697   [1.660 sec/step, loss=0.79302, avg_loss=0.78158]\n",
            "Step 22698   [1.655 sec/step, loss=0.76111, avg_loss=0.78140]\n",
            "Step 22699   [1.658 sec/step, loss=0.77324, avg_loss=0.78145]\n",
            "Step 22700   [1.700 sec/step, loss=0.60504, avg_loss=0.77972]\n",
            "Writing summary at step: 22700\n",
            "Step 22701   [1.687 sec/step, loss=0.81601, avg_loss=0.77976]\n",
            "Step 22702   [1.688 sec/step, loss=0.81047, avg_loss=0.77993]\n",
            "Step 22703   [1.689 sec/step, loss=0.77346, avg_loss=0.77999]\n",
            "Generated 32 batches of size 8 in 9.251 sec\n",
            "Step 22704   [1.706 sec/step, loss=0.83177, avg_loss=0.78014]\n",
            "Step 22705   [1.701 sec/step, loss=0.80367, avg_loss=0.77994]\n",
            "Step 22706   [1.706 sec/step, loss=0.70827, avg_loss=0.77937]\n",
            "Step 22707   [1.658 sec/step, loss=0.77975, avg_loss=0.77965]\n",
            "Step 22708   [1.673 sec/step, loss=0.67795, avg_loss=0.77834]\n",
            "Step 22709   [1.676 sec/step, loss=0.77054, avg_loss=0.77830]\n",
            "Step 22710   [1.680 sec/step, loss=0.75892, avg_loss=0.77802]\n",
            "Step 22711   [1.695 sec/step, loss=0.75955, avg_loss=0.77737]\n",
            "Step 22712   [1.694 sec/step, loss=0.78813, avg_loss=0.77816]\n",
            "Step 22713   [1.688 sec/step, loss=0.79758, avg_loss=0.77853]\n",
            "Step 22714   [1.688 sec/step, loss=0.78131, avg_loss=0.77815]\n",
            "Step 22715   [1.684 sec/step, loss=0.75111, avg_loss=0.77780]\n",
            "Step 22716   [1.685 sec/step, loss=0.80323, avg_loss=0.77787]\n",
            "Step 22717   [1.689 sec/step, loss=0.81781, avg_loss=0.77865]\n",
            "Step 22718   [1.679 sec/step, loss=0.81179, avg_loss=0.77891]\n",
            "Step 22719   [1.679 sec/step, loss=0.77074, avg_loss=0.77868]\n",
            "Step 22720   [1.674 sec/step, loss=0.75635, avg_loss=0.77865]\n",
            "Step 22721   [1.663 sec/step, loss=0.77108, avg_loss=0.77851]\n",
            "Step 22722   [1.660 sec/step, loss=0.81030, avg_loss=0.77868]\n",
            "Step 22723   [1.661 sec/step, loss=0.78474, avg_loss=0.77851]\n",
            "Step 22724   [1.669 sec/step, loss=0.76415, avg_loss=0.77846]\n",
            "Step 22725   [1.670 sec/step, loss=0.72516, avg_loss=0.77758]\n",
            "Step 22726   [1.672 sec/step, loss=0.80833, avg_loss=0.77817]\n",
            "Step 22727   [1.665 sec/step, loss=0.82766, avg_loss=0.77929]\n",
            "Step 22728   [1.655 sec/step, loss=0.76468, avg_loss=0.77937]\n",
            "Step 22729   [1.652 sec/step, loss=0.80443, avg_loss=0.77978]\n",
            "Step 22730   [1.648 sec/step, loss=0.77099, avg_loss=0.77984]\n",
            "Step 22731   [1.649 sec/step, loss=0.77443, avg_loss=0.78019]\n",
            "Step 22732   [1.629 sec/step, loss=0.75999, avg_loss=0.77998]\n",
            "Step 22733   [1.632 sec/step, loss=0.73913, avg_loss=0.77917]\n",
            "Step 22734   [1.664 sec/step, loss=0.72569, avg_loss=0.77894]\n",
            "Step 22735   [1.668 sec/step, loss=0.81240, avg_loss=0.77895]\n",
            "Generated 32 batches of size 8 in 9.223 sec\n",
            "Step 22736   [1.669 sec/step, loss=0.80014, avg_loss=0.77887]\n",
            "Step 22737   [1.707 sec/step, loss=0.76276, avg_loss=0.77899]\n",
            "Step 22738   [1.711 sec/step, loss=0.74770, avg_loss=0.77869]\n",
            "Step 22739   [1.701 sec/step, loss=0.79737, avg_loss=0.77915]\n",
            "Step 22740   [1.699 sec/step, loss=0.78514, avg_loss=0.77911]\n",
            "Step 22741   [1.655 sec/step, loss=0.77563, avg_loss=0.77938]\n",
            "Step 22742   [1.654 sec/step, loss=0.77777, avg_loss=0.77963]\n",
            "Step 22743   [1.660 sec/step, loss=0.78726, avg_loss=0.77954]\n",
            "Step 22744   [1.671 sec/step, loss=0.76769, avg_loss=0.77916]\n",
            "Step 22745   [1.667 sec/step, loss=0.76975, avg_loss=0.77918]\n",
            "Step 22746   [1.678 sec/step, loss=0.81391, avg_loss=0.77987]\n",
            "Step 22747   [1.672 sec/step, loss=0.72951, avg_loss=0.77978]\n",
            "Step 22748   [1.676 sec/step, loss=0.75426, avg_loss=0.77903]\n",
            "Step 22749   [1.673 sec/step, loss=0.81953, avg_loss=0.77858]\n",
            "Step 22750   [1.675 sec/step, loss=0.82515, avg_loss=0.77867]\n",
            "Step 22751   [1.704 sec/step, loss=0.69594, avg_loss=0.77781]\n",
            "Step 22752   [1.705 sec/step, loss=0.80738, avg_loss=0.77815]\n",
            "Step 22753   [1.711 sec/step, loss=0.79366, avg_loss=0.77806]\n",
            "Step 22754   [1.703 sec/step, loss=0.74816, avg_loss=0.77733]\n",
            "Step 22755   [1.704 sec/step, loss=0.80571, avg_loss=0.77733]\n",
            "Step 22756   [1.703 sec/step, loss=0.75973, avg_loss=0.77708]\n",
            "Step 22757   [1.701 sec/step, loss=0.77142, avg_loss=0.77663]\n",
            "Step 22758   [1.699 sec/step, loss=0.68664, avg_loss=0.77599]\n",
            "Step 22759   [1.687 sec/step, loss=0.72755, avg_loss=0.77554]\n",
            "Step 22760   [1.647 sec/step, loss=0.76701, avg_loss=0.77675]\n",
            "Step 22761   [1.640 sec/step, loss=0.79155, avg_loss=0.77697]\n",
            "Step 22762   [1.639 sec/step, loss=0.79014, avg_loss=0.77681]\n",
            "Step 22763   [1.639 sec/step, loss=0.82594, avg_loss=0.77751]\n",
            "Step 22764   [1.645 sec/step, loss=0.73416, avg_loss=0.77713]\n",
            "Step 22765   [1.652 sec/step, loss=0.77997, avg_loss=0.77702]\n",
            "Step 22766   [1.657 sec/step, loss=0.82547, avg_loss=0.77701]\n",
            "Step 22767   [1.655 sec/step, loss=0.81899, avg_loss=0.77772]\n",
            "Step 22768   [1.665 sec/step, loss=0.79378, avg_loss=0.77763]\n",
            "Step 22769   [1.661 sec/step, loss=0.80742, avg_loss=0.77761]\n",
            "Generated 32 batches of size 8 in 10.339 sec\n",
            "Step 22770   [1.654 sec/step, loss=0.74079, avg_loss=0.77766]\n",
            "Step 22771   [1.652 sec/step, loss=0.78326, avg_loss=0.77745]\n",
            "Step 22772   [1.660 sec/step, loss=0.82695, avg_loss=0.77760]\n",
            "Step 22773   [1.669 sec/step, loss=0.80208, avg_loss=0.77839]\n",
            "Step 22774   [1.673 sec/step, loss=0.80225, avg_loss=0.77896]\n",
            "Step 22775   [1.668 sec/step, loss=0.81924, avg_loss=0.77983]\n",
            "Step 22776   [1.671 sec/step, loss=0.75852, avg_loss=0.77931]\n",
            "Step 22777   [1.662 sec/step, loss=0.80962, avg_loss=0.77956]\n",
            "Step 22778   [1.676 sec/step, loss=0.76314, avg_loss=0.77905]\n",
            "Step 22779   [1.676 sec/step, loss=0.74509, avg_loss=0.77846]\n",
            "Step 22780   [1.672 sec/step, loss=0.81045, avg_loss=0.77892]\n",
            "Step 22781   [1.673 sec/step, loss=0.83368, avg_loss=0.77937]\n",
            "Step 22782   [1.703 sec/step, loss=0.85586, avg_loss=0.77997]\n",
            "Step 22783   [1.709 sec/step, loss=0.74529, avg_loss=0.77921]\n",
            "Step 22784   [1.714 sec/step, loss=0.78313, avg_loss=0.77902]\n",
            "Step 22785   [1.707 sec/step, loss=0.72847, avg_loss=0.77835]\n",
            "Step 22786   [1.702 sec/step, loss=0.72839, avg_loss=0.77745]\n",
            "Step 22787   [1.696 sec/step, loss=0.75479, avg_loss=0.77713]\n",
            "Step 22788   [1.692 sec/step, loss=0.79043, avg_loss=0.77709]\n",
            "Step 22789   [1.693 sec/step, loss=0.80911, avg_loss=0.77707]\n",
            "Step 22790   [1.693 sec/step, loss=0.73400, avg_loss=0.77631]\n",
            "Step 22791   [1.696 sec/step, loss=0.76398, avg_loss=0.77621]\n",
            "Step 22792   [1.736 sec/step, loss=0.84299, avg_loss=0.77740]\n",
            "Step 22793   [1.721 sec/step, loss=0.81302, avg_loss=0.77779]\n",
            "Step 22794   [1.722 sec/step, loss=0.81838, avg_loss=0.77800]\n",
            "Step 22795   [1.719 sec/step, loss=0.82054, avg_loss=0.77836]\n",
            "Step 22796   [1.723 sec/step, loss=0.83077, avg_loss=0.77902]\n",
            "Step 22797   [1.726 sec/step, loss=0.74594, avg_loss=0.77855]\n",
            "Step 22798   [1.723 sec/step, loss=0.77944, avg_loss=0.77874]\n",
            "Step 22799   [1.729 sec/step, loss=0.79487, avg_loss=0.77895]\n",
            "Step 22800   [1.688 sec/step, loss=0.76466, avg_loss=0.78055]\n",
            "Writing summary at step: 22800\n",
            "Generated 32 batches of size 8 in 9.995 sec\n",
            "Step 22801   [1.682 sec/step, loss=0.76077, avg_loss=0.78000]\n",
            "Step 22802   [1.677 sec/step, loss=0.79217, avg_loss=0.77981]\n",
            "Step 22803   [1.696 sec/step, loss=0.76080, avg_loss=0.77969]\n",
            "Step 22804   [1.678 sec/step, loss=0.80645, avg_loss=0.77943]\n",
            "Step 22805   [1.678 sec/step, loss=0.76995, avg_loss=0.77910]\n",
            "Step 22806   [1.672 sec/step, loss=0.81491, avg_loss=0.78016]\n",
            "Step 22807   [1.673 sec/step, loss=0.82017, avg_loss=0.78057]\n",
            "Step 22808   [1.674 sec/step, loss=0.85592, avg_loss=0.78235]\n",
            "Step 22809   [1.669 sec/step, loss=0.75628, avg_loss=0.78220]\n",
            "Step 22810   [1.660 sec/step, loss=0.81410, avg_loss=0.78276]\n",
            "Step 22811   [1.654 sec/step, loss=0.75327, avg_loss=0.78269]\n",
            "Step 22812   [1.657 sec/step, loss=0.78032, avg_loss=0.78262]\n",
            "Step 22813   [1.652 sec/step, loss=0.80643, avg_loss=0.78270]\n",
            "Step 22814   [1.651 sec/step, loss=0.78969, avg_loss=0.78279]\n",
            "Step 22815   [1.656 sec/step, loss=0.76940, avg_loss=0.78297]\n",
            "Step 22816   [1.669 sec/step, loss=0.86810, avg_loss=0.78362]\n",
            "Step 22817   [1.663 sec/step, loss=0.80932, avg_loss=0.78353]\n",
            "Step 22818   [1.668 sec/step, loss=0.81605, avg_loss=0.78358]\n",
            "Step 22819   [1.661 sec/step, loss=0.81429, avg_loss=0.78401]\n",
            "Step 22820   [1.664 sec/step, loss=0.77922, avg_loss=0.78424]\n",
            "Step 22821   [1.668 sec/step, loss=0.74378, avg_loss=0.78397]\n",
            "Step 22822   [1.670 sec/step, loss=0.83064, avg_loss=0.78417]\n",
            "Step 22823   [1.668 sec/step, loss=0.77795, avg_loss=0.78410]\n",
            "Step 22824   [1.669 sec/step, loss=0.71775, avg_loss=0.78364]\n",
            "Step 22825   [1.668 sec/step, loss=0.81186, avg_loss=0.78451]\n",
            "Step 22826   [1.665 sec/step, loss=0.75447, avg_loss=0.78397]\n",
            "Step 22827   [1.660 sec/step, loss=0.75566, avg_loss=0.78325]\n",
            "Step 22828   [1.663 sec/step, loss=0.75413, avg_loss=0.78314]\n",
            "Step 22829   [1.665 sec/step, loss=0.78430, avg_loss=0.78294]\n",
            "Step 22830   [1.666 sec/step, loss=0.80602, avg_loss=0.78329]\n",
            "Step 22831   [1.678 sec/step, loss=0.78808, avg_loss=0.78343]\n",
            "Step 22832   [1.684 sec/step, loss=0.86934, avg_loss=0.78452]\n",
            "Generated 32 batches of size 8 in 10.307 sec\n",
            "Step 22833   [1.674 sec/step, loss=0.73053, avg_loss=0.78444]\n",
            "Step 22834   [1.642 sec/step, loss=0.76313, avg_loss=0.78481]\n",
            "Step 22835   [1.682 sec/step, loss=0.70069, avg_loss=0.78369]\n",
            "Step 22836   [1.682 sec/step, loss=0.75370, avg_loss=0.78323]\n",
            "Step 22837   [1.683 sec/step, loss=0.67851, avg_loss=0.78239]\n",
            "Step 22838   [1.697 sec/step, loss=0.76984, avg_loss=0.78261]\n",
            "Step 22839   [1.700 sec/step, loss=0.80347, avg_loss=0.78267]\n",
            "Step 22840   [1.705 sec/step, loss=0.76146, avg_loss=0.78243]\n",
            "Step 22841   [1.714 sec/step, loss=0.78042, avg_loss=0.78248]\n",
            "Step 22842   [1.719 sec/step, loss=0.73683, avg_loss=0.78207]\n",
            "Step 22843   [1.710 sec/step, loss=0.80790, avg_loss=0.78228]\n",
            "Step 22844   [1.703 sec/step, loss=0.76489, avg_loss=0.78225]\n",
            "Step 22845   [1.706 sec/step, loss=0.79764, avg_loss=0.78253]\n",
            "Step 22846   [1.691 sec/step, loss=0.80014, avg_loss=0.78239]\n",
            "Step 22847   [1.694 sec/step, loss=0.77713, avg_loss=0.78287]\n",
            "Step 22848   [1.689 sec/step, loss=0.81898, avg_loss=0.78351]\n",
            "Step 22849   [1.677 sec/step, loss=0.76926, avg_loss=0.78301]\n",
            "Step 22850   [1.676 sec/step, loss=0.82799, avg_loss=0.78304]\n",
            "Step 22851   [1.645 sec/step, loss=0.78217, avg_loss=0.78390]\n",
            "Step 22852   [1.650 sec/step, loss=0.77093, avg_loss=0.78354]\n",
            "Step 22853   [1.658 sec/step, loss=0.74133, avg_loss=0.78301]\n",
            "Step 22854   [1.660 sec/step, loss=0.84044, avg_loss=0.78394]\n",
            "Step 22855   [1.660 sec/step, loss=0.77179, avg_loss=0.78360]\n",
            "Step 22856   [1.670 sec/step, loss=0.81390, avg_loss=0.78414]\n",
            "Step 22857   [1.673 sec/step, loss=0.79304, avg_loss=0.78435]\n",
            "Step 22858   [1.673 sec/step, loss=0.86367, avg_loss=0.78612]\n",
            "Step 22859   [1.678 sec/step, loss=0.77942, avg_loss=0.78664]\n",
            "Step 22860   [1.684 sec/step, loss=0.72585, avg_loss=0.78623]\n",
            "Step 22861   [1.686 sec/step, loss=0.80464, avg_loss=0.78636]\n",
            "Step 22862   [1.707 sec/step, loss=0.79909, avg_loss=0.78645]\n",
            "Step 22863   [1.706 sec/step, loss=0.81611, avg_loss=0.78635]\n",
            "Generated 32 batches of size 8 in 10.111 sec\n",
            "Step 22864   [1.709 sec/step, loss=0.81216, avg_loss=0.78713]\n",
            "Step 22865   [1.701 sec/step, loss=0.83060, avg_loss=0.78764]\n",
            "Step 22866   [1.688 sec/step, loss=0.76947, avg_loss=0.78708]\n",
            "Step 22867   [1.689 sec/step, loss=0.77067, avg_loss=0.78660]\n",
            "Step 22868   [1.681 sec/step, loss=0.79411, avg_loss=0.78660]\n",
            "Step 22869   [1.687 sec/step, loss=0.76761, avg_loss=0.78620]\n",
            "Step 22870   [1.674 sec/step, loss=0.85654, avg_loss=0.78736]\n",
            "Step 22871   [1.670 sec/step, loss=0.81268, avg_loss=0.78765]\n",
            "Step 22872   [1.660 sec/step, loss=0.77310, avg_loss=0.78712]\n",
            "Step 22873   [1.654 sec/step, loss=0.77820, avg_loss=0.78688]\n",
            "Step 22874   [1.658 sec/step, loss=0.79462, avg_loss=0.78680]\n",
            "Step 22875   [1.656 sec/step, loss=0.79805, avg_loss=0.78659]\n",
            "Step 22876   [1.660 sec/step, loss=0.84834, avg_loss=0.78749]\n",
            "Step 22877   [1.675 sec/step, loss=0.82657, avg_loss=0.78766]\n",
            "Step 22878   [1.667 sec/step, loss=0.78218, avg_loss=0.78785]\n",
            "Step 22879   [1.679 sec/step, loss=0.73375, avg_loss=0.78773]\n",
            "Step 22880   [1.687 sec/step, loss=0.78409, avg_loss=0.78747]\n",
            "Step 22881   [1.689 sec/step, loss=0.75249, avg_loss=0.78666]\n",
            "Step 22882   [1.659 sec/step, loss=0.80550, avg_loss=0.78615]\n",
            "Step 22883   [1.655 sec/step, loss=0.79042, avg_loss=0.78661]\n",
            "Step 22884   [1.651 sec/step, loss=0.76721, avg_loss=0.78645]\n",
            "Step 22885   [1.650 sec/step, loss=0.81989, avg_loss=0.78736]\n",
            "Step 22886   [1.685 sec/step, loss=0.60020, avg_loss=0.78608]\n",
            "Step 22887   [1.700 sec/step, loss=0.78952, avg_loss=0.78643]\n",
            "Step 22888   [1.700 sec/step, loss=0.74012, avg_loss=0.78592]\n",
            "Step 22889   [1.688 sec/step, loss=0.80581, avg_loss=0.78589]\n",
            "Step 22890   [1.691 sec/step, loss=0.77722, avg_loss=0.78632]\n",
            "Step 22891   [1.692 sec/step, loss=0.81400, avg_loss=0.78682]\n",
            "Step 22892   [1.660 sec/step, loss=0.79107, avg_loss=0.78630]\n",
            "Step 22893   [1.662 sec/step, loss=0.76771, avg_loss=0.78585]\n",
            "Step 22894   [1.665 sec/step, loss=0.80436, avg_loss=0.78571]\n",
            "Step 22895   [1.673 sec/step, loss=0.80869, avg_loss=0.78559]\n",
            "Step 22896   [1.670 sec/step, loss=0.80822, avg_loss=0.78537]\n",
            "Generated 32 batches of size 8 in 9.947 sec\n",
            "Step 22897   [1.667 sec/step, loss=0.76808, avg_loss=0.78559]\n",
            "Step 22898   [1.670 sec/step, loss=0.75641, avg_loss=0.78536]\n",
            "Step 22899   [1.657 sec/step, loss=0.78443, avg_loss=0.78525]\n",
            "Step 22900   [1.672 sec/step, loss=0.80486, avg_loss=0.78565]\n",
            "Writing summary at step: 22900\n",
            "Step 22901   [1.671 sec/step, loss=0.83485, avg_loss=0.78640]\n",
            "Step 22902   [1.676 sec/step, loss=0.76239, avg_loss=0.78610]\n",
            "Step 22903   [1.657 sec/step, loss=0.76030, avg_loss=0.78609]\n",
            "Step 22904   [1.652 sec/step, loss=0.76941, avg_loss=0.78572]\n",
            "Step 22905   [1.650 sec/step, loss=0.80041, avg_loss=0.78603]\n",
            "Step 22906   [1.651 sec/step, loss=0.78771, avg_loss=0.78575]\n",
            "Step 22907   [1.653 sec/step, loss=0.78300, avg_loss=0.78538]\n",
            "Step 22908   [1.633 sec/step, loss=0.78383, avg_loss=0.78466]\n",
            "Step 22909   [1.633 sec/step, loss=0.76835, avg_loss=0.78478]\n",
            "Step 22910   [1.635 sec/step, loss=0.81489, avg_loss=0.78479]\n",
            "Step 22911   [1.633 sec/step, loss=0.72765, avg_loss=0.78453]\n",
            "Step 22912   [1.666 sec/step, loss=0.62539, avg_loss=0.78298]\n",
            "Step 22913   [1.673 sec/step, loss=0.81682, avg_loss=0.78309]\n",
            "Step 22914   [1.674 sec/step, loss=0.78489, avg_loss=0.78304]\n",
            "Step 22915   [1.683 sec/step, loss=0.78634, avg_loss=0.78321]\n",
            "Step 22916   [1.672 sec/step, loss=0.77369, avg_loss=0.78227]\n",
            "Step 22917   [1.667 sec/step, loss=0.76516, avg_loss=0.78182]\n",
            "Step 22918   [1.664 sec/step, loss=0.78015, avg_loss=0.78147]\n",
            "Step 22919   [1.678 sec/step, loss=0.82867, avg_loss=0.78161]\n",
            "Step 22920   [1.679 sec/step, loss=0.80231, avg_loss=0.78184]\n",
            "Step 22921   [1.678 sec/step, loss=0.79717, avg_loss=0.78237]\n",
            "Step 22922   [1.675 sec/step, loss=0.80263, avg_loss=0.78209]\n",
            "Step 22923   [1.684 sec/step, loss=0.75849, avg_loss=0.78190]\n",
            "Step 22924   [1.682 sec/step, loss=0.82198, avg_loss=0.78294]\n",
            "Step 22925   [1.688 sec/step, loss=0.76957, avg_loss=0.78252]\n",
            "Step 22926   [1.689 sec/step, loss=0.78448, avg_loss=0.78282]\n",
            "Step 22927   [1.686 sec/step, loss=0.75569, avg_loss=0.78282]\n",
            "Generated 32 batches of size 8 in 10.311 sec\n",
            "Step 22928   [1.686 sec/step, loss=0.78923, avg_loss=0.78317]\n",
            "Step 22929   [1.682 sec/step, loss=0.80656, avg_loss=0.78339]\n",
            "Step 22930   [1.679 sec/step, loss=0.76675, avg_loss=0.78300]\n",
            "Step 22931   [1.664 sec/step, loss=0.73553, avg_loss=0.78247]\n",
            "Step 22932   [1.655 sec/step, loss=0.79529, avg_loss=0.78173]\n",
            "Step 22933   [1.656 sec/step, loss=0.75634, avg_loss=0.78199]\n",
            "Step 22934   [1.662 sec/step, loss=0.79372, avg_loss=0.78230]\n",
            "Step 22935   [1.625 sec/step, loss=0.84242, avg_loss=0.78372]\n",
            "Step 22936   [1.625 sec/step, loss=0.80844, avg_loss=0.78426]\n",
            "Step 22937   [1.583 sec/step, loss=0.77300, avg_loss=0.78521]\n",
            "Step 22938   [1.584 sec/step, loss=0.81742, avg_loss=0.78568]\n",
            "Step 22939   [1.588 sec/step, loss=0.79624, avg_loss=0.78561]\n",
            "Step 22940   [1.583 sec/step, loss=0.79680, avg_loss=0.78596]\n",
            "Step 22941   [1.572 sec/step, loss=0.79625, avg_loss=0.78612]\n",
            "Step 22942   [1.575 sec/step, loss=0.81138, avg_loss=0.78687]\n",
            "Step 22943   [1.574 sec/step, loss=0.76954, avg_loss=0.78648]\n",
            "Step 22944   [1.575 sec/step, loss=0.67464, avg_loss=0.78558]\n",
            "Step 22945   [1.573 sec/step, loss=0.82974, avg_loss=0.78590]\n",
            "Step 22946   [1.569 sec/step, loss=0.82842, avg_loss=0.78619]\n",
            "Step 22947   [1.565 sec/step, loss=0.76531, avg_loss=0.78607]\n",
            "Step 22948   [1.573 sec/step, loss=0.77527, avg_loss=0.78563]\n",
            "Step 22949   [1.586 sec/step, loss=0.80072, avg_loss=0.78595]\n",
            "Step 22950   [1.585 sec/step, loss=0.79990, avg_loss=0.78566]\n",
            "Step 22951   [1.604 sec/step, loss=0.73818, avg_loss=0.78522]\n",
            "Step 22952   [1.601 sec/step, loss=0.78436, avg_loss=0.78536]\n",
            "Step 22953   [1.590 sec/step, loss=0.74707, avg_loss=0.78542]\n",
            "Step 22954   [1.643 sec/step, loss=0.67127, avg_loss=0.78372]\n",
            "Generated 32 batches of size 8 in 8.038 sec\n",
            "Step 22955   [1.651 sec/step, loss=0.82179, avg_loss=0.78422]\n",
            "Step 22956   [1.641 sec/step, loss=0.84324, avg_loss=0.78452]\n",
            "Step 22957   [1.642 sec/step, loss=0.71393, avg_loss=0.78373]\n",
            "Step 22958   [1.647 sec/step, loss=0.78435, avg_loss=0.78293]\n",
            "Step 22959   [1.643 sec/step, loss=0.79148, avg_loss=0.78305]\n",
            "Step 22960   [1.643 sec/step, loss=0.81780, avg_loss=0.78397]\n",
            "Step 22961   [1.640 sec/step, loss=0.80547, avg_loss=0.78398]\n",
            "Step 22962   [1.620 sec/step, loss=0.80191, avg_loss=0.78401]\n",
            "Step 22963   [1.616 sec/step, loss=0.81010, avg_loss=0.78395]\n",
            "Step 22964   [1.628 sec/step, loss=0.77736, avg_loss=0.78360]\n",
            "Step 22965   [1.663 sec/step, loss=0.77128, avg_loss=0.78301]\n",
            "Step 22966   [1.663 sec/step, loss=0.80076, avg_loss=0.78332]\n",
            "Step 22967   [1.660 sec/step, loss=0.79922, avg_loss=0.78361]\n",
            "Step 22968   [1.677 sec/step, loss=0.65499, avg_loss=0.78222]\n",
            "Step 22969   [1.669 sec/step, loss=0.78587, avg_loss=0.78240]\n",
            "Step 22970   [1.671 sec/step, loss=0.79492, avg_loss=0.78178]\n",
            "Step 22971   [1.683 sec/step, loss=0.83211, avg_loss=0.78198]\n",
            "Step 22972   [1.686 sec/step, loss=0.72095, avg_loss=0.78145]\n",
            "Step 22973   [1.699 sec/step, loss=0.80638, avg_loss=0.78174]\n",
            "Step 22974   [1.693 sec/step, loss=0.80286, avg_loss=0.78182]\n",
            "Step 22975   [1.693 sec/step, loss=0.78314, avg_loss=0.78167]\n",
            "Step 22976   [1.680 sec/step, loss=0.74660, avg_loss=0.78065]\n",
            "Step 22977   [1.668 sec/step, loss=0.73973, avg_loss=0.77978]\n",
            "Step 22978   [1.663 sec/step, loss=0.80186, avg_loss=0.77998]\n",
            "Step 22979   [1.653 sec/step, loss=0.77489, avg_loss=0.78039]\n",
            "Step 22980   [1.655 sec/step, loss=0.76924, avg_loss=0.78024]\n",
            "Step 22981   [1.652 sec/step, loss=0.77895, avg_loss=0.78051]\n",
            "Step 22982   [1.653 sec/step, loss=0.81495, avg_loss=0.78060]\n",
            "Step 22983   [1.656 sec/step, loss=0.76603, avg_loss=0.78036]\n",
            "Step 22984   [1.657 sec/step, loss=0.82520, avg_loss=0.78094]\n",
            "Step 22985   [1.659 sec/step, loss=0.78927, avg_loss=0.78063]\n",
            "Step 22986   [1.629 sec/step, loss=0.82586, avg_loss=0.78289]\n",
            "Step 22987   [1.613 sec/step, loss=0.75022, avg_loss=0.78250]\n",
            "Step 22988   [1.620 sec/step, loss=0.70065, avg_loss=0.78210]\n",
            "Generated 32 batches of size 8 in 6.966 sec\n",
            "Step 22989   [1.629 sec/step, loss=0.78752, avg_loss=0.78192]\n",
            "Step 22990   [1.625 sec/step, loss=0.77258, avg_loss=0.78187]\n",
            "Step 22991   [1.623 sec/step, loss=0.81136, avg_loss=0.78185]\n",
            "Step 22992   [1.623 sec/step, loss=0.79304, avg_loss=0.78187]\n",
            "Step 22993   [1.624 sec/step, loss=0.80513, avg_loss=0.78224]\n",
            "Step 22994   [1.625 sec/step, loss=0.73980, avg_loss=0.78159]\n",
            "Step 22995   [1.616 sec/step, loss=0.81042, avg_loss=0.78161]\n",
            "Step 22996   [1.626 sec/step, loss=0.76505, avg_loss=0.78118]\n",
            "Step 22997   [1.629 sec/step, loss=0.78274, avg_loss=0.78133]\n",
            "Step 22998   [1.629 sec/step, loss=0.75727, avg_loss=0.78134]\n",
            "Step 22999   [1.632 sec/step, loss=0.79941, avg_loss=0.78148]\n",
            "Step 23000   [1.617 sec/step, loss=0.84219, avg_loss=0.78186]\n",
            "Writing summary at step: 23000\n",
            "Saving checkpoint to: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/model.ckpt-23000\n",
            "Step 23001   [1.614 sec/step, loss=0.81187, avg_loss=0.78163]\n",
            "Step 23002   [1.609 sec/step, loss=0.75780, avg_loss=0.78158]\n",
            "Step 23003   [1.617 sec/step, loss=0.82637, avg_loss=0.78224]\n",
            "Step 23004   [1.668 sec/step, loss=0.57027, avg_loss=0.78025]\n",
            "Step 23005   [1.670 sec/step, loss=0.73651, avg_loss=0.77961]\n",
            "Step 23006   [1.688 sec/step, loss=0.70795, avg_loss=0.77882]\n",
            "Step 23007   [1.683 sec/step, loss=0.80356, avg_loss=0.77902]\n",
            "Step 23008   [1.690 sec/step, loss=0.83547, avg_loss=0.77954]\n",
            "Step 23009   [1.689 sec/step, loss=0.77979, avg_loss=0.77965]\n",
            "Step 23010   [1.694 sec/step, loss=0.81435, avg_loss=0.77965]\n",
            "Step 23011   [1.704 sec/step, loss=0.82237, avg_loss=0.78059]\n",
            "Step 23012   [1.666 sec/step, loss=0.77360, avg_loss=0.78208]\n",
            "Step 23013   [1.667 sec/step, loss=0.78409, avg_loss=0.78175]\n",
            "Step 23014   [1.667 sec/step, loss=0.78410, avg_loss=0.78174]\n",
            "Step 23015   [1.660 sec/step, loss=0.76001, avg_loss=0.78148]\n",
            "Step 23016   [1.667 sec/step, loss=0.79668, avg_loss=0.78171]\n",
            "Step 23017   [1.666 sec/step, loss=0.81367, avg_loss=0.78219]\n",
            "Step 23018   [1.668 sec/step, loss=0.76240, avg_loss=0.78201]\n",
            "Step 23019   [1.654 sec/step, loss=0.71141, avg_loss=0.78084]\n",
            "Step 23020   [1.655 sec/step, loss=0.77669, avg_loss=0.78059]\n",
            "Generated 32 batches of size 8 in 7.536 sec\n",
            "Step 23021   [1.661 sec/step, loss=0.78017, avg_loss=0.78042]\n",
            "Step 23022   [1.662 sec/step, loss=0.79222, avg_loss=0.78031]\n",
            "Step 23023   [1.654 sec/step, loss=0.75794, avg_loss=0.78031]\n",
            "Step 23024   [1.654 sec/step, loss=0.75875, avg_loss=0.77967]\n",
            "Step 23025   [1.647 sec/step, loss=0.83397, avg_loss=0.78032]\n",
            "Step 23026   [1.643 sec/step, loss=0.77083, avg_loss=0.78018]\n",
            "Step 23027   [1.652 sec/step, loss=0.83369, avg_loss=0.78096]\n",
            "Step 23028   [1.653 sec/step, loss=0.80198, avg_loss=0.78109]\n",
            "Step 23029   [1.653 sec/step, loss=0.80364, avg_loss=0.78106]\n",
            "Step 23030   [1.656 sec/step, loss=0.81843, avg_loss=0.78158]\n",
            "Step 23031   [1.697 sec/step, loss=0.69599, avg_loss=0.78118]\n",
            "Step 23032   [1.701 sec/step, loss=0.80547, avg_loss=0.78128]\n",
            "Step 23033   [1.719 sec/step, loss=0.83837, avg_loss=0.78210]\n",
            "Step 23034   [1.712 sec/step, loss=0.78716, avg_loss=0.78204]\n",
            "Step 23035   [1.715 sec/step, loss=0.75550, avg_loss=0.78117]\n",
            "Step 23036   [1.715 sec/step, loss=0.77711, avg_loss=0.78086]\n",
            "Step 23037   [1.718 sec/step, loss=0.71577, avg_loss=0.78028]\n",
            "Step 23038   [1.698 sec/step, loss=0.78024, avg_loss=0.77991]\n",
            "Step 23039   [1.686 sec/step, loss=0.85677, avg_loss=0.78052]\n",
            "Step 23040   [1.684 sec/step, loss=0.80785, avg_loss=0.78063]\n",
            "Step 23041   [1.686 sec/step, loss=0.74792, avg_loss=0.78014]\n",
            "Step 23042   [1.681 sec/step, loss=0.75981, avg_loss=0.77963]\n",
            "Step 23043   [1.681 sec/step, loss=0.76070, avg_loss=0.77954]\n",
            "Step 23044   [1.679 sec/step, loss=0.77628, avg_loss=0.78056]\n",
            "Step 23045   [1.682 sec/step, loss=0.75425, avg_loss=0.77980]\n",
            "Step 23046   [1.699 sec/step, loss=0.78861, avg_loss=0.77940]\n",
            "Step 23047   [1.708 sec/step, loss=0.71408, avg_loss=0.77889]\n",
            "Step 23048   [1.701 sec/step, loss=0.75468, avg_loss=0.77868]\n",
            "Step 23049   [1.693 sec/step, loss=0.81313, avg_loss=0.77881]\n",
            "Step 23050   [1.703 sec/step, loss=0.75831, avg_loss=0.77839]\n",
            "Step 23051   [1.689 sec/step, loss=0.76816, avg_loss=0.77869]\n",
            "Step 23052   [1.689 sec/step, loss=0.80717, avg_loss=0.77892]\n",
            "Step 23053   [1.692 sec/step, loss=0.77382, avg_loss=0.77919]\n",
            "Generated 32 batches of size 8 in 8.515 sec\n",
            "Step 23054   [1.641 sec/step, loss=0.78331, avg_loss=0.78031]\n",
            "Step 23055   [1.633 sec/step, loss=0.73198, avg_loss=0.77941]\n",
            "Step 23056   [1.630 sec/step, loss=0.77616, avg_loss=0.77874]\n",
            "Step 23057   [1.631 sec/step, loss=0.77646, avg_loss=0.77937]\n",
            "Step 23058   [1.628 sec/step, loss=0.79374, avg_loss=0.77946]\n",
            "Step 23059   [1.636 sec/step, loss=0.78815, avg_loss=0.77943]\n",
            "Step 23060   [1.631 sec/step, loss=0.73148, avg_loss=0.77856]\n",
            "Step 23061   [1.628 sec/step, loss=0.77141, avg_loss=0.77822]\n",
            "Step 23062   [1.627 sec/step, loss=0.75446, avg_loss=0.77775]\n",
            "Step 23063   [1.626 sec/step, loss=0.80261, avg_loss=0.77767]\n",
            "Step 23064   [1.603 sec/step, loss=0.79007, avg_loss=0.77780]\n",
            "Step 23065   [1.572 sec/step, loss=0.71223, avg_loss=0.77721]\n",
            "Step 23066   [1.573 sec/step, loss=0.75263, avg_loss=0.77673]\n",
            "Step 23067   [1.574 sec/step, loss=0.77989, avg_loss=0.77653]\n",
            "Step 23068   [1.568 sec/step, loss=0.79320, avg_loss=0.77792]\n",
            "Step 23069   [1.568 sec/step, loss=0.73425, avg_loss=0.77740]\n",
            "Step 23070   [1.571 sec/step, loss=0.71851, avg_loss=0.77664]\n",
            "Step 23071   [1.556 sec/step, loss=0.80270, avg_loss=0.77634]\n",
            "Step 23072   [1.558 sec/step, loss=0.78595, avg_loss=0.77699]\n",
            "Step 23073   [1.545 sec/step, loss=0.81158, avg_loss=0.77704]\n",
            "Step 23074   [1.545 sec/step, loss=0.78483, avg_loss=0.77686]\n",
            "Step 23075   [1.570 sec/step, loss=0.74935, avg_loss=0.77653]\n",
            "Step 23076   [1.573 sec/step, loss=0.81716, avg_loss=0.77723]\n",
            "Step 23077   [1.585 sec/step, loss=0.79351, avg_loss=0.77777]\n",
            "Step 23078   [1.591 sec/step, loss=0.79024, avg_loss=0.77765]\n",
            "Step 23079   [1.589 sec/step, loss=0.72260, avg_loss=0.77713]\n",
            "Step 23080   [1.599 sec/step, loss=0.79258, avg_loss=0.77736]\n",
            "Step 23081   [1.610 sec/step, loss=0.73359, avg_loss=0.77691]\n",
            "Step 23082   [1.620 sec/step, loss=0.81832, avg_loss=0.77694]\n",
            "Step 23083   [1.626 sec/step, loss=0.70804, avg_loss=0.77636]\n",
            "Step 23084   [1.626 sec/step, loss=0.78909, avg_loss=0.77600]\n",
            "Generated 32 batches of size 8 in 8.753 sec\n",
            "Step 23085   [1.660 sec/step, loss=0.71330, avg_loss=0.77524]\n",
            "Step 23086   [1.656 sec/step, loss=0.84613, avg_loss=0.77545]\n",
            "Step 23087   [1.659 sec/step, loss=0.80046, avg_loss=0.77595]\n",
            "Step 23088   [1.660 sec/step, loss=0.74829, avg_loss=0.77642]\n",
            "Step 23089   [1.652 sec/step, loss=0.79070, avg_loss=0.77646]\n",
            "Step 23090   [1.652 sec/step, loss=0.79452, avg_loss=0.77668]\n",
            "Step 23091   [1.659 sec/step, loss=0.83712, avg_loss=0.77693]\n",
            "Step 23092   [1.660 sec/step, loss=0.77335, avg_loss=0.77674]\n",
            "Step 23093   [1.657 sec/step, loss=0.78479, avg_loss=0.77653]\n",
            "Step 23094   [1.658 sec/step, loss=0.75030, avg_loss=0.77664]\n",
            "Step 23095   [1.659 sec/step, loss=0.77011, avg_loss=0.77624]\n",
            "Step 23096   [1.650 sec/step, loss=0.80934, avg_loss=0.77668]\n",
            "Step 23097   [1.649 sec/step, loss=0.78718, avg_loss=0.77672]\n",
            "Step 23098   [1.682 sec/step, loss=0.77616, avg_loss=0.77691]\n",
            "Step 23099   [1.693 sec/step, loss=0.82237, avg_loss=0.77714]\n",
            "Step 23100   [1.693 sec/step, loss=0.78951, avg_loss=0.77661]\n",
            "Writing summary at step: 23100\n",
            "Step 23101   [1.704 sec/step, loss=0.79006, avg_loss=0.77640]\n",
            "Step 23102   [1.707 sec/step, loss=0.75172, avg_loss=0.77634]\n",
            "Step 23103   [1.713 sec/step, loss=0.79508, avg_loss=0.77602]\n",
            "Step 23104   [1.663 sec/step, loss=0.78841, avg_loss=0.77820]\n",
            "Step 23105   [1.664 sec/step, loss=0.81375, avg_loss=0.77898]\n",
            "Step 23106   [1.652 sec/step, loss=0.75396, avg_loss=0.77944]\n",
            "Step 23107   [1.675 sec/step, loss=0.69563, avg_loss=0.77836]\n",
            "Step 23108   [1.668 sec/step, loss=0.79334, avg_loss=0.77794]\n",
            "Step 23109   [1.675 sec/step, loss=0.69067, avg_loss=0.77704]\n",
            "Step 23110   [1.668 sec/step, loss=0.80048, avg_loss=0.77691]\n",
            "Step 23111   [1.660 sec/step, loss=0.73242, avg_loss=0.77601]\n",
            "Step 23112   [1.658 sec/step, loss=0.77668, avg_loss=0.77604]\n",
            "Step 23113   [1.650 sec/step, loss=0.77119, avg_loss=0.77591]\n",
            "Step 23114   [1.660 sec/step, loss=0.73262, avg_loss=0.77539]\n",
            "Step 23115   [1.659 sec/step, loss=0.77254, avg_loss=0.77552]\n",
            "Step 23116   [1.655 sec/step, loss=0.78736, avg_loss=0.77543]\n",
            "Step 23117   [1.660 sec/step, loss=0.82065, avg_loss=0.77550]\n",
            "Generated 32 batches of size 8 in 9.535 sec\n",
            "Step 23118   [1.660 sec/step, loss=0.80909, avg_loss=0.77596]\n",
            "Step 23119   [1.665 sec/step, loss=0.81745, avg_loss=0.77702]\n",
            "Step 23120   [1.662 sec/step, loss=0.80957, avg_loss=0.77735]\n",
            "Step 23121   [1.666 sec/step, loss=0.78342, avg_loss=0.77738]\n",
            "Step 23122   [1.663 sec/step, loss=0.81560, avg_loss=0.77762]\n",
            "Step 23123   [1.658 sec/step, loss=0.79588, avg_loss=0.77800]\n",
            "Step 23124   [1.656 sec/step, loss=0.79344, avg_loss=0.77834]\n",
            "Step 23125   [1.655 sec/step, loss=0.77120, avg_loss=0.77772]\n",
            "Step 23126   [1.655 sec/step, loss=0.79695, avg_loss=0.77798]\n",
            "Step 23127   [1.652 sec/step, loss=0.73260, avg_loss=0.77697]\n",
            "Step 23128   [1.648 sec/step, loss=0.72514, avg_loss=0.77620]\n",
            "Step 23129   [1.649 sec/step, loss=0.74123, avg_loss=0.77557]\n",
            "Step 23130   [1.645 sec/step, loss=0.79102, avg_loss=0.77530]\n",
            "Step 23131   [1.607 sec/step, loss=0.76794, avg_loss=0.77602]\n",
            "Step 23132   [1.603 sec/step, loss=0.78254, avg_loss=0.77579]\n",
            "Step 23133   [1.594 sec/step, loss=0.83595, avg_loss=0.77577]\n",
            "Step 23134   [1.598 sec/step, loss=0.74791, avg_loss=0.77537]\n",
            "Step 23135   [1.594 sec/step, loss=0.77632, avg_loss=0.77558]\n",
            "Step 23136   [1.597 sec/step, loss=0.82006, avg_loss=0.77601]\n",
            "Step 23137   [1.601 sec/step, loss=0.75405, avg_loss=0.77639]\n",
            "Step 23138   [1.602 sec/step, loss=0.76259, avg_loss=0.77622]\n",
            "Step 23139   [1.617 sec/step, loss=0.83002, avg_loss=0.77595]\n",
            "Step 23140   [1.620 sec/step, loss=0.83267, avg_loss=0.77620]\n",
            "Step 23141   [1.621 sec/step, loss=0.79234, avg_loss=0.77664]\n",
            "Step 23142   [1.619 sec/step, loss=0.74805, avg_loss=0.77653]\n",
            "Step 23143   [1.621 sec/step, loss=0.73696, avg_loss=0.77629]\n",
            "Step 23144   [1.623 sec/step, loss=0.78720, avg_loss=0.77640]\n",
            "Step 23145   [1.674 sec/step, loss=0.72160, avg_loss=0.77607]\n",
            "Generated 32 batches of size 8 in 9.609 sec\n",
            "Step 23146   [1.667 sec/step, loss=0.77032, avg_loss=0.77589]\n",
            "Step 23147   [1.677 sec/step, loss=0.85490, avg_loss=0.77730]\n",
            "Step 23148   [1.691 sec/step, loss=0.72850, avg_loss=0.77703]\n",
            "Step 23149   [1.693 sec/step, loss=0.77185, avg_loss=0.77662]\n",
            "Step 23150   [1.686 sec/step, loss=0.75652, avg_loss=0.77660]\n",
            "Step 23151   [1.683 sec/step, loss=0.79983, avg_loss=0.77692]\n",
            "Step 23152   [1.682 sec/step, loss=0.79859, avg_loss=0.77683]\n",
            "Step 23153   [1.696 sec/step, loss=0.72889, avg_loss=0.77638]\n",
            "Step 23154   [1.696 sec/step, loss=0.80750, avg_loss=0.77663]\n",
            "Step 23155   [1.696 sec/step, loss=0.79730, avg_loss=0.77728]\n",
            "Step 23156   [1.705 sec/step, loss=0.73159, avg_loss=0.77683]\n",
            "Step 23157   [1.704 sec/step, loss=0.79334, avg_loss=0.77700]\n",
            "Step 23158   [1.704 sec/step, loss=0.77152, avg_loss=0.77678]\n",
            "Step 23159   [1.698 sec/step, loss=0.84749, avg_loss=0.77737]\n",
            "Step 23160   [1.703 sec/step, loss=0.72080, avg_loss=0.77727]\n",
            "Step 23161   [1.707 sec/step, loss=0.78164, avg_loss=0.77737]\n",
            "Step 23162   [1.706 sec/step, loss=0.76139, avg_loss=0.77744]\n",
            "Step 23163   [1.712 sec/step, loss=0.80278, avg_loss=0.77744]\n",
            "Step 23164   [1.713 sec/step, loss=0.76777, avg_loss=0.77722]\n",
            "Step 23165   [1.719 sec/step, loss=0.78226, avg_loss=0.77792]\n",
            "Step 23166   [1.723 sec/step, loss=0.77108, avg_loss=0.77810]\n",
            "Step 23167   [1.720 sec/step, loss=0.79623, avg_loss=0.77827]\n",
            "Step 23168   [1.710 sec/step, loss=0.76931, avg_loss=0.77803]\n",
            "Step 23169   [1.751 sec/step, loss=0.67541, avg_loss=0.77744]\n",
            "Step 23170   [1.741 sec/step, loss=0.74556, avg_loss=0.77771]\n",
            "Step 23171   [1.746 sec/step, loss=0.74453, avg_loss=0.77713]\n",
            "Step 23172   [1.744 sec/step, loss=0.81331, avg_loss=0.77740]\n",
            "Step 23173   [1.743 sec/step, loss=0.74523, avg_loss=0.77674]\n",
            "Step 23174   [1.742 sec/step, loss=0.79678, avg_loss=0.77686]\n",
            "Step 23175   [1.719 sec/step, loss=0.81876, avg_loss=0.77755]\n",
            "Step 23176   [1.718 sec/step, loss=0.77942, avg_loss=0.77717]\n",
            "Step 23177   [1.705 sec/step, loss=0.79039, avg_loss=0.77714]\n",
            "Step 23178   [1.711 sec/step, loss=0.78434, avg_loss=0.77708]\n",
            "Step 23179   [1.715 sec/step, loss=0.76132, avg_loss=0.77747]\n",
            "Step 23180   [1.697 sec/step, loss=0.80301, avg_loss=0.77758]\n",
            "Step 23181   [1.695 sec/step, loss=0.82706, avg_loss=0.77851]\n",
            "Generated 32 batches of size 8 in 10.233 sec\n",
            "Step 23182   [1.685 sec/step, loss=0.74932, avg_loss=0.77782]\n",
            "Step 23183   [1.689 sec/step, loss=0.76576, avg_loss=0.77840]\n",
            "Step 23184   [1.682 sec/step, loss=0.76799, avg_loss=0.77819]\n",
            "Step 23185   [1.643 sec/step, loss=0.79612, avg_loss=0.77901]\n",
            "Step 23186   [1.637 sec/step, loss=0.81023, avg_loss=0.77866]\n",
            "Step 23187   [1.631 sec/step, loss=0.76156, avg_loss=0.77827]\n",
            "Step 23188   [1.624 sec/step, loss=0.78707, avg_loss=0.77865]\n",
            "Step 23189   [1.622 sec/step, loss=0.82825, avg_loss=0.77903]\n",
            "Step 23190   [1.623 sec/step, loss=0.79027, avg_loss=0.77899]\n",
            "Step 23191   [1.613 sec/step, loss=0.78407, avg_loss=0.77846]\n",
            "Step 23192   [1.610 sec/step, loss=0.79293, avg_loss=0.77865]\n",
            "Step 23193   [1.615 sec/step, loss=0.75072, avg_loss=0.77831]\n",
            "Step 23194   [1.611 sec/step, loss=0.82279, avg_loss=0.77904]\n",
            "Step 23195   [1.617 sec/step, loss=0.81315, avg_loss=0.77947]\n",
            "Step 23196   [1.615 sec/step, loss=0.80503, avg_loss=0.77942]\n",
            "Step 23197   [1.611 sec/step, loss=0.75801, avg_loss=0.77913]\n",
            "Step 23198   [1.581 sec/step, loss=0.79228, avg_loss=0.77929]\n",
            "Step 23199   [1.571 sec/step, loss=0.80831, avg_loss=0.77915]\n",
            "Step 23200   [1.575 sec/step, loss=0.82877, avg_loss=0.77955]\n",
            "Writing summary at step: 23200\n",
            "Step 23201   [1.573 sec/step, loss=0.83427, avg_loss=0.77999]\n",
            "Step 23202   [1.571 sec/step, loss=0.84303, avg_loss=0.78090]\n",
            "Step 23203   [1.556 sec/step, loss=0.80802, avg_loss=0.78103]\n",
            "Step 23204   [1.562 sec/step, loss=0.73022, avg_loss=0.78045]\n",
            "Step 23205   [1.600 sec/step, loss=0.70431, avg_loss=0.77935]\n",
            "Step 23206   [1.611 sec/step, loss=0.79182, avg_loss=0.77973]\n",
            "Step 23207   [1.592 sec/step, loss=0.82176, avg_loss=0.78099]\n",
            "Step 23208   [1.597 sec/step, loss=0.80922, avg_loss=0.78115]\n",
            "Step 23209   [1.623 sec/step, loss=0.86201, avg_loss=0.78287]\n",
            "Step 23210   [1.630 sec/step, loss=0.78001, avg_loss=0.78266]\n",
            "Step 23211   [1.622 sec/step, loss=0.82935, avg_loss=0.78363]\n",
            "Generated 32 batches of size 8 in 10.242 sec\n",
            "Step 23212   [1.627 sec/step, loss=0.82152, avg_loss=0.78408]\n",
            "Step 23213   [1.634 sec/step, loss=0.84148, avg_loss=0.78478]\n",
            "Step 23214   [1.636 sec/step, loss=0.80570, avg_loss=0.78551]\n",
            "Step 23215   [1.632 sec/step, loss=0.82576, avg_loss=0.78604]\n",
            "Step 23216   [1.627 sec/step, loss=0.77141, avg_loss=0.78589]\n",
            "Step 23217   [1.620 sec/step, loss=0.77186, avg_loss=0.78540]\n",
            "Step 23218   [1.615 sec/step, loss=0.72451, avg_loss=0.78455]\n",
            "Step 23219   [1.626 sec/step, loss=0.81567, avg_loss=0.78453]\n",
            "Step 23220   [1.622 sec/step, loss=0.75793, avg_loss=0.78402]\n",
            "Step 23221   [1.618 sec/step, loss=0.77210, avg_loss=0.78390]\n",
            "Step 23222   [1.638 sec/step, loss=0.71184, avg_loss=0.78287]\n",
            "Step 23223   [1.639 sec/step, loss=0.73242, avg_loss=0.78223]\n",
            "Step 23224   [1.640 sec/step, loss=0.73434, avg_loss=0.78164]\n",
            "Step 23225   [1.643 sec/step, loss=0.80705, avg_loss=0.78200]\n",
            "Step 23226   [1.640 sec/step, loss=0.72863, avg_loss=0.78132]\n",
            "Step 23227   [1.631 sec/step, loss=0.83941, avg_loss=0.78238]\n",
            "Step 23228   [1.632 sec/step, loss=0.77304, avg_loss=0.78286]\n",
            "Step 23229   [1.632 sec/step, loss=0.80376, avg_loss=0.78349]\n",
            "Step 23230   [1.638 sec/step, loss=0.80025, avg_loss=0.78358]\n",
            "Step 23231   [1.678 sec/step, loss=0.68354, avg_loss=0.78274]\n",
            "Step 23232   [1.679 sec/step, loss=0.83839, avg_loss=0.78330]\n",
            "Step 23233   [1.680 sec/step, loss=0.83664, avg_loss=0.78330]\n",
            "Step 23234   [1.679 sec/step, loss=0.79000, avg_loss=0.78372]\n",
            "Step 23235   [1.680 sec/step, loss=0.75219, avg_loss=0.78348]\n",
            "Step 23236   [1.679 sec/step, loss=0.82390, avg_loss=0.78352]\n",
            "Step 23237   [1.683 sec/step, loss=0.80017, avg_loss=0.78398]\n",
            "Step 23238   [1.682 sec/step, loss=0.74881, avg_loss=0.78384]\n",
            "Step 23239   [1.679 sec/step, loss=0.83457, avg_loss=0.78389]\n",
            "Step 23240   [1.678 sec/step, loss=0.81470, avg_loss=0.78371]\n",
            "Step 23241   [1.674 sec/step, loss=0.81558, avg_loss=0.78394]\n",
            "Step 23242   [1.679 sec/step, loss=0.81353, avg_loss=0.78460]\n",
            "Step 23243   [1.692 sec/step, loss=0.87616, avg_loss=0.78599]\n",
            "Generated 32 batches of size 8 in 10.520 sec\n",
            "Step 23244   [1.700 sec/step, loss=0.83799, avg_loss=0.78650]\n",
            "Step 23245   [1.644 sec/step, loss=0.80943, avg_loss=0.78738]\n",
            "Step 23246   [1.639 sec/step, loss=0.79967, avg_loss=0.78767]\n",
            "Step 23247   [1.618 sec/step, loss=0.73998, avg_loss=0.78652]\n",
            "Step 23248   [1.607 sec/step, loss=0.79962, avg_loss=0.78723]\n",
            "Step 23249   [1.603 sec/step, loss=0.76675, avg_loss=0.78718]\n",
            "Step 23250   [1.601 sec/step, loss=0.81911, avg_loss=0.78781]\n",
            "Step 23251   [1.600 sec/step, loss=0.82279, avg_loss=0.78804]\n",
            "Step 23252   [1.603 sec/step, loss=0.81198, avg_loss=0.78817]\n",
            "Step 23253   [1.590 sec/step, loss=0.78414, avg_loss=0.78872]\n",
            "Step 23254   [1.585 sec/step, loss=0.82047, avg_loss=0.78885]\n",
            "Step 23255   [1.584 sec/step, loss=0.81071, avg_loss=0.78899]\n",
            "Step 23256   [1.580 sec/step, loss=0.78129, avg_loss=0.78948]\n",
            "Step 23257   [1.581 sec/step, loss=0.83749, avg_loss=0.78992]\n",
            "Step 23258   [1.590 sec/step, loss=0.75470, avg_loss=0.78976]\n",
            "Step 23259   [1.592 sec/step, loss=0.73373, avg_loss=0.78862]\n",
            "Step 23260   [1.597 sec/step, loss=0.77218, avg_loss=0.78913]\n",
            "Step 23261   [1.594 sec/step, loss=0.79966, avg_loss=0.78931]\n",
            "Step 23262   [1.614 sec/step, loss=0.81088, avg_loss=0.78981]\n",
            "Step 23263   [1.613 sec/step, loss=0.76021, avg_loss=0.78938]\n",
            "Step 23264   [1.627 sec/step, loss=0.79487, avg_loss=0.78965]\n",
            "Step 23265   [1.616 sec/step, loss=0.82994, avg_loss=0.79013]\n",
            "Step 23266   [1.614 sec/step, loss=0.80973, avg_loss=0.79052]\n",
            "Step 23267   [1.619 sec/step, loss=0.76460, avg_loss=0.79020]\n",
            "Step 23268   [1.625 sec/step, loss=0.82361, avg_loss=0.79074]\n",
            "Step 23269   [1.590 sec/step, loss=0.71263, avg_loss=0.79111]\n",
            "Step 23270   [1.598 sec/step, loss=0.83188, avg_loss=0.79198]\n",
            "Step 23271   [1.618 sec/step, loss=0.78681, avg_loss=0.79240]\n",
            "Step 23272   [1.626 sec/step, loss=0.77497, avg_loss=0.79202]\n",
            "Step 23273   [1.628 sec/step, loss=0.82051, avg_loss=0.79277]\n",
            "Generated 32 batches of size 8 in 9.928 sec\n",
            "Step 23274   [1.676 sec/step, loss=0.76993, avg_loss=0.79250]\n",
            "Step 23275   [1.677 sec/step, loss=0.78950, avg_loss=0.79221]\n",
            "Step 23276   [1.676 sec/step, loss=0.77325, avg_loss=0.79215]\n",
            "Step 23277   [1.675 sec/step, loss=0.79804, avg_loss=0.79222]\n",
            "Step 23278   [1.664 sec/step, loss=0.79246, avg_loss=0.79230]\n",
            "Step 23279   [1.659 sec/step, loss=0.78430, avg_loss=0.79253]\n",
            "Step 23280   [1.664 sec/step, loss=0.84334, avg_loss=0.79294]\n",
            "Step 23281   [1.661 sec/step, loss=0.78478, avg_loss=0.79252]\n",
            "Step 23282   [1.663 sec/step, loss=0.81990, avg_loss=0.79322]\n",
            "Step 23283   [1.651 sec/step, loss=0.80485, avg_loss=0.79361]\n",
            "Step 23284   [1.664 sec/step, loss=0.82580, avg_loss=0.79419]\n",
            "Step 23285   [1.665 sec/step, loss=0.82405, avg_loss=0.79447]\n",
            "Step 23286   [1.671 sec/step, loss=0.79373, avg_loss=0.79430]\n",
            "Step 23287   [1.676 sec/step, loss=0.80187, avg_loss=0.79471]\n",
            "Step 23288   [1.684 sec/step, loss=0.77318, avg_loss=0.79457]\n",
            "Step 23289   [1.684 sec/step, loss=0.82221, avg_loss=0.79451]\n",
            "Step 23290   [1.680 sec/step, loss=0.81339, avg_loss=0.79474]\n",
            "Step 23291   [1.678 sec/step, loss=0.75485, avg_loss=0.79445]\n",
            "Step 23292   [1.672 sec/step, loss=0.77841, avg_loss=0.79430]\n",
            "Step 23293   [1.670 sec/step, loss=0.77917, avg_loss=0.79459]\n",
            "Step 23294   [1.672 sec/step, loss=0.80592, avg_loss=0.79442]\n",
            "Step 23295   [1.669 sec/step, loss=0.72089, avg_loss=0.79349]\n",
            "Step 23296   [1.684 sec/step, loss=0.70734, avg_loss=0.79252]\n",
            "Step 23297   [1.688 sec/step, loss=0.77822, avg_loss=0.79272]\n",
            "Step 23298   [1.685 sec/step, loss=0.82106, avg_loss=0.79301]\n",
            "Step 23299   [1.717 sec/step, loss=0.76182, avg_loss=0.79254]\n",
            "Step 23300   [1.718 sec/step, loss=0.73915, avg_loss=0.79165]\n",
            "Writing summary at step: 23300\n",
            "Step 23301   [1.712 sec/step, loss=0.83356, avg_loss=0.79164]\n",
            "Step 23302   [1.726 sec/step, loss=0.76318, avg_loss=0.79084]\n",
            "Step 23303   [1.734 sec/step, loss=0.80497, avg_loss=0.79081]\n",
            "Step 23304   [1.733 sec/step, loss=0.79018, avg_loss=0.79141]\n",
            "Step 23305   [1.705 sec/step, loss=0.81992, avg_loss=0.79257]\n",
            "Step 23306   [1.696 sec/step, loss=0.73124, avg_loss=0.79196]\n",
            "Generated 32 batches of size 8 in 10.516 sec\n",
            "Step 23307   [1.694 sec/step, loss=0.80093, avg_loss=0.79175]\n",
            "Step 23308   [1.689 sec/step, loss=0.73527, avg_loss=0.79101]\n",
            "Step 23309   [1.658 sec/step, loss=0.70950, avg_loss=0.78949]\n",
            "Step 23310   [1.651 sec/step, loss=0.84490, avg_loss=0.79014]\n",
            "Step 23311   [1.653 sec/step, loss=0.78833, avg_loss=0.78973]\n",
            "Step 23312   [1.647 sec/step, loss=0.80112, avg_loss=0.78952]\n",
            "Step 23313   [1.648 sec/step, loss=0.79471, avg_loss=0.78905]\n",
            "Step 23314   [1.635 sec/step, loss=0.77808, avg_loss=0.78878]\n",
            "Step 23315   [1.639 sec/step, loss=0.81626, avg_loss=0.78868]\n",
            "Step 23316   [1.644 sec/step, loss=0.79233, avg_loss=0.78889]\n",
            "Step 23317   [1.642 sec/step, loss=0.77359, avg_loss=0.78891]\n",
            "Step 23318   [1.644 sec/step, loss=0.78327, avg_loss=0.78950]\n",
            "Step 23319   [1.629 sec/step, loss=0.77664, avg_loss=0.78911]\n",
            "Step 23320   [1.632 sec/step, loss=0.71135, avg_loss=0.78864]\n",
            "Step 23321   [1.627 sec/step, loss=0.78468, avg_loss=0.78877]\n",
            "Step 23322   [1.627 sec/step, loss=0.75155, avg_loss=0.78916]\n",
            "Step 23323   [1.637 sec/step, loss=0.76162, avg_loss=0.78946]\n",
            "Step 23324   [1.664 sec/step, loss=0.66406, avg_loss=0.78875]\n",
            "Step 23325   [1.671 sec/step, loss=0.73809, avg_loss=0.78806]\n",
            "Step 23326   [1.675 sec/step, loss=0.79449, avg_loss=0.78872]\n",
            "Step 23327   [1.679 sec/step, loss=0.76904, avg_loss=0.78802]\n",
            "Step 23328   [1.681 sec/step, loss=0.83094, avg_loss=0.78860]\n",
            "Step 23329   [1.681 sec/step, loss=0.82034, avg_loss=0.78876]\n",
            "Step 23330   [1.718 sec/step, loss=0.81143, avg_loss=0.78888]\n",
            "Step 23331   [1.679 sec/step, loss=0.81032, avg_loss=0.79014]\n",
            "Step 23332   [1.684 sec/step, loss=0.81920, avg_loss=0.78995]\n",
            "Step 23333   [1.685 sec/step, loss=0.76049, avg_loss=0.78919]\n",
            "Step 23334   [1.701 sec/step, loss=0.73521, avg_loss=0.78864]\n",
            "Step 23335   [1.701 sec/step, loss=0.79209, avg_loss=0.78904]\n",
            "Step 23336   [1.703 sec/step, loss=0.74554, avg_loss=0.78826]\n",
            "Step 23337   [1.699 sec/step, loss=0.75409, avg_loss=0.78780]\n",
            "Step 23338   [1.697 sec/step, loss=0.76991, avg_loss=0.78801]\n",
            "Generated 32 batches of size 8 in 10.312 sec\n",
            "Step 23339   [1.695 sec/step, loss=0.77333, avg_loss=0.78739]\n",
            "Step 23340   [1.694 sec/step, loss=0.81883, avg_loss=0.78744]\n",
            "Step 23341   [1.693 sec/step, loss=0.77171, avg_loss=0.78700]\n",
            "Step 23342   [1.684 sec/step, loss=0.77090, avg_loss=0.78657]\n",
            "Step 23343   [1.671 sec/step, loss=0.82923, avg_loss=0.78610]\n",
            "Step 23344   [1.659 sec/step, loss=0.79107, avg_loss=0.78563]\n",
            "Step 23345   [1.662 sec/step, loss=0.81366, avg_loss=0.78567]\n",
            "Step 23346   [1.665 sec/step, loss=0.80525, avg_loss=0.78573]\n",
            "Step 23347   [1.676 sec/step, loss=0.80259, avg_loss=0.78636]\n",
            "Step 23348   [1.674 sec/step, loss=0.78805, avg_loss=0.78624]\n",
            "Step 23349   [1.675 sec/step, loss=0.78196, avg_loss=0.78639]\n",
            "Step 23350   [1.680 sec/step, loss=0.81748, avg_loss=0.78638]\n",
            "Step 23351   [1.680 sec/step, loss=0.76556, avg_loss=0.78580]\n",
            "Step 23352   [1.681 sec/step, loss=0.76155, avg_loss=0.78530]\n",
            "Step 23353   [1.676 sec/step, loss=0.80700, avg_loss=0.78553]\n",
            "Step 23354   [1.702 sec/step, loss=0.78394, avg_loss=0.78516]\n",
            "Step 23355   [1.717 sec/step, loss=0.75368, avg_loss=0.78459]\n",
            "Step 23356   [1.720 sec/step, loss=0.80751, avg_loss=0.78486]\n",
            "Step 23357   [1.756 sec/step, loss=0.68702, avg_loss=0.78335]\n",
            "Step 23358   [1.751 sec/step, loss=0.78395, avg_loss=0.78364]\n",
            "Step 23359   [1.746 sec/step, loss=0.79489, avg_loss=0.78425]\n",
            "Step 23360   [1.742 sec/step, loss=0.77348, avg_loss=0.78427]\n",
            "Step 23361   [1.748 sec/step, loss=0.75272, avg_loss=0.78380]\n",
            "Step 23362   [1.726 sec/step, loss=0.78798, avg_loss=0.78357]\n",
            "Step 23363   [1.724 sec/step, loss=0.81995, avg_loss=0.78417]\n",
            "Step 23364   [1.711 sec/step, loss=0.83261, avg_loss=0.78454]\n",
            "Step 23365   [1.714 sec/step, loss=0.82205, avg_loss=0.78447]\n",
            "Step 23366   [1.719 sec/step, loss=0.75043, avg_loss=0.78387]\n",
            "Step 23367   [1.725 sec/step, loss=0.76989, avg_loss=0.78393]\n",
            "Generated 32 batches of size 8 in 6.792 sec\n",
            "Step 23368   [1.738 sec/step, loss=0.82688, avg_loss=0.78396]\n",
            "Step 23369   [1.732 sec/step, loss=0.76569, avg_loss=0.78449]\n",
            "Step 23370   [1.736 sec/step, loss=0.75351, avg_loss=0.78370]\n",
            "Step 23371   [1.711 sec/step, loss=0.83834, avg_loss=0.78422]\n",
            "Step 23372   [1.702 sec/step, loss=0.71765, avg_loss=0.78365]\n",
            "Step 23373   [1.698 sec/step, loss=0.76763, avg_loss=0.78312]\n",
            "Step 23374   [1.650 sec/step, loss=0.82593, avg_loss=0.78368]\n",
            "Step 23375   [1.654 sec/step, loss=0.72328, avg_loss=0.78302]\n",
            "Step 23376   [1.660 sec/step, loss=0.71911, avg_loss=0.78247]\n",
            "Step 23377   [1.658 sec/step, loss=0.84392, avg_loss=0.78293]\n",
            "Step 23378   [1.659 sec/step, loss=0.77381, avg_loss=0.78275]\n",
            "Step 23379   [1.656 sec/step, loss=0.74030, avg_loss=0.78231]\n",
            "Step 23380   [1.651 sec/step, loss=0.79232, avg_loss=0.78180]\n",
            "Step 23381   [1.644 sec/step, loss=0.78189, avg_loss=0.78177]\n",
            "Step 23382   [1.645 sec/step, loss=0.81626, avg_loss=0.78173]\n",
            "Step 23383   [1.642 sec/step, loss=0.78985, avg_loss=0.78158]\n",
            "Step 23384   [1.630 sec/step, loss=0.76588, avg_loss=0.78098]\n",
            "Step 23385   [1.633 sec/step, loss=0.79791, avg_loss=0.78072]\n",
            "Step 23386   [1.631 sec/step, loss=0.78690, avg_loss=0.78065]\n",
            "Step 23387   [1.634 sec/step, loss=0.80666, avg_loss=0.78070]\n",
            "Step 23388   [1.629 sec/step, loss=0.74870, avg_loss=0.78046]\n",
            "Step 23389   [1.629 sec/step, loss=0.81929, avg_loss=0.78043]\n",
            "Step 23390   [1.648 sec/step, loss=0.78382, avg_loss=0.78013]\n",
            "Step 23391   [1.653 sec/step, loss=0.73315, avg_loss=0.77991]\n",
            "Step 23392   [1.653 sec/step, loss=0.80232, avg_loss=0.78015]\n",
            "Step 23393   [1.657 sec/step, loss=0.77349, avg_loss=0.78010]\n",
            "Step 23394   [1.657 sec/step, loss=0.83197, avg_loss=0.78036]\n",
            "Step 23395   [1.656 sec/step, loss=0.69821, avg_loss=0.78013]\n",
            "Step 23396   [1.642 sec/step, loss=0.76337, avg_loss=0.78069]\n",
            "Step 23397   [1.638 sec/step, loss=0.75413, avg_loss=0.78045]\n",
            "Step 23398   [1.641 sec/step, loss=0.78459, avg_loss=0.78008]\n",
            "Step 23399   [1.618 sec/step, loss=0.77792, avg_loss=0.78025]\n",
            "Generated 32 batches of size 8 in 7.226 sec\n",
            "Step 23400   [1.638 sec/step, loss=0.70395, avg_loss=0.77989]\n",
            "Writing summary at step: 23400\n",
            "Step 23401   [1.634 sec/step, loss=0.82792, avg_loss=0.77984]\n",
            "Step 23402   [1.653 sec/step, loss=0.63256, avg_loss=0.77853]\n",
            "Step 23403   [1.654 sec/step, loss=0.71383, avg_loss=0.77762]\n",
            "Step 23404   [1.651 sec/step, loss=0.79844, avg_loss=0.77770]\n",
            "Step 23405   [1.654 sec/step, loss=0.78990, avg_loss=0.77740]\n",
            "Step 23406   [1.645 sec/step, loss=0.80524, avg_loss=0.77814]\n",
            "Step 23407   [1.646 sec/step, loss=0.80960, avg_loss=0.77823]\n",
            "Step 23408   [1.646 sec/step, loss=0.79583, avg_loss=0.77883]\n",
            "Step 23409   [1.644 sec/step, loss=0.81174, avg_loss=0.77986]\n",
            "Step 23410   [1.650 sec/step, loss=0.73914, avg_loss=0.77880]\n",
            "Step 23411   [1.647 sec/step, loss=0.80551, avg_loss=0.77897]\n",
            "Step 23412   [1.649 sec/step, loss=0.76520, avg_loss=0.77861]\n",
            "Step 23413   [1.643 sec/step, loss=0.80984, avg_loss=0.77876]\n",
            "Step 23414   [1.641 sec/step, loss=0.80232, avg_loss=0.77900]\n",
            "Step 23415   [1.662 sec/step, loss=0.82245, avg_loss=0.77907]\n",
            "Step 23416   [1.667 sec/step, loss=0.80040, avg_loss=0.77915]\n",
            "Step 23417   [1.679 sec/step, loss=0.78132, avg_loss=0.77922]\n",
            "Step 23418   [1.680 sec/step, loss=0.78594, avg_loss=0.77925]\n",
            "Step 23419   [1.683 sec/step, loss=0.78110, avg_loss=0.77930]\n",
            "Step 23420   [1.686 sec/step, loss=0.74625, avg_loss=0.77965]\n",
            "Step 23421   [1.692 sec/step, loss=0.69283, avg_loss=0.77873]\n",
            "Step 23422   [1.676 sec/step, loss=0.82078, avg_loss=0.77942]\n",
            "Step 23423   [1.665 sec/step, loss=0.76953, avg_loss=0.77950]\n",
            "Step 23424   [1.637 sec/step, loss=0.77830, avg_loss=0.78064]\n",
            "Step 23425   [1.667 sec/step, loss=0.73390, avg_loss=0.78060]\n",
            "Step 23426   [1.675 sec/step, loss=0.76579, avg_loss=0.78031]\n",
            "Step 23427   [1.677 sec/step, loss=0.76392, avg_loss=0.78026]\n",
            "Step 23428   [1.674 sec/step, loss=0.79272, avg_loss=0.77988]\n",
            "Step 23429   [1.681 sec/step, loss=0.74347, avg_loss=0.77911]\n",
            "Step 23430   [1.663 sec/step, loss=0.72125, avg_loss=0.77821]\n",
            "Step 23431   [1.671 sec/step, loss=0.76582, avg_loss=0.77776]\n",
            "Generated 32 batches of size 8 in 7.857 sec\n",
            "Step 23432   [1.683 sec/step, loss=0.79673, avg_loss=0.77754]\n",
            "Step 23433   [1.673 sec/step, loss=0.71808, avg_loss=0.77711]\n",
            "Step 23434   [1.655 sec/step, loss=0.80940, avg_loss=0.77786]\n",
            "Step 23435   [1.652 sec/step, loss=0.69617, avg_loss=0.77690]\n",
            "Step 23436   [1.650 sec/step, loss=0.76458, avg_loss=0.77709]\n",
            "Step 23437   [1.641 sec/step, loss=0.76667, avg_loss=0.77721]\n",
            "Step 23438   [1.649 sec/step, loss=0.78799, avg_loss=0.77739]\n",
            "Step 23439   [1.648 sec/step, loss=0.73397, avg_loss=0.77700]\n",
            "Step 23440   [1.648 sec/step, loss=0.81342, avg_loss=0.77695]\n",
            "Step 23441   [1.658 sec/step, loss=0.76748, avg_loss=0.77690]\n",
            "Step 23442   [1.664 sec/step, loss=0.77650, avg_loss=0.77696]\n",
            "Step 23443   [1.662 sec/step, loss=0.73313, avg_loss=0.77600]\n",
            "Step 23444   [1.679 sec/step, loss=0.81946, avg_loss=0.77628]\n",
            "Step 23445   [1.675 sec/step, loss=0.77568, avg_loss=0.77590]\n",
            "Step 23446   [1.675 sec/step, loss=0.73548, avg_loss=0.77520]\n",
            "Step 23447   [1.693 sec/step, loss=0.81407, avg_loss=0.77532]\n",
            "Step 23448   [1.700 sec/step, loss=0.77265, avg_loss=0.77517]\n",
            "Step 23449   [1.702 sec/step, loss=0.75167, avg_loss=0.77486]\n",
            "Step 23450   [1.735 sec/step, loss=0.76398, avg_loss=0.77433]\n",
            "Step 23451   [1.740 sec/step, loss=0.76776, avg_loss=0.77435]\n",
            "Step 23452   [1.735 sec/step, loss=0.82126, avg_loss=0.77495]\n",
            "Step 23453   [1.736 sec/step, loss=0.81695, avg_loss=0.77505]\n",
            "Step 23454   [1.719 sec/step, loss=0.81780, avg_loss=0.77539]\n",
            "Step 23455   [1.706 sec/step, loss=0.77664, avg_loss=0.77561]\n",
            "Step 23456   [1.714 sec/step, loss=0.75002, avg_loss=0.77504]\n",
            "Step 23457   [1.671 sec/step, loss=0.82997, avg_loss=0.77647]\n",
            "Step 23458   [1.672 sec/step, loss=0.76606, avg_loss=0.77629]\n",
            "Step 23459   [1.673 sec/step, loss=0.74990, avg_loss=0.77584]\n",
            "Step 23460   [1.664 sec/step, loss=0.79948, avg_loss=0.77610]\n",
            "Step 23461   [1.660 sec/step, loss=0.80224, avg_loss=0.77660]\n",
            "Step 23462   [1.665 sec/step, loss=0.77522, avg_loss=0.77647]\n",
            "Step 23463   [1.666 sec/step, loss=0.79732, avg_loss=0.77624]\n",
            "Step 23464   [1.673 sec/step, loss=0.73928, avg_loss=0.77531]\n",
            "Step 23465   [1.672 sec/step, loss=0.80062, avg_loss=0.77509]\n",
            "Step 23466   [1.669 sec/step, loss=0.81087, avg_loss=0.77570]\n",
            "Generated 32 batches of size 8 in 8.709 sec\n",
            "Step 23467   [1.663 sec/step, loss=0.79350, avg_loss=0.77593]\n",
            "Step 23468   [1.642 sec/step, loss=0.80725, avg_loss=0.77574]\n",
            "Step 23469   [1.656 sec/step, loss=0.84785, avg_loss=0.77656]\n",
            "Step 23470   [1.656 sec/step, loss=0.80929, avg_loss=0.77712]\n",
            "Step 23471   [1.662 sec/step, loss=0.77536, avg_loss=0.77649]\n",
            "Step 23472   [1.660 sec/step, loss=0.76730, avg_loss=0.77698]\n",
            "Step 23473   [1.674 sec/step, loss=0.81443, avg_loss=0.77745]\n",
            "Step 23474   [1.675 sec/step, loss=0.79942, avg_loss=0.77719]\n",
            "Step 23475   [1.668 sec/step, loss=0.81414, avg_loss=0.77810]\n",
            "Step 23476   [1.662 sec/step, loss=0.76251, avg_loss=0.77853]\n",
            "Step 23477   [1.704 sec/step, loss=0.71802, avg_loss=0.77727]\n",
            "Step 23478   [1.710 sec/step, loss=0.72969, avg_loss=0.77683]\n",
            "Step 23479   [1.711 sec/step, loss=0.80289, avg_loss=0.77746]\n",
            "Step 23480   [1.710 sec/step, loss=0.77722, avg_loss=0.77730]\n",
            "Step 23481   [1.710 sec/step, loss=0.76420, avg_loss=0.77713]\n",
            "Step 23482   [1.715 sec/step, loss=0.80297, avg_loss=0.77699]\n",
            "Step 23483   [1.724 sec/step, loss=0.84382, avg_loss=0.77753]\n",
            "Step 23484   [1.726 sec/step, loss=0.75762, avg_loss=0.77745]\n",
            "Step 23485   [1.725 sec/step, loss=0.82334, avg_loss=0.77771]\n",
            "Step 23486   [1.723 sec/step, loss=0.80096, avg_loss=0.77785]\n",
            "Step 23487   [1.718 sec/step, loss=0.74164, avg_loss=0.77720]\n",
            "Step 23488   [1.715 sec/step, loss=0.79598, avg_loss=0.77767]\n",
            "Step 23489   [1.713 sec/step, loss=0.79899, avg_loss=0.77747]\n",
            "Step 23490   [1.698 sec/step, loss=0.77566, avg_loss=0.77738]\n",
            "Step 23491   [1.699 sec/step, loss=0.81099, avg_loss=0.77816]\n",
            "Step 23492   [1.709 sec/step, loss=0.81850, avg_loss=0.77832]\n",
            "Step 23493   [1.734 sec/step, loss=0.73234, avg_loss=0.77791]\n",
            "Step 23494   [1.755 sec/step, loss=0.81480, avg_loss=0.77774]\n",
            "Step 23495   [1.751 sec/step, loss=0.72620, avg_loss=0.77802]\n",
            "Generated 32 batches of size 8 in 8.809 sec\n",
            "Step 23496   [1.749 sec/step, loss=0.76019, avg_loss=0.77799]\n",
            "Step 23497   [1.752 sec/step, loss=0.79846, avg_loss=0.77843]\n",
            "Step 23498   [1.745 sec/step, loss=0.80920, avg_loss=0.77868]\n",
            "Step 23499   [1.737 sec/step, loss=0.77100, avg_loss=0.77861]\n",
            "Step 23500   [1.714 sec/step, loss=0.80615, avg_loss=0.77963]\n",
            "Writing summary at step: 23500\n",
            "Step 23501   [1.738 sec/step, loss=0.80328, avg_loss=0.77939]\n",
            "Step 23502   [1.721 sec/step, loss=0.73457, avg_loss=0.78041]\n",
            "Step 23503   [1.713 sec/step, loss=0.76728, avg_loss=0.78094]\n",
            "Step 23504   [1.712 sec/step, loss=0.74543, avg_loss=0.78041]\n",
            "Step 23505   [1.700 sec/step, loss=0.80471, avg_loss=0.78056]\n",
            "Step 23506   [1.700 sec/step, loss=0.80402, avg_loss=0.78055]\n",
            "Step 23507   [1.706 sec/step, loss=0.82453, avg_loss=0.78070]\n",
            "Step 23508   [1.704 sec/step, loss=0.89368, avg_loss=0.78167]\n",
            "Step 23509   [1.707 sec/step, loss=0.74242, avg_loss=0.78098]\n",
            "Step 23510   [1.707 sec/step, loss=0.76535, avg_loss=0.78124]\n",
            "Step 23511   [1.707 sec/step, loss=0.73269, avg_loss=0.78051]\n",
            "Step 23512   [1.711 sec/step, loss=0.74645, avg_loss=0.78033]\n",
            "Step 23513   [1.713 sec/step, loss=0.84374, avg_loss=0.78067]\n",
            "Step 23514   [1.718 sec/step, loss=0.77917, avg_loss=0.78043]\n",
            "Step 23515   [1.694 sec/step, loss=0.78178, avg_loss=0.78003]\n",
            "Step 23516   [1.686 sec/step, loss=0.80839, avg_loss=0.78011]\n",
            "Step 23517   [1.682 sec/step, loss=0.79709, avg_loss=0.78027]\n",
            "Step 23518   [1.687 sec/step, loss=0.73578, avg_loss=0.77976]\n",
            "Step 23519   [1.690 sec/step, loss=0.81484, avg_loss=0.78010]\n",
            "Step 23520   [1.685 sec/step, loss=0.77578, avg_loss=0.78040]\n",
            "Step 23521   [1.675 sec/step, loss=0.83339, avg_loss=0.78180]\n",
            "Step 23522   [1.673 sec/step, loss=0.76686, avg_loss=0.78126]\n",
            "Step 23523   [1.673 sec/step, loss=0.77036, avg_loss=0.78127]\n",
            "Step 23524   [1.672 sec/step, loss=0.81361, avg_loss=0.78162]\n",
            "Step 23525   [1.637 sec/step, loss=0.82190, avg_loss=0.78250]\n",
            "Step 23526   [1.631 sec/step, loss=0.79984, avg_loss=0.78284]\n",
            "Step 23527   [1.626 sec/step, loss=0.80292, avg_loss=0.78323]\n",
            "Step 23528   [1.640 sec/step, loss=0.83126, avg_loss=0.78362]\n",
            "Generated 32 batches of size 8 in 9.153 sec\n",
            "Step 23529   [1.640 sec/step, loss=0.80205, avg_loss=0.78421]\n",
            "Step 23530   [1.660 sec/step, loss=0.76391, avg_loss=0.78463]\n",
            "Step 23531   [1.649 sec/step, loss=0.74359, avg_loss=0.78441]\n",
            "Step 23532   [1.628 sec/step, loss=0.78746, avg_loss=0.78432]\n",
            "Step 23533   [1.632 sec/step, loss=0.84155, avg_loss=0.78555]\n",
            "Step 23534   [1.635 sec/step, loss=0.86146, avg_loss=0.78607]\n",
            "Step 23535   [1.630 sec/step, loss=0.76858, avg_loss=0.78680]\n",
            "Step 23536   [1.627 sec/step, loss=0.79265, avg_loss=0.78708]\n",
            "Step 23537   [1.638 sec/step, loss=0.80820, avg_loss=0.78749]\n",
            "Step 23538   [1.645 sec/step, loss=0.77301, avg_loss=0.78734]\n",
            "Step 23539   [1.641 sec/step, loss=0.73229, avg_loss=0.78733]\n",
            "Step 23540   [1.640 sec/step, loss=0.78121, avg_loss=0.78700]\n",
            "Step 23541   [1.630 sec/step, loss=0.79885, avg_loss=0.78732]\n",
            "Step 23542   [1.668 sec/step, loss=0.71723, avg_loss=0.78673]\n",
            "Step 23543   [1.671 sec/step, loss=0.78077, avg_loss=0.78720]\n",
            "Step 23544   [1.661 sec/step, loss=0.78874, avg_loss=0.78689]\n",
            "Step 23545   [1.661 sec/step, loss=0.79986, avg_loss=0.78714]\n",
            "Step 23546   [1.664 sec/step, loss=0.77736, avg_loss=0.78756]\n",
            "Step 23547   [1.638 sec/step, loss=0.86156, avg_loss=0.78803]\n",
            "Step 23548   [1.630 sec/step, loss=0.81237, avg_loss=0.78843]\n",
            "Step 23549   [1.626 sec/step, loss=0.75124, avg_loss=0.78842]\n",
            "Step 23550   [1.588 sec/step, loss=0.78940, avg_loss=0.78868]\n",
            "Step 23551   [1.584 sec/step, loss=0.76679, avg_loss=0.78867]\n",
            "Step 23552   [1.588 sec/step, loss=0.79731, avg_loss=0.78843]\n",
            "Step 23553   [1.609 sec/step, loss=0.77629, avg_loss=0.78802]\n",
            "Step 23554   [1.601 sec/step, loss=0.79257, avg_loss=0.78777]\n",
            "Step 23555   [1.604 sec/step, loss=0.71183, avg_loss=0.78712]\n",
            "Step 23556   [1.597 sec/step, loss=0.73388, avg_loss=0.78696]\n",
            "Step 23557   [1.601 sec/step, loss=0.84052, avg_loss=0.78707]\n",
            "Step 23558   [1.606 sec/step, loss=0.79911, avg_loss=0.78740]\n",
            "Step 23559   [1.614 sec/step, loss=0.78969, avg_loss=0.78779]\n",
            "Step 23560   [1.620 sec/step, loss=0.79548, avg_loss=0.78775]\n",
            "Generated 32 batches of size 8 in 9.407 sec\n",
            "Step 23561   [1.634 sec/step, loss=0.76215, avg_loss=0.78735]\n",
            "Step 23562   [1.632 sec/step, loss=0.80504, avg_loss=0.78765]\n",
            "Step 23563   [1.630 sec/step, loss=0.80254, avg_loss=0.78770]\n",
            "Step 23564   [1.631 sec/step, loss=0.77903, avg_loss=0.78810]\n",
            "Step 23565   [1.649 sec/step, loss=0.78407, avg_loss=0.78793]\n",
            "Step 23566   [1.646 sec/step, loss=0.78300, avg_loss=0.78766]\n",
            "Step 23567   [1.644 sec/step, loss=0.80039, avg_loss=0.78773]\n",
            "Step 23568   [1.645 sec/step, loss=0.79431, avg_loss=0.78760]\n",
            "Step 23569   [1.633 sec/step, loss=0.77862, avg_loss=0.78690]\n",
            "Step 23570   [1.621 sec/step, loss=0.74448, avg_loss=0.78626]\n",
            "Step 23571   [1.617 sec/step, loss=0.81703, avg_loss=0.78667]\n",
            "Step 23572   [1.624 sec/step, loss=0.77484, avg_loss=0.78675]\n",
            "Step 23573   [1.618 sec/step, loss=0.80180, avg_loss=0.78662]\n",
            "Step 23574   [1.633 sec/step, loss=0.79078, avg_loss=0.78653]\n",
            "Step 23575   [1.631 sec/step, loss=0.82130, avg_loss=0.78661]\n",
            "Step 23576   [1.630 sec/step, loss=0.84207, avg_loss=0.78740]\n",
            "Step 23577   [1.600 sec/step, loss=0.80183, avg_loss=0.78824]\n",
            "Step 23578   [1.597 sec/step, loss=0.80215, avg_loss=0.78896]\n",
            "Step 23579   [1.598 sec/step, loss=0.74505, avg_loss=0.78839]\n",
            "Step 23580   [1.605 sec/step, loss=0.75413, avg_loss=0.78816]\n",
            "Step 23581   [1.607 sec/step, loss=0.80171, avg_loss=0.78853]\n",
            "Step 23582   [1.599 sec/step, loss=0.78944, avg_loss=0.78839]\n",
            "Step 23583   [1.594 sec/step, loss=0.75237, avg_loss=0.78748]\n",
            "Step 23584   [1.634 sec/step, loss=0.65395, avg_loss=0.78644]\n",
            "Step 23585   [1.637 sec/step, loss=0.84444, avg_loss=0.78665]\n",
            "Step 23586   [1.633 sec/step, loss=0.73128, avg_loss=0.78596]\n",
            "Step 23587   [1.634 sec/step, loss=0.80560, avg_loss=0.78660]\n",
            "Step 23588   [1.640 sec/step, loss=0.78263, avg_loss=0.78646]\n",
            "Step 23589   [1.655 sec/step, loss=0.81200, avg_loss=0.78659]\n",
            "Step 23590   [1.658 sec/step, loss=0.76262, avg_loss=0.78646]\n",
            "Step 23591   [1.659 sec/step, loss=0.76616, avg_loss=0.78602]\n",
            "Step 23592   [1.654 sec/step, loss=0.84316, avg_loss=0.78626]\n",
            "Generated 32 batches of size 8 in 10.309 sec\n",
            "Step 23593   [1.631 sec/step, loss=0.73214, avg_loss=0.78626]\n",
            "Step 23594   [1.606 sec/step, loss=0.81587, avg_loss=0.78627]\n",
            "Step 23595   [1.607 sec/step, loss=0.80659, avg_loss=0.78707]\n",
            "Step 23596   [1.611 sec/step, loss=0.80911, avg_loss=0.78756]\n",
            "Step 23597   [1.612 sec/step, loss=0.74655, avg_loss=0.78704]\n",
            "Step 23598   [1.618 sec/step, loss=0.84058, avg_loss=0.78736]\n",
            "Step 23599   [1.621 sec/step, loss=0.75768, avg_loss=0.78723]\n",
            "Step 23600   [1.624 sec/step, loss=0.80470, avg_loss=0.78721]\n",
            "Writing summary at step: 23600\n",
            "Step 23601   [1.600 sec/step, loss=0.76173, avg_loss=0.78680]\n",
            "Step 23602   [1.585 sec/step, loss=0.78558, avg_loss=0.78731]\n",
            "Step 23603   [1.586 sec/step, loss=0.79033, avg_loss=0.78754]\n",
            "Step 23604   [1.585 sec/step, loss=0.75686, avg_loss=0.78765]\n",
            "Step 23605   [1.582 sec/step, loss=0.73976, avg_loss=0.78700]\n",
            "Step 23606   [1.592 sec/step, loss=0.77093, avg_loss=0.78667]\n",
            "Step 23607   [1.582 sec/step, loss=0.81356, avg_loss=0.78656]\n",
            "Step 23608   [1.587 sec/step, loss=0.81441, avg_loss=0.78577]\n",
            "Step 23609   [1.588 sec/step, loss=0.75362, avg_loss=0.78588]\n",
            "Step 23610   [1.584 sec/step, loss=0.76159, avg_loss=0.78584]\n",
            "Step 23611   [1.585 sec/step, loss=0.79835, avg_loss=0.78650]\n",
            "Step 23612   [1.579 sec/step, loss=0.72034, avg_loss=0.78624]\n",
            "Step 23613   [1.586 sec/step, loss=0.83765, avg_loss=0.78618]\n",
            "Step 23614   [1.587 sec/step, loss=0.77792, avg_loss=0.78616]\n",
            "Step 23615   [1.587 sec/step, loss=0.79237, avg_loss=0.78627]\n",
            "Step 23616   [1.584 sec/step, loss=0.82516, avg_loss=0.78644]\n",
            "Step 23617   [1.575 sec/step, loss=0.80327, avg_loss=0.78650]\n",
            "Step 23618   [1.609 sec/step, loss=0.72112, avg_loss=0.78635]\n",
            "Step 23619   [1.627 sec/step, loss=0.74349, avg_loss=0.78564]\n",
            "Step 23620   [1.658 sec/step, loss=0.80433, avg_loss=0.78592]\n",
            "Step 23621   [1.671 sec/step, loss=0.77203, avg_loss=0.78531]\n",
            "Generated 32 batches of size 8 in 10.220 sec\n",
            "Step 23622   [1.678 sec/step, loss=0.79374, avg_loss=0.78558]\n",
            "Step 23623   [1.683 sec/step, loss=0.77037, avg_loss=0.78558]\n",
            "Step 23624   [1.684 sec/step, loss=0.79298, avg_loss=0.78537]\n",
            "Step 23625   [1.685 sec/step, loss=0.71377, avg_loss=0.78429]\n",
            "Step 23626   [1.689 sec/step, loss=0.81841, avg_loss=0.78448]\n",
            "Step 23627   [1.689 sec/step, loss=0.81963, avg_loss=0.78465]\n",
            "Step 23628   [1.678 sec/step, loss=0.73744, avg_loss=0.78371]\n",
            "Step 23629   [1.684 sec/step, loss=0.75440, avg_loss=0.78323]\n",
            "Step 23630   [1.675 sec/step, loss=0.74623, avg_loss=0.78305]\n",
            "Step 23631   [1.690 sec/step, loss=0.78379, avg_loss=0.78346]\n",
            "Step 23632   [1.691 sec/step, loss=0.81535, avg_loss=0.78373]\n",
            "Step 23633   [1.691 sec/step, loss=0.80255, avg_loss=0.78334]\n",
            "Step 23634   [1.691 sec/step, loss=0.75490, avg_loss=0.78228]\n",
            "Step 23635   [1.696 sec/step, loss=0.75155, avg_loss=0.78211]\n",
            "Step 23636   [1.697 sec/step, loss=0.77450, avg_loss=0.78193]\n",
            "Step 23637   [1.696 sec/step, loss=0.78728, avg_loss=0.78172]\n",
            "Step 23638   [1.702 sec/step, loss=0.77936, avg_loss=0.78178]\n",
            "Step 23639   [1.696 sec/step, loss=0.80091, avg_loss=0.78247]\n",
            "Step 23640   [1.702 sec/step, loss=0.80874, avg_loss=0.78274]\n",
            "Step 23641   [1.707 sec/step, loss=0.77852, avg_loss=0.78254]\n",
            "Step 23642   [1.667 sec/step, loss=0.71545, avg_loss=0.78252]\n",
            "Step 23643   [1.675 sec/step, loss=0.73297, avg_loss=0.78204]\n",
            "Step 23644   [1.666 sec/step, loss=0.72325, avg_loss=0.78139]\n",
            "Step 23645   [1.667 sec/step, loss=0.77427, avg_loss=0.78113]\n",
            "Step 23646   [1.662 sec/step, loss=0.72402, avg_loss=0.78060]\n",
            "Step 23647   [1.662 sec/step, loss=0.75566, avg_loss=0.77954]\n",
            "Step 23648   [1.661 sec/step, loss=0.82156, avg_loss=0.77963]\n",
            "Step 23649   [1.658 sec/step, loss=0.75854, avg_loss=0.77971]\n",
            "Step 23650   [1.657 sec/step, loss=0.78406, avg_loss=0.77965]\n",
            "Step 23651   [1.679 sec/step, loss=0.79728, avg_loss=0.77996]\n",
            "Step 23652   [1.676 sec/step, loss=0.77527, avg_loss=0.77974]\n",
            "Step 23653   [1.655 sec/step, loss=0.79177, avg_loss=0.77989]\n",
            "Step 23654   [1.658 sec/step, loss=0.79708, avg_loss=0.77994]\n",
            "Step 23655   [1.663 sec/step, loss=0.75015, avg_loss=0.78032]\n",
            "Generated 32 batches of size 8 in 10.109 sec\n",
            "Step 23656   [1.664 sec/step, loss=0.72455, avg_loss=0.78023]\n",
            "Step 23657   [1.666 sec/step, loss=0.79177, avg_loss=0.77974]\n",
            "Step 23658   [1.663 sec/step, loss=0.82907, avg_loss=0.78004]\n",
            "Step 23659   [1.655 sec/step, loss=0.78360, avg_loss=0.77998]\n",
            "Step 23660   [1.650 sec/step, loss=0.77198, avg_loss=0.77974]\n",
            "Step 23661   [1.655 sec/step, loss=0.84839, avg_loss=0.78061]\n",
            "Step 23662   [1.658 sec/step, loss=0.80302, avg_loss=0.78058]\n",
            "Step 23663   [1.659 sec/step, loss=0.80736, avg_loss=0.78063]\n",
            "Step 23664   [1.652 sec/step, loss=0.72846, avg_loss=0.78013]\n",
            "Step 23665   [1.639 sec/step, loss=0.76754, avg_loss=0.77996]\n",
            "Step 23666   [1.641 sec/step, loss=0.79802, avg_loss=0.78011]\n",
            "Step 23667   [1.641 sec/step, loss=0.74634, avg_loss=0.77957]\n",
            "Step 23668   [1.639 sec/step, loss=0.80524, avg_loss=0.77968]\n",
            "Step 23669   [1.651 sec/step, loss=0.76311, avg_loss=0.77953]\n",
            "Step 23670   [1.657 sec/step, loss=0.73603, avg_loss=0.77944]\n",
            "Step 23671   [1.657 sec/step, loss=0.82313, avg_loss=0.77950]\n",
            "Step 23672   [1.653 sec/step, loss=0.74775, avg_loss=0.77923]\n",
            "Step 23673   [1.647 sec/step, loss=0.79177, avg_loss=0.77913]\n",
            "Step 23674   [1.632 sec/step, loss=0.76552, avg_loss=0.77888]\n",
            "Step 23675   [1.644 sec/step, loss=0.78742, avg_loss=0.77854]\n",
            "Step 23676   [1.661 sec/step, loss=0.72672, avg_loss=0.77739]\n",
            "Step 23677   [1.650 sec/step, loss=0.75521, avg_loss=0.77692]\n",
            "Step 23678   [1.656 sec/step, loss=0.76436, avg_loss=0.77654]\n",
            "Step 23679   [1.664 sec/step, loss=0.77329, avg_loss=0.77682]\n",
            "Step 23680   [1.657 sec/step, loss=0.80807, avg_loss=0.77736]\n",
            "Step 23681   [1.661 sec/step, loss=0.82609, avg_loss=0.77761]\n",
            "Step 23682   [1.668 sec/step, loss=0.81197, avg_loss=0.77783]\n",
            "Step 23683   [1.666 sec/step, loss=0.77871, avg_loss=0.77810]\n",
            "Step 23684   [1.627 sec/step, loss=0.77755, avg_loss=0.77933]\n",
            "Step 23685   [1.625 sec/step, loss=0.78189, avg_loss=0.77871]\n",
            "Step 23686   [1.636 sec/step, loss=0.74694, avg_loss=0.77886]\n",
            "Step 23687   [1.638 sec/step, loss=0.71771, avg_loss=0.77798]\n",
            "Step 23688   [1.633 sec/step, loss=0.76796, avg_loss=0.77784]\n",
            "Generated 32 batches of size 8 in 9.228 sec\n",
            "Step 23689   [1.661 sec/step, loss=0.76249, avg_loss=0.77734]\n",
            "Step 23690   [1.662 sec/step, loss=0.78068, avg_loss=0.77752]\n",
            "Step 23691   [1.658 sec/step, loss=0.81933, avg_loss=0.77806]\n",
            "Step 23692   [1.652 sec/step, loss=0.76087, avg_loss=0.77723]\n",
            "Step 23693   [1.650 sec/step, loss=0.79571, avg_loss=0.77787]\n",
            "Step 23694   [1.652 sec/step, loss=0.80825, avg_loss=0.77779]\n",
            "Step 23695   [1.652 sec/step, loss=0.76255, avg_loss=0.77735]\n",
            "Step 23696   [1.651 sec/step, loss=0.79960, avg_loss=0.77726]\n",
            "Step 23697   [1.646 sec/step, loss=0.73779, avg_loss=0.77717]\n",
            "Step 23698   [1.643 sec/step, loss=0.81764, avg_loss=0.77694]\n",
            "Step 23699   [1.673 sec/step, loss=0.64458, avg_loss=0.77581]\n",
            "Step 23700   [1.672 sec/step, loss=0.73601, avg_loss=0.77512]\n",
            "Writing summary at step: 23700\n",
            "Step 23701   [1.674 sec/step, loss=0.81825, avg_loss=0.77569]\n",
            "Step 23702   [1.672 sec/step, loss=0.78120, avg_loss=0.77564]\n",
            "Step 23703   [1.667 sec/step, loss=0.73604, avg_loss=0.77510]\n",
            "Step 23704   [1.668 sec/step, loss=0.80240, avg_loss=0.77556]\n",
            "Step 23705   [1.673 sec/step, loss=0.80560, avg_loss=0.77621]\n",
            "Step 23706   [1.662 sec/step, loss=0.83844, avg_loss=0.77689]\n",
            "Step 23707   [1.664 sec/step, loss=0.76889, avg_loss=0.77644]\n",
            "Step 23708   [1.662 sec/step, loss=0.78286, avg_loss=0.77613]\n",
            "Step 23709   [1.660 sec/step, loss=0.74942, avg_loss=0.77609]\n",
            "Step 23710   [1.663 sec/step, loss=0.74940, avg_loss=0.77596]\n",
            "Step 23711   [1.670 sec/step, loss=0.76979, avg_loss=0.77568]\n",
            "Step 23712   [1.675 sec/step, loss=0.79068, avg_loss=0.77638]\n",
            "Step 23713   [1.677 sec/step, loss=0.79237, avg_loss=0.77593]\n",
            "Step 23714   [1.686 sec/step, loss=0.73690, avg_loss=0.77552]\n",
            "Step 23715   [1.687 sec/step, loss=0.78173, avg_loss=0.77541]\n",
            "Step 23716   [1.697 sec/step, loss=0.76341, avg_loss=0.77479]\n",
            "Step 23717   [1.711 sec/step, loss=0.74102, avg_loss=0.77417]\n",
            "Step 23718   [1.669 sec/step, loss=0.78387, avg_loss=0.77480]\n",
            "Generated 32 batches of size 8 in 9.615 sec\n",
            "Step 23719   [1.654 sec/step, loss=0.80471, avg_loss=0.77541]\n",
            "Step 23720   [1.635 sec/step, loss=0.80275, avg_loss=0.77540]\n",
            "Step 23721   [1.623 sec/step, loss=0.79935, avg_loss=0.77567]\n",
            "Step 23722   [1.619 sec/step, loss=0.69119, avg_loss=0.77464]\n",
            "Step 23723   [1.621 sec/step, loss=0.81536, avg_loss=0.77509]\n",
            "Step 23724   [1.621 sec/step, loss=0.76307, avg_loss=0.77479]\n",
            "Step 23725   [1.634 sec/step, loss=0.75795, avg_loss=0.77524]\n",
            "Step 23726   [1.635 sec/step, loss=0.81578, avg_loss=0.77521]\n",
            "Step 23727   [1.643 sec/step, loss=0.77218, avg_loss=0.77474]\n",
            "Step 23728   [1.639 sec/step, loss=0.78264, avg_loss=0.77519]\n",
            "Step 23729   [1.629 sec/step, loss=0.78777, avg_loss=0.77552]\n",
            "Step 23730   [1.595 sec/step, loss=0.76895, avg_loss=0.77575]\n",
            "Step 23731   [1.582 sec/step, loss=0.76049, avg_loss=0.77552]\n",
            "Step 23732   [1.583 sec/step, loss=0.77187, avg_loss=0.77508]\n",
            "Step 23733   [1.579 sec/step, loss=0.77815, avg_loss=0.77484]\n",
            "Step 23734   [1.577 sec/step, loss=0.78491, avg_loss=0.77514]\n",
            "Step 23735   [1.570 sec/step, loss=0.79589, avg_loss=0.77558]\n",
            "Step 23736   [1.576 sec/step, loss=0.80202, avg_loss=0.77585]\n",
            "Step 23737   [1.581 sec/step, loss=0.80771, avg_loss=0.77606]\n",
            "Step 23738   [1.560 sec/step, loss=0.78118, avg_loss=0.77608]\n",
            "Step 23739   [1.562 sec/step, loss=0.77341, avg_loss=0.77580]\n",
            "Step 23740   [1.557 sec/step, loss=0.81028, avg_loss=0.77582]\n",
            "Step 23741   [1.571 sec/step, loss=0.71171, avg_loss=0.77515]\n",
            "Step 23742   [1.566 sec/step, loss=0.78112, avg_loss=0.77581]\n",
            "Step 23743   [1.555 sec/step, loss=0.80461, avg_loss=0.77652]\n",
            "Step 23744   [1.558 sec/step, loss=0.77373, avg_loss=0.77703]\n",
            "Step 23745   [1.565 sec/step, loss=0.80695, avg_loss=0.77735]\n",
            "Step 23746   [1.567 sec/step, loss=0.72384, avg_loss=0.77735]\n",
            "Step 23747   [1.610 sec/step, loss=0.70180, avg_loss=0.77681]\n",
            "Generated 32 batches of size 8 in 8.602 sec\n",
            "Step 23748   [1.620 sec/step, loss=0.76424, avg_loss=0.77624]\n",
            "Step 23749   [1.624 sec/step, loss=0.77526, avg_loss=0.77641]\n",
            "Step 23750   [1.632 sec/step, loss=0.81114, avg_loss=0.77668]\n",
            "Step 23751   [1.615 sec/step, loss=0.80365, avg_loss=0.77674]\n",
            "Step 23752   [1.616 sec/step, loss=0.74910, avg_loss=0.77648]\n",
            "Step 23753   [1.623 sec/step, loss=0.81951, avg_loss=0.77676]\n",
            "Step 23754   [1.625 sec/step, loss=0.73574, avg_loss=0.77614]\n",
            "Step 23755   [1.619 sec/step, loss=0.76498, avg_loss=0.77629]\n",
            "Step 23756   [1.613 sec/step, loss=0.80865, avg_loss=0.77713]\n",
            "Step 23757   [1.614 sec/step, loss=0.81324, avg_loss=0.77735]\n",
            "Step 23758   [1.605 sec/step, loss=0.76084, avg_loss=0.77667]\n",
            "Step 23759   [1.602 sec/step, loss=0.79595, avg_loss=0.77679]\n",
            "Step 23760   [1.606 sec/step, loss=0.80489, avg_loss=0.77712]\n",
            "Step 23761   [1.593 sec/step, loss=0.78339, avg_loss=0.77647]\n",
            "Step 23762   [1.592 sec/step, loss=0.76116, avg_loss=0.77605]\n",
            "Step 23763   [1.593 sec/step, loss=0.78028, avg_loss=0.77578]\n",
            "Step 23764   [1.594 sec/step, loss=0.76444, avg_loss=0.77614]\n",
            "Step 23765   [1.602 sec/step, loss=0.86478, avg_loss=0.77711]\n",
            "Step 23766   [1.598 sec/step, loss=0.79252, avg_loss=0.77706]\n",
            "Step 23767   [1.598 sec/step, loss=0.80681, avg_loss=0.77766]\n",
            "Step 23768   [1.600 sec/step, loss=0.78547, avg_loss=0.77746]\n",
            "Step 23769   [1.585 sec/step, loss=0.79657, avg_loss=0.77780]\n",
            "Step 23770   [1.585 sec/step, loss=0.76006, avg_loss=0.77804]\n",
            "Step 23771   [1.584 sec/step, loss=0.76374, avg_loss=0.77745]\n",
            "Step 23772   [1.582 sec/step, loss=0.82251, avg_loss=0.77819]\n",
            "Step 23773   [1.589 sec/step, loss=0.78247, avg_loss=0.77810]\n",
            "Step 23774   [1.589 sec/step, loss=0.75042, avg_loss=0.77795]\n",
            "Step 23775   [1.583 sec/step, loss=0.76585, avg_loss=0.77773]\n",
            "Step 23776   [1.566 sec/step, loss=0.82602, avg_loss=0.77873]\n",
            "Step 23777   [1.570 sec/step, loss=0.81019, avg_loss=0.77928]\n",
            "Step 23778   [1.582 sec/step, loss=0.76450, avg_loss=0.77928]\n",
            "Step 23779   [1.593 sec/step, loss=0.80466, avg_loss=0.77959]\n",
            "Generated 32 batches of size 8 in 6.513 sec\n",
            "Step 23780   [1.600 sec/step, loss=0.79161, avg_loss=0.77943]\n",
            "Step 23781   [1.613 sec/step, loss=0.75090, avg_loss=0.77867]\n",
            "Step 23782   [1.608 sec/step, loss=0.78503, avg_loss=0.77840]\n",
            "Step 23783   [1.618 sec/step, loss=0.81894, avg_loss=0.77881]\n",
            "Step 23784   [1.618 sec/step, loss=0.74557, avg_loss=0.77849]\n",
            "Step 23785   [1.622 sec/step, loss=0.72751, avg_loss=0.77794]\n",
            "Step 23786   [1.656 sec/step, loss=0.67695, avg_loss=0.77724]\n",
            "Step 23787   [1.662 sec/step, loss=0.76985, avg_loss=0.77777]\n",
            "Step 23788   [1.701 sec/step, loss=0.61556, avg_loss=0.77624]\n",
            "Step 23789   [1.658 sec/step, loss=0.73740, avg_loss=0.77599]\n",
            "Step 23790   [1.662 sec/step, loss=0.77051, avg_loss=0.77589]\n",
            "Step 23791   [1.662 sec/step, loss=0.77950, avg_loss=0.77549]\n",
            "Step 23792   [1.666 sec/step, loss=0.75184, avg_loss=0.77540]\n",
            "Step 23793   [1.664 sec/step, loss=0.79279, avg_loss=0.77537]\n",
            "Step 23794   [1.665 sec/step, loss=0.76908, avg_loss=0.77498]\n",
            "Step 23795   [1.662 sec/step, loss=0.75886, avg_loss=0.77494]\n",
            "Step 23796   [1.665 sec/step, loss=0.80025, avg_loss=0.77495]\n",
            "Step 23797   [1.673 sec/step, loss=0.75248, avg_loss=0.77510]\n",
            "Step 23798   [1.676 sec/step, loss=0.84453, avg_loss=0.77536]\n",
            "Step 23799   [1.656 sec/step, loss=0.75488, avg_loss=0.77647]\n",
            "Step 23800   [1.651 sec/step, loss=0.74093, avg_loss=0.77652]\n",
            "Writing summary at step: 23800\n",
            "Step 23801   [1.656 sec/step, loss=0.77205, avg_loss=0.77605]\n",
            "Step 23802   [1.676 sec/step, loss=0.79749, avg_loss=0.77622]\n",
            "Step 23803   [1.680 sec/step, loss=0.83416, avg_loss=0.77720]\n",
            "Step 23804   [1.686 sec/step, loss=0.78460, avg_loss=0.77702]\n",
            "Step 23805   [1.681 sec/step, loss=0.78773, avg_loss=0.77684]\n",
            "Step 23806   [1.682 sec/step, loss=0.78406, avg_loss=0.77630]\n",
            "Step 23807   [1.690 sec/step, loss=0.77416, avg_loss=0.77635]\n",
            "Step 23808   [1.690 sec/step, loss=0.77306, avg_loss=0.77625]\n",
            "Step 23809   [1.694 sec/step, loss=0.76116, avg_loss=0.77637]\n",
            "Step 23810   [1.697 sec/step, loss=0.79869, avg_loss=0.77686]\n",
            "Step 23811   [1.699 sec/step, loss=0.76160, avg_loss=0.77678]\n",
            "Step 23812   [1.703 sec/step, loss=0.70070, avg_loss=0.77588]\n",
            "Generated 32 batches of size 8 in 7.738 sec\n",
            "Step 23813   [1.689 sec/step, loss=0.71546, avg_loss=0.77511]\n",
            "Step 23814   [1.692 sec/step, loss=0.78116, avg_loss=0.77555]\n",
            "Step 23815   [1.694 sec/step, loss=0.72557, avg_loss=0.77499]\n",
            "Step 23816   [1.686 sec/step, loss=0.77620, avg_loss=0.77512]\n",
            "Step 23817   [1.673 sec/step, loss=0.73447, avg_loss=0.77506]\n",
            "Step 23818   [1.682 sec/step, loss=0.81124, avg_loss=0.77533]\n",
            "Step 23819   [1.682 sec/step, loss=0.80012, avg_loss=0.77528]\n",
            "Step 23820   [1.672 sec/step, loss=0.76883, avg_loss=0.77494]\n",
            "Step 23821   [1.682 sec/step, loss=0.77192, avg_loss=0.77467]\n",
            "Step 23822   [1.677 sec/step, loss=0.71228, avg_loss=0.77488]\n",
            "Step 23823   [1.672 sec/step, loss=0.74808, avg_loss=0.77421]\n",
            "Step 23824   [1.669 sec/step, loss=0.81786, avg_loss=0.77476]\n",
            "Step 23825   [1.654 sec/step, loss=0.73863, avg_loss=0.77456]\n",
            "Step 23826   [1.683 sec/step, loss=0.70234, avg_loss=0.77343]\n",
            "Step 23827   [1.693 sec/step, loss=0.78328, avg_loss=0.77354]\n",
            "Step 23828   [1.697 sec/step, loss=0.76732, avg_loss=0.77339]\n",
            "Step 23829   [1.698 sec/step, loss=0.79091, avg_loss=0.77342]\n",
            "Step 23830   [1.706 sec/step, loss=0.77507, avg_loss=0.77348]\n",
            "Step 23831   [1.707 sec/step, loss=0.77150, avg_loss=0.77359]\n",
            "Step 23832   [1.708 sec/step, loss=0.75769, avg_loss=0.77345]\n",
            "Step 23833   [1.716 sec/step, loss=0.77150, avg_loss=0.77338]\n",
            "Step 23834   [1.721 sec/step, loss=0.74759, avg_loss=0.77301]\n",
            "Step 23835   [1.729 sec/step, loss=0.76132, avg_loss=0.77266]\n",
            "Step 23836   [1.731 sec/step, loss=0.77730, avg_loss=0.77241]\n",
            "Step 23837   [1.720 sec/step, loss=0.80194, avg_loss=0.77236]\n",
            "Step 23838   [1.721 sec/step, loss=0.75754, avg_loss=0.77212]\n",
            "Step 23839   [1.719 sec/step, loss=0.75150, avg_loss=0.77190]\n",
            "Step 23840   [1.718 sec/step, loss=0.80104, avg_loss=0.77181]\n",
            "Step 23841   [1.707 sec/step, loss=0.79196, avg_loss=0.77261]\n",
            "Step 23842   [1.709 sec/step, loss=0.80278, avg_loss=0.77283]\n",
            "Step 23843   [1.730 sec/step, loss=0.74294, avg_loss=0.77221]\n",
            "Step 23844   [1.731 sec/step, loss=0.74569, avg_loss=0.77193]\n",
            "Generated 32 batches of size 8 in 7.570 sec\n",
            "Step 23845   [1.728 sec/step, loss=0.78274, avg_loss=0.77169]\n",
            "Step 23846   [1.722 sec/step, loss=0.73828, avg_loss=0.77183]\n",
            "Step 23847   [1.680 sec/step, loss=0.80779, avg_loss=0.77289]\n",
            "Step 23848   [1.676 sec/step, loss=0.81794, avg_loss=0.77343]\n",
            "Step 23849   [1.673 sec/step, loss=0.79006, avg_loss=0.77358]\n",
            "Step 23850   [1.664 sec/step, loss=0.79538, avg_loss=0.77342]\n",
            "Step 23851   [1.661 sec/step, loss=0.75677, avg_loss=0.77295]\n",
            "Step 23852   [1.664 sec/step, loss=0.77253, avg_loss=0.77319]\n",
            "Step 23853   [1.659 sec/step, loss=0.78385, avg_loss=0.77283]\n",
            "Step 23854   [1.656 sec/step, loss=0.82586, avg_loss=0.77373]\n",
            "Step 23855   [1.658 sec/step, loss=0.83112, avg_loss=0.77439]\n",
            "Step 23856   [1.662 sec/step, loss=0.69999, avg_loss=0.77331]\n",
            "Step 23857   [1.657 sec/step, loss=0.76726, avg_loss=0.77285]\n",
            "Step 23858   [1.657 sec/step, loss=0.79244, avg_loss=0.77316]\n",
            "Step 23859   [1.668 sec/step, loss=0.75939, avg_loss=0.77280]\n",
            "Step 23860   [1.665 sec/step, loss=0.74326, avg_loss=0.77218]\n",
            "Step 23861   [1.673 sec/step, loss=0.76251, avg_loss=0.77197]\n",
            "Step 23862   [1.682 sec/step, loss=0.76239, avg_loss=0.77198]\n",
            "Step 23863   [1.680 sec/step, loss=0.82480, avg_loss=0.77243]\n",
            "Step 23864   [1.699 sec/step, loss=0.77263, avg_loss=0.77251]\n",
            "Step 23865   [1.692 sec/step, loss=0.74611, avg_loss=0.77132]\n",
            "Step 23866   [1.695 sec/step, loss=0.81125, avg_loss=0.77151]\n",
            "Step 23867   [1.698 sec/step, loss=0.75185, avg_loss=0.77096]\n",
            "Step 23868   [1.698 sec/step, loss=0.80827, avg_loss=0.77119]\n",
            "Step 23869   [1.703 sec/step, loss=0.75147, avg_loss=0.77074]\n",
            "Step 23870   [1.697 sec/step, loss=0.78608, avg_loss=0.77100]\n",
            "Step 23871   [1.699 sec/step, loss=0.78142, avg_loss=0.77118]\n",
            "Step 23872   [1.708 sec/step, loss=0.75331, avg_loss=0.77048]\n",
            "Step 23873   [1.706 sec/step, loss=0.77674, avg_loss=0.77043]\n",
            "Step 23874   [1.709 sec/step, loss=0.71784, avg_loss=0.77010]\n",
            "Step 23875   [1.710 sec/step, loss=0.76048, avg_loss=0.77005]\n",
            "Step 23876   [1.712 sec/step, loss=0.80170, avg_loss=0.76980]\n",
            "Step 23877   [1.721 sec/step, loss=0.79894, avg_loss=0.76969]\n",
            "Generated 32 batches of size 8 in 8.368 sec\n",
            "Step 23878   [1.706 sec/step, loss=0.78828, avg_loss=0.76993]\n",
            "Step 23879   [1.691 sec/step, loss=0.74798, avg_loss=0.76936]\n",
            "Step 23880   [1.688 sec/step, loss=0.80005, avg_loss=0.76945]\n",
            "Step 23881   [1.713 sec/step, loss=0.66842, avg_loss=0.76862]\n",
            "Step 23882   [1.713 sec/step, loss=0.75993, avg_loss=0.76837]\n",
            "Step 23883   [1.704 sec/step, loss=0.72713, avg_loss=0.76745]\n",
            "Step 23884   [1.703 sec/step, loss=0.78555, avg_loss=0.76785]\n",
            "Step 23885   [1.698 sec/step, loss=0.79125, avg_loss=0.76849]\n",
            "Step 23886   [1.663 sec/step, loss=0.79058, avg_loss=0.76963]\n",
            "Step 23887   [1.668 sec/step, loss=0.72800, avg_loss=0.76921]\n",
            "Step 23888   [1.640 sec/step, loss=0.75446, avg_loss=0.77060]\n",
            "Step 23889   [1.638 sec/step, loss=0.79055, avg_loss=0.77113]\n",
            "Step 23890   [1.628 sec/step, loss=0.80349, avg_loss=0.77146]\n",
            "Step 23891   [1.657 sec/step, loss=0.64956, avg_loss=0.77016]\n",
            "Step 23892   [1.659 sec/step, loss=0.77205, avg_loss=0.77036]\n",
            "Step 23893   [1.665 sec/step, loss=0.68690, avg_loss=0.76930]\n",
            "Step 23894   [1.666 sec/step, loss=0.77848, avg_loss=0.76940]\n",
            "Step 23895   [1.669 sec/step, loss=0.74886, avg_loss=0.76930]\n",
            "Step 23896   [1.674 sec/step, loss=0.75242, avg_loss=0.76882]\n",
            "Step 23897   [1.667 sec/step, loss=0.77812, avg_loss=0.76907]\n",
            "Step 23898   [1.667 sec/step, loss=0.82623, avg_loss=0.76889]\n",
            "Step 23899   [1.662 sec/step, loss=0.81865, avg_loss=0.76953]\n",
            "Step 23900   [1.664 sec/step, loss=0.76826, avg_loss=0.76980]\n",
            "Writing summary at step: 23900\n",
            "Step 23901   [1.656 sec/step, loss=0.78862, avg_loss=0.76997]\n",
            "Step 23902   [1.644 sec/step, loss=0.73543, avg_loss=0.76935]\n",
            "Step 23903   [1.647 sec/step, loss=0.75995, avg_loss=0.76860]\n",
            "Step 23904   [1.645 sec/step, loss=0.81021, avg_loss=0.76886]\n",
            "Step 23905   [1.654 sec/step, loss=0.78741, avg_loss=0.76886]\n",
            "Step 23906   [1.658 sec/step, loss=0.76530, avg_loss=0.76867]\n",
            "Step 23907   [1.652 sec/step, loss=0.82117, avg_loss=0.76914]\n",
            "Step 23908   [1.654 sec/step, loss=0.75469, avg_loss=0.76896]\n",
            "Generated 32 batches of size 8 in 8.206 sec\n",
            "Step 23909   [1.653 sec/step, loss=0.80082, avg_loss=0.76935]\n",
            "Step 23910   [1.646 sec/step, loss=0.74043, avg_loss=0.76877]\n",
            "Step 23911   [1.656 sec/step, loss=0.78655, avg_loss=0.76902]\n",
            "Step 23912   [1.647 sec/step, loss=0.75887, avg_loss=0.76960]\n",
            "Step 23913   [1.658 sec/step, loss=0.84928, avg_loss=0.77094]\n",
            "Step 23914   [1.643 sec/step, loss=0.80963, avg_loss=0.77122]\n",
            "Step 23915   [1.641 sec/step, loss=0.76566, avg_loss=0.77163]\n",
            "Step 23916   [1.645 sec/step, loss=0.76658, avg_loss=0.77153]\n",
            "Step 23917   [1.660 sec/step, loss=0.66086, avg_loss=0.77079]\n",
            "Step 23918   [1.667 sec/step, loss=0.82272, avg_loss=0.77091]\n",
            "Step 23919   [1.663 sec/step, loss=0.75909, avg_loss=0.77050]\n",
            "Step 23920   [1.662 sec/step, loss=0.78626, avg_loss=0.77067]\n",
            "Step 23921   [1.654 sec/step, loss=0.77462, avg_loss=0.77070]\n",
            "Step 23922   [1.663 sec/step, loss=0.81033, avg_loss=0.77168]\n",
            "Step 23923   [1.667 sec/step, loss=0.72853, avg_loss=0.77148]\n",
            "Step 23924   [1.691 sec/step, loss=0.78168, avg_loss=0.77112]\n",
            "Step 23925   [1.687 sec/step, loss=0.77955, avg_loss=0.77153]\n",
            "Step 23926   [1.652 sec/step, loss=0.71047, avg_loss=0.77161]\n",
            "Step 23927   [1.636 sec/step, loss=0.77894, avg_loss=0.77157]\n",
            "Step 23928   [1.631 sec/step, loss=0.77138, avg_loss=0.77161]\n",
            "Step 23929   [1.629 sec/step, loss=0.77540, avg_loss=0.77145]\n",
            "Step 23930   [1.623 sec/step, loss=0.68281, avg_loss=0.77053]\n",
            "Step 23931   [1.630 sec/step, loss=0.79139, avg_loss=0.77073]\n",
            "Step 23932   [1.628 sec/step, loss=0.80647, avg_loss=0.77122]\n",
            "Step 23933   [1.624 sec/step, loss=0.74399, avg_loss=0.77094]\n",
            "Step 23934   [1.627 sec/step, loss=0.77982, avg_loss=0.77127]\n",
            "Step 23935   [1.625 sec/step, loss=0.79332, avg_loss=0.77159]\n",
            "Step 23936   [1.622 sec/step, loss=0.77402, avg_loss=0.77155]\n",
            "Step 23937   [1.630 sec/step, loss=0.79773, avg_loss=0.77151]\n",
            "Step 23938   [1.631 sec/step, loss=0.80620, avg_loss=0.77200]\n",
            "Step 23939   [1.637 sec/step, loss=0.79129, avg_loss=0.77240]\n",
            "Step 23940   [1.639 sec/step, loss=0.76059, avg_loss=0.77199]\n",
            "Step 23941   [1.637 sec/step, loss=0.76783, avg_loss=0.77175]\n",
            "Generated 32 batches of size 8 in 9.438 sec\n",
            "Step 23942   [1.635 sec/step, loss=0.75947, avg_loss=0.77132]\n",
            "Step 23943   [1.613 sec/step, loss=0.79649, avg_loss=0.77185]\n",
            "Step 23944   [1.618 sec/step, loss=0.73642, avg_loss=0.77176]\n",
            "Step 23945   [1.616 sec/step, loss=0.77428, avg_loss=0.77167]\n",
            "Step 23946   [1.616 sec/step, loss=0.77836, avg_loss=0.77208]\n",
            "Step 23947   [1.632 sec/step, loss=0.77950, avg_loss=0.77179]\n",
            "Step 23948   [1.630 sec/step, loss=0.75646, avg_loss=0.77118]\n",
            "Step 23949   [1.638 sec/step, loss=0.72420, avg_loss=0.77052]\n",
            "Step 23950   [1.641 sec/step, loss=0.76767, avg_loss=0.77024]\n",
            "Step 23951   [1.641 sec/step, loss=0.80739, avg_loss=0.77075]\n",
            "Step 23952   [1.639 sec/step, loss=0.79936, avg_loss=0.77102]\n",
            "Step 23953   [1.633 sec/step, loss=0.79568, avg_loss=0.77114]\n",
            "Step 23954   [1.638 sec/step, loss=0.72816, avg_loss=0.77016]\n",
            "Step 23955   [1.631 sec/step, loss=0.77595, avg_loss=0.76961]\n",
            "Step 23956   [1.631 sec/step, loss=0.78554, avg_loss=0.77046]\n",
            "Step 23957   [1.639 sec/step, loss=0.79422, avg_loss=0.77073]\n",
            "Step 23958   [1.642 sec/step, loss=0.75718, avg_loss=0.77038]\n",
            "Step 23959   [1.674 sec/step, loss=0.85626, avg_loss=0.77135]\n",
            "Step 23960   [1.679 sec/step, loss=0.81925, avg_loss=0.77211]\n",
            "Step 23961   [1.692 sec/step, loss=0.78086, avg_loss=0.77229]\n",
            "Step 23962   [1.680 sec/step, loss=0.75762, avg_loss=0.77224]\n",
            "Step 23963   [1.691 sec/step, loss=0.78656, avg_loss=0.77186]\n",
            "Step 23964   [1.678 sec/step, loss=0.83214, avg_loss=0.77246]\n",
            "Step 23965   [1.674 sec/step, loss=0.73171, avg_loss=0.77231]\n",
            "Step 23966   [1.673 sec/step, loss=0.73963, avg_loss=0.77160]\n",
            "Step 23967   [1.678 sec/step, loss=0.76154, avg_loss=0.77169]\n",
            "Step 23968   [1.696 sec/step, loss=0.74040, avg_loss=0.77101]\n",
            "Step 23969   [1.696 sec/step, loss=0.78899, avg_loss=0.77139]\n",
            "Step 23970   [1.705 sec/step, loss=0.78998, avg_loss=0.77143]\n",
            "Step 23971   [1.708 sec/step, loss=0.78041, avg_loss=0.77142]\n",
            "Step 23972   [1.699 sec/step, loss=0.77356, avg_loss=0.77162]\n",
            "Generated 32 batches of size 8 in 9.723 sec\n",
            "Step 23973   [1.698 sec/step, loss=0.83626, avg_loss=0.77222]\n",
            "Step 23974   [1.695 sec/step, loss=0.76451, avg_loss=0.77268]\n",
            "Step 23975   [1.697 sec/step, loss=0.73388, avg_loss=0.77242]\n",
            "Step 23976   [1.695 sec/step, loss=0.76666, avg_loss=0.77207]\n",
            "Step 23977   [1.684 sec/step, loss=0.83069, avg_loss=0.77238]\n",
            "Step 23978   [1.689 sec/step, loss=0.79860, avg_loss=0.77249]\n",
            "Step 23979   [1.694 sec/step, loss=0.83287, avg_loss=0.77334]\n",
            "Step 23980   [1.692 sec/step, loss=0.83171, avg_loss=0.77365]\n",
            "Step 23981   [1.654 sec/step, loss=0.78493, avg_loss=0.77482]\n",
            "Step 23982   [1.651 sec/step, loss=0.77455, avg_loss=0.77496]\n",
            "Step 23983   [1.652 sec/step, loss=0.79540, avg_loss=0.77565]\n",
            "Step 23984   [1.685 sec/step, loss=0.75369, avg_loss=0.77533]\n",
            "Step 23985   [1.683 sec/step, loss=0.81266, avg_loss=0.77554]\n",
            "Step 23986   [1.682 sec/step, loss=0.80210, avg_loss=0.77566]\n",
            "Step 23987   [1.666 sec/step, loss=0.71881, avg_loss=0.77557]\n",
            "Step 23988   [1.657 sec/step, loss=0.67211, avg_loss=0.77474]\n",
            "Step 23989   [1.658 sec/step, loss=0.80212, avg_loss=0.77486]\n",
            "Step 23990   [1.665 sec/step, loss=0.82569, avg_loss=0.77508]\n",
            "Step 23991   [1.636 sec/step, loss=0.82933, avg_loss=0.77688]\n",
            "Step 23992   [1.632 sec/step, loss=0.81544, avg_loss=0.77731]\n",
            "Step 23993   [1.645 sec/step, loss=0.82949, avg_loss=0.77874]\n",
            "Step 23994   [1.644 sec/step, loss=0.78220, avg_loss=0.77877]\n",
            "Step 23995   [1.642 sec/step, loss=0.73827, avg_loss=0.77867]\n",
            "Step 23996   [1.638 sec/step, loss=0.80392, avg_loss=0.77918]\n",
            "Step 23997   [1.636 sec/step, loss=0.79191, avg_loss=0.77932]\n",
            "Step 23998   [1.639 sec/step, loss=0.84265, avg_loss=0.77949]\n",
            "Step 23999   [1.630 sec/step, loss=0.82423, avg_loss=0.77954]\n",
            "Step 24000   [1.632 sec/step, loss=0.75457, avg_loss=0.77940]\n",
            "Writing summary at step: 24000\n",
            "Saving checkpoint to: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/model.ckpt-24000\n",
            "Step 24001   [1.634 sec/step, loss=0.80309, avg_loss=0.77955]\n",
            "Step 24002   [1.631 sec/step, loss=0.79197, avg_loss=0.78011]\n",
            "Step 24003   [1.625 sec/step, loss=0.80432, avg_loss=0.78056]\n",
            "Generated 32 batches of size 8 in 11.635 sec\n",
            "Step 24004   [1.638 sec/step, loss=0.73227, avg_loss=0.77978]\n",
            "Step 24005   [1.633 sec/step, loss=0.83243, avg_loss=0.78023]\n",
            "Step 24006   [1.636 sec/step, loss=0.81129, avg_loss=0.78069]\n",
            "Step 24007   [1.640 sec/step, loss=0.72829, avg_loss=0.77976]\n",
            "Step 24008   [1.642 sec/step, loss=0.80390, avg_loss=0.78025]\n",
            "Step 24009   [1.652 sec/step, loss=0.79154, avg_loss=0.78016]\n",
            "Step 24010   [1.654 sec/step, loss=0.82701, avg_loss=0.78102]\n",
            "Step 24011   [1.634 sec/step, loss=0.76717, avg_loss=0.78083]\n",
            "Step 24012   [1.640 sec/step, loss=0.80272, avg_loss=0.78127]\n",
            "Step 24013   [1.629 sec/step, loss=0.74440, avg_loss=0.78022]\n",
            "Step 24014   [1.631 sec/step, loss=0.78480, avg_loss=0.77997]\n",
            "Step 24015   [1.640 sec/step, loss=0.72883, avg_loss=0.77960]\n",
            "Step 24016   [1.639 sec/step, loss=0.72273, avg_loss=0.77917]\n",
            "Step 24017   [1.622 sec/step, loss=0.81142, avg_loss=0.78067]\n",
            "Step 24018   [1.610 sec/step, loss=0.76031, avg_loss=0.78005]\n",
            "Step 24019   [1.606 sec/step, loss=0.75765, avg_loss=0.78003]\n",
            "Step 24020   [1.612 sec/step, loss=0.81421, avg_loss=0.78031]\n",
            "Step 24021   [1.611 sec/step, loss=0.78002, avg_loss=0.78037]\n",
            "Step 24022   [1.604 sec/step, loss=0.80688, avg_loss=0.78033]\n",
            "Step 24023   [1.599 sec/step, loss=0.71806, avg_loss=0.78023]\n",
            "Step 24024   [1.614 sec/step, loss=0.72205, avg_loss=0.77963]\n",
            "Step 24025   [1.623 sec/step, loss=0.80396, avg_loss=0.77987]\n",
            "Step 24026   [1.621 sec/step, loss=0.76562, avg_loss=0.78043]\n",
            "Step 24027   [1.622 sec/step, loss=0.77236, avg_loss=0.78036]\n",
            "Step 24028   [1.629 sec/step, loss=0.77739, avg_loss=0.78042]\n",
            "Step 24029   [1.627 sec/step, loss=0.76645, avg_loss=0.78033]\n",
            "Step 24030   [1.625 sec/step, loss=0.78714, avg_loss=0.78137]\n",
            "Step 24031   [1.633 sec/step, loss=0.78474, avg_loss=0.78131]\n",
            "Step 24032   [1.639 sec/step, loss=0.75950, avg_loss=0.78084]\n",
            "Step 24033   [1.643 sec/step, loss=0.72836, avg_loss=0.78068]\n",
            "Step 24034   [1.635 sec/step, loss=0.72930, avg_loss=0.78018]\n",
            "Step 24035   [1.638 sec/step, loss=0.80475, avg_loss=0.78029]\n",
            "Generated 32 batches of size 8 in 10.341 sec\n",
            "Step 24036   [1.640 sec/step, loss=0.73809, avg_loss=0.77993]\n",
            "Step 24037   [1.644 sec/step, loss=0.82076, avg_loss=0.78016]\n",
            "Step 24038   [1.665 sec/step, loss=0.78055, avg_loss=0.77991]\n",
            "Step 24039   [1.659 sec/step, loss=0.81095, avg_loss=0.78010]\n",
            "Step 24040   [1.661 sec/step, loss=0.76735, avg_loss=0.78017]\n",
            "Step 24041   [1.661 sec/step, loss=0.75910, avg_loss=0.78008]\n",
            "Step 24042   [1.668 sec/step, loss=0.73819, avg_loss=0.77987]\n",
            "Step 24043   [1.677 sec/step, loss=0.79724, avg_loss=0.77988]\n",
            "Step 24044   [1.685 sec/step, loss=0.75806, avg_loss=0.78009]\n",
            "Step 24045   [1.685 sec/step, loss=0.79952, avg_loss=0.78035]\n",
            "Step 24046   [1.688 sec/step, loss=0.80803, avg_loss=0.78064]\n",
            "Step 24047   [1.674 sec/step, loss=0.73517, avg_loss=0.78020]\n",
            "Step 24048   [1.674 sec/step, loss=0.80184, avg_loss=0.78065]\n",
            "Step 24049   [1.665 sec/step, loss=0.69316, avg_loss=0.78034]\n",
            "Step 24050   [1.662 sec/step, loss=0.76364, avg_loss=0.78030]\n",
            "Step 24051   [1.670 sec/step, loss=0.76384, avg_loss=0.77987]\n",
            "Step 24052   [1.673 sec/step, loss=0.75328, avg_loss=0.77941]\n",
            "Step 24053   [1.675 sec/step, loss=0.77532, avg_loss=0.77920]\n",
            "Step 24054   [1.672 sec/step, loss=0.77887, avg_loss=0.77971]\n",
            "Step 24055   [1.683 sec/step, loss=0.86766, avg_loss=0.78063]\n",
            "Step 24056   [1.685 sec/step, loss=0.80061, avg_loss=0.78078]\n",
            "Step 24057   [1.716 sec/step, loss=0.72230, avg_loss=0.78006]\n",
            "Step 24058   [1.718 sec/step, loss=0.77606, avg_loss=0.78025]\n",
            "Step 24059   [1.678 sec/step, loss=0.77102, avg_loss=0.77939]\n",
            "Step 24060   [1.676 sec/step, loss=0.77162, avg_loss=0.77892]\n",
            "Step 24061   [1.652 sec/step, loss=0.77947, avg_loss=0.77890]\n",
            "Step 24062   [1.652 sec/step, loss=0.75642, avg_loss=0.77889]\n",
            "Step 24063   [1.641 sec/step, loss=0.80155, avg_loss=0.77904]\n",
            "Step 24064   [1.662 sec/step, loss=0.71833, avg_loss=0.77790]\n",
            "Step 24065   [1.665 sec/step, loss=0.73419, avg_loss=0.77793]\n",
            "Step 24066   [1.677 sec/step, loss=0.75459, avg_loss=0.77808]\n",
            "Generated 32 batches of size 8 in 9.915 sec\n",
            "Step 24067   [1.677 sec/step, loss=0.83916, avg_loss=0.77885]\n",
            "Step 24068   [1.657 sec/step, loss=0.76806, avg_loss=0.77913]\n",
            "Step 24069   [1.653 sec/step, loss=0.72256, avg_loss=0.77847]\n",
            "Step 24070   [1.657 sec/step, loss=0.76898, avg_loss=0.77826]\n",
            "Step 24071   [1.656 sec/step, loss=0.77905, avg_loss=0.77824]\n",
            "Step 24072   [1.652 sec/step, loss=0.76245, avg_loss=0.77813]\n",
            "Step 24073   [1.653 sec/step, loss=0.78939, avg_loss=0.77766]\n",
            "Step 24074   [1.658 sec/step, loss=0.72694, avg_loss=0.77729]\n",
            "Step 24075   [1.652 sec/step, loss=0.81326, avg_loss=0.77808]\n",
            "Step 24076   [1.655 sec/step, loss=0.80273, avg_loss=0.77844]\n",
            "Step 24077   [1.662 sec/step, loss=0.70154, avg_loss=0.77715]\n",
            "Step 24078   [1.652 sec/step, loss=0.80340, avg_loss=0.77720]\n",
            "Step 24079   [1.652 sec/step, loss=0.78733, avg_loss=0.77674]\n",
            "Step 24080   [1.649 sec/step, loss=0.75515, avg_loss=0.77598]\n",
            "Step 24081   [1.642 sec/step, loss=0.79429, avg_loss=0.77607]\n",
            "Step 24082   [1.643 sec/step, loss=0.81474, avg_loss=0.77647]\n",
            "Step 24083   [1.642 sec/step, loss=0.78809, avg_loss=0.77640]\n",
            "Step 24084   [1.603 sec/step, loss=0.72805, avg_loss=0.77614]\n",
            "Step 24085   [1.608 sec/step, loss=0.80477, avg_loss=0.77606]\n",
            "Step 24086   [1.607 sec/step, loss=0.81523, avg_loss=0.77620]\n",
            "Step 24087   [1.607 sec/step, loss=0.79899, avg_loss=0.77700]\n",
            "Step 24088   [1.604 sec/step, loss=0.72019, avg_loss=0.77748]\n",
            "Step 24089   [1.606 sec/step, loss=0.80995, avg_loss=0.77756]\n",
            "Step 24090   [1.601 sec/step, loss=0.82441, avg_loss=0.77754]\n",
            "Step 24091   [1.598 sec/step, loss=0.78837, avg_loss=0.77713]\n",
            "Step 24092   [1.602 sec/step, loss=0.67920, avg_loss=0.77577]\n",
            "Step 24093   [1.587 sec/step, loss=0.75458, avg_loss=0.77502]\n",
            "Step 24094   [1.586 sec/step, loss=0.75956, avg_loss=0.77480]\n",
            "Step 24095   [1.592 sec/step, loss=0.81701, avg_loss=0.77558]\n",
            "Step 24096   [1.596 sec/step, loss=0.78601, avg_loss=0.77540]\n",
            "Step 24097   [1.607 sec/step, loss=0.76089, avg_loss=0.77509]\n",
            "Step 24098   [1.626 sec/step, loss=0.75535, avg_loss=0.77422]\n",
            "Generated 32 batches of size 8 in 10.525 sec\n",
            "Step 24099   [1.627 sec/step, loss=0.78961, avg_loss=0.77388]\n",
            "Step 24100   [1.661 sec/step, loss=0.69425, avg_loss=0.77327]\n",
            "Writing summary at step: 24100\n",
            "Step 24101   [1.662 sec/step, loss=0.77482, avg_loss=0.77299]\n",
            "Step 24102   [1.669 sec/step, loss=0.77210, avg_loss=0.77279]\n",
            "Step 24103   [1.670 sec/step, loss=0.81292, avg_loss=0.77288]\n",
            "Step 24104   [1.657 sec/step, loss=0.76449, avg_loss=0.77320]\n",
            "Step 24105   [1.666 sec/step, loss=0.83057, avg_loss=0.77318]\n",
            "Step 24106   [1.658 sec/step, loss=0.78125, avg_loss=0.77288]\n",
            "Step 24107   [1.660 sec/step, loss=0.80265, avg_loss=0.77362]\n",
            "Step 24108   [1.657 sec/step, loss=0.78481, avg_loss=0.77343]\n",
            "Step 24109   [1.647 sec/step, loss=0.78219, avg_loss=0.77334]\n",
            "Step 24110   [1.668 sec/step, loss=0.82221, avg_loss=0.77329]\n",
            "Step 24111   [1.674 sec/step, loss=0.81490, avg_loss=0.77377]\n",
            "Step 24112   [1.672 sec/step, loss=0.82177, avg_loss=0.77396]\n",
            "Step 24113   [1.671 sec/step, loss=0.80872, avg_loss=0.77460]\n",
            "Step 24114   [1.668 sec/step, loss=0.74522, avg_loss=0.77421]\n",
            "Step 24115   [1.661 sec/step, loss=0.79817, avg_loss=0.77490]\n",
            "Step 24116   [1.659 sec/step, loss=0.76172, avg_loss=0.77529]\n",
            "Step 24117   [1.663 sec/step, loss=0.80743, avg_loss=0.77525]\n",
            "Step 24118   [1.660 sec/step, loss=0.79954, avg_loss=0.77564]\n",
            "Step 24119   [1.660 sec/step, loss=0.79615, avg_loss=0.77603]\n",
            "Step 24120   [1.663 sec/step, loss=0.76801, avg_loss=0.77557]\n",
            "Step 24121   [1.695 sec/step, loss=0.79326, avg_loss=0.77570]\n",
            "Step 24122   [1.703 sec/step, loss=0.75752, avg_loss=0.77520]\n",
            "Step 24123   [1.704 sec/step, loss=0.74876, avg_loss=0.77551]\n",
            "Step 24124   [1.665 sec/step, loss=0.76696, avg_loss=0.77596]\n",
            "Step 24125   [1.659 sec/step, loss=0.79506, avg_loss=0.77587]\n",
            "Step 24126   [1.679 sec/step, loss=0.73258, avg_loss=0.77554]\n",
            "Step 24127   [1.689 sec/step, loss=0.77368, avg_loss=0.77555]\n",
            "Step 24128   [1.693 sec/step, loss=0.77282, avg_loss=0.77551]\n",
            "Step 24129   [1.700 sec/step, loss=0.77501, avg_loss=0.77559]\n",
            "Generated 32 batches of size 8 in 10.539 sec\n",
            "Step 24130   [1.702 sec/step, loss=0.78782, avg_loss=0.77560]\n",
            "Step 24131   [1.690 sec/step, loss=0.75743, avg_loss=0.77533]\n",
            "Step 24132   [1.697 sec/step, loss=0.79424, avg_loss=0.77568]\n",
            "Step 24133   [1.694 sec/step, loss=0.75051, avg_loss=0.77590]\n",
            "Step 24134   [1.692 sec/step, loss=0.73950, avg_loss=0.77600]\n",
            "Step 24135   [1.692 sec/step, loss=0.79727, avg_loss=0.77592]\n",
            "Step 24136   [1.686 sec/step, loss=0.81922, avg_loss=0.77674]\n",
            "Step 24137   [1.674 sec/step, loss=0.83278, avg_loss=0.77686]\n",
            "Step 24138   [1.650 sec/step, loss=0.76463, avg_loss=0.77670]\n",
            "Step 24139   [1.649 sec/step, loss=0.75106, avg_loss=0.77610]\n",
            "Step 24140   [1.646 sec/step, loss=0.82890, avg_loss=0.77671]\n",
            "Step 24141   [1.643 sec/step, loss=0.79784, avg_loss=0.77710]\n",
            "Step 24142   [1.679 sec/step, loss=0.72011, avg_loss=0.77692]\n",
            "Step 24143   [1.681 sec/step, loss=0.79254, avg_loss=0.77687]\n",
            "Step 24144   [1.670 sec/step, loss=0.70758, avg_loss=0.77637]\n",
            "Step 24145   [1.669 sec/step, loss=0.78517, avg_loss=0.77622]\n",
            "Step 24146   [1.675 sec/step, loss=0.80453, avg_loss=0.77619]\n",
            "Step 24147   [1.676 sec/step, loss=0.76467, avg_loss=0.77648]\n",
            "Step 24148   [1.689 sec/step, loss=0.75122, avg_loss=0.77598]\n",
            "Step 24149   [1.697 sec/step, loss=0.78215, avg_loss=0.77687]\n",
            "Step 24150   [1.702 sec/step, loss=0.82833, avg_loss=0.77751]\n",
            "Step 24151   [1.711 sec/step, loss=0.80978, avg_loss=0.77797]\n",
            "Step 24152   [1.705 sec/step, loss=0.74773, avg_loss=0.77792]\n",
            "Step 24153   [1.709 sec/step, loss=0.74770, avg_loss=0.77764]\n",
            "Step 24154   [1.708 sec/step, loss=0.77598, avg_loss=0.77761]\n",
            "Step 24155   [1.708 sec/step, loss=0.77444, avg_loss=0.77668]\n",
            "Step 24156   [1.703 sec/step, loss=0.79267, avg_loss=0.77660]\n",
            "Step 24157   [1.663 sec/step, loss=0.78280, avg_loss=0.77721]\n",
            "Step 24158   [1.669 sec/step, loss=0.75345, avg_loss=0.77698]\n",
            "Step 24159   [1.672 sec/step, loss=0.79759, avg_loss=0.77725]\n",
            "Step 24160   [1.676 sec/step, loss=0.78318, avg_loss=0.77736]\n",
            "Step 24161   [1.672 sec/step, loss=0.79896, avg_loss=0.77756]\n",
            "Generated 32 batches of size 8 in 7.824 sec\n",
            "Step 24162   [1.682 sec/step, loss=0.76021, avg_loss=0.77759]\n",
            "Step 24163   [1.706 sec/step, loss=0.90241, avg_loss=0.77860]\n",
            "Step 24164   [1.679 sec/step, loss=0.75350, avg_loss=0.77896]\n",
            "Step 24165   [1.674 sec/step, loss=0.78015, avg_loss=0.77941]\n",
            "Step 24166   [1.663 sec/step, loss=0.81353, avg_loss=0.78000]\n",
            "Step 24167   [1.658 sec/step, loss=0.79081, avg_loss=0.77952]\n",
            "Step 24168   [1.660 sec/step, loss=0.78352, avg_loss=0.77968]\n",
            "Step 24169   [1.658 sec/step, loss=0.81483, avg_loss=0.78060]\n",
            "Step 24170   [1.645 sec/step, loss=0.75750, avg_loss=0.78048]\n",
            "Step 24171   [1.657 sec/step, loss=0.79744, avg_loss=0.78067]\n",
            "Step 24172   [1.668 sec/step, loss=0.81674, avg_loss=0.78121]\n",
            "Step 24173   [1.662 sec/step, loss=0.79512, avg_loss=0.78127]\n",
            "Step 24174   [1.699 sec/step, loss=0.72549, avg_loss=0.78125]\n",
            "Step 24175   [1.705 sec/step, loss=0.78994, avg_loss=0.78102]\n",
            "Step 24176   [1.704 sec/step, loss=0.74253, avg_loss=0.78042]\n",
            "Step 24177   [1.703 sec/step, loss=0.80920, avg_loss=0.78149]\n",
            "Step 24178   [1.702 sec/step, loss=0.86022, avg_loss=0.78206]\n",
            "Step 24179   [1.698 sec/step, loss=0.77950, avg_loss=0.78198]\n",
            "Step 24180   [1.702 sec/step, loss=0.75673, avg_loss=0.78200]\n",
            "Step 24181   [1.709 sec/step, loss=0.80378, avg_loss=0.78209]\n",
            "Step 24182   [1.711 sec/step, loss=0.78186, avg_loss=0.78177]\n",
            "Step 24183   [1.708 sec/step, loss=0.77334, avg_loss=0.78162]\n",
            "Step 24184   [1.717 sec/step, loss=0.73064, avg_loss=0.78164]\n",
            "Step 24185   [1.712 sec/step, loss=0.81840, avg_loss=0.78178]\n",
            "Step 24186   [1.709 sec/step, loss=0.81912, avg_loss=0.78182]\n",
            "Step 24187   [1.711 sec/step, loss=0.79096, avg_loss=0.78174]\n",
            "Step 24188   [1.718 sec/step, loss=0.75225, avg_loss=0.78206]\n",
            "Step 24189   [1.742 sec/step, loss=0.74794, avg_loss=0.78144]\n",
            "Step 24190   [1.744 sec/step, loss=0.75943, avg_loss=0.78079]\n",
            "Step 24191   [1.760 sec/step, loss=0.81991, avg_loss=0.78111]\n",
            "Step 24192   [1.766 sec/step, loss=0.76789, avg_loss=0.78199]\n",
            "Generated 32 batches of size 8 in 7.366 sec\n",
            "Step 24193   [1.768 sec/step, loss=0.82737, avg_loss=0.78272]\n",
            "Step 24194   [1.767 sec/step, loss=0.79977, avg_loss=0.78312]\n",
            "Step 24195   [1.765 sec/step, loss=0.77860, avg_loss=0.78274]\n",
            "Step 24196   [1.761 sec/step, loss=0.82426, avg_loss=0.78312]\n",
            "Step 24197   [1.769 sec/step, loss=0.75129, avg_loss=0.78302]\n",
            "Step 24198   [1.740 sec/step, loss=0.77483, avg_loss=0.78322]\n",
            "Step 24199   [1.738 sec/step, loss=0.77750, avg_loss=0.78310]\n",
            "Step 24200   [1.703 sec/step, loss=0.79995, avg_loss=0.78416]\n",
            "Writing summary at step: 24200\n",
            "Step 24201   [1.712 sec/step, loss=0.80280, avg_loss=0.78444]\n",
            "Step 24202   [1.707 sec/step, loss=0.77335, avg_loss=0.78445]\n",
            "Step 24203   [1.707 sec/step, loss=0.75433, avg_loss=0.78386]\n",
            "Step 24204   [1.704 sec/step, loss=0.77134, avg_loss=0.78393]\n",
            "Step 24205   [1.697 sec/step, loss=0.80482, avg_loss=0.78367]\n",
            "Step 24206   [1.696 sec/step, loss=0.80453, avg_loss=0.78391]\n",
            "Step 24207   [1.686 sec/step, loss=0.76351, avg_loss=0.78351]\n",
            "Step 24208   [1.688 sec/step, loss=0.78034, avg_loss=0.78347]\n",
            "Step 24209   [1.692 sec/step, loss=0.79236, avg_loss=0.78357]\n",
            "Step 24210   [1.674 sec/step, loss=0.75481, avg_loss=0.78290]\n",
            "Step 24211   [1.686 sec/step, loss=0.80235, avg_loss=0.78277]\n",
            "Step 24212   [1.728 sec/step, loss=0.71647, avg_loss=0.78172]\n",
            "Step 24213   [1.736 sec/step, loss=0.76955, avg_loss=0.78133]\n",
            "Step 24214   [1.736 sec/step, loss=0.78896, avg_loss=0.78176]\n",
            "Step 24215   [1.746 sec/step, loss=0.78116, avg_loss=0.78159]\n",
            "Step 24216   [1.746 sec/step, loss=0.73297, avg_loss=0.78131]\n",
            "Step 24217   [1.744 sec/step, loss=0.80582, avg_loss=0.78129]\n",
            "Step 24218   [1.769 sec/step, loss=0.82844, avg_loss=0.78158]\n",
            "Step 24219   [1.770 sec/step, loss=0.74820, avg_loss=0.78110]\n",
            "Step 24220   [1.770 sec/step, loss=0.77021, avg_loss=0.78112]\n",
            "Step 24221   [1.743 sec/step, loss=0.75111, avg_loss=0.78070]\n",
            "Step 24222   [1.735 sec/step, loss=0.77854, avg_loss=0.78091]\n",
            "Step 24223   [1.750 sec/step, loss=0.79631, avg_loss=0.78139]\n",
            "Step 24224   [1.753 sec/step, loss=0.80901, avg_loss=0.78181]\n",
            "Generated 32 batches of size 8 in 7.406 sec\n",
            "Step 24225   [1.753 sec/step, loss=0.79197, avg_loss=0.78178]\n",
            "Step 24226   [1.731 sec/step, loss=0.79067, avg_loss=0.78236]\n",
            "Step 24227   [1.730 sec/step, loss=0.82819, avg_loss=0.78290]\n",
            "Step 24228   [1.726 sec/step, loss=0.73422, avg_loss=0.78252]\n",
            "Step 24229   [1.722 sec/step, loss=0.78240, avg_loss=0.78259]\n",
            "Step 24230   [1.719 sec/step, loss=0.77743, avg_loss=0.78249]\n",
            "Step 24231   [1.712 sec/step, loss=0.82029, avg_loss=0.78311]\n",
            "Step 24232   [1.706 sec/step, loss=0.81782, avg_loss=0.78335]\n",
            "Step 24233   [1.708 sec/step, loss=0.84336, avg_loss=0.78428]\n",
            "Step 24234   [1.713 sec/step, loss=0.74396, avg_loss=0.78432]\n",
            "Step 24235   [1.708 sec/step, loss=0.76769, avg_loss=0.78403]\n",
            "Step 24236   [1.707 sec/step, loss=0.76034, avg_loss=0.78344]\n",
            "Step 24237   [1.706 sec/step, loss=0.79421, avg_loss=0.78305]\n",
            "Step 24238   [1.710 sec/step, loss=0.77306, avg_loss=0.78314]\n",
            "Step 24239   [1.714 sec/step, loss=0.74747, avg_loss=0.78310]\n",
            "Step 24240   [1.713 sec/step, loss=0.78320, avg_loss=0.78264]\n",
            "Step 24241   [1.726 sec/step, loss=0.81563, avg_loss=0.78282]\n",
            "Step 24242   [1.725 sec/step, loss=0.62571, avg_loss=0.78188]\n",
            "Step 24243   [1.720 sec/step, loss=0.74342, avg_loss=0.78139]\n",
            "Step 24244   [1.716 sec/step, loss=0.77214, avg_loss=0.78203]\n",
            "Step 24245   [1.716 sec/step, loss=0.81604, avg_loss=0.78234]\n",
            "Step 24246   [1.710 sec/step, loss=0.79243, avg_loss=0.78222]\n",
            "Step 24247   [1.709 sec/step, loss=0.78878, avg_loss=0.78246]\n",
            "Step 24248   [1.695 sec/step, loss=0.78378, avg_loss=0.78279]\n",
            "Step 24249   [1.692 sec/step, loss=0.81790, avg_loss=0.78314]\n",
            "Step 24250   [1.698 sec/step, loss=0.73193, avg_loss=0.78218]\n",
            "Step 24251   [1.681 sec/step, loss=0.78168, avg_loss=0.78190]\n",
            "Step 24252   [1.699 sec/step, loss=0.75917, avg_loss=0.78201]\n",
            "Step 24253   [1.707 sec/step, loss=0.80373, avg_loss=0.78257]\n",
            "Step 24254   [1.707 sec/step, loss=0.76781, avg_loss=0.78249]\n",
            "Step 24255   [1.715 sec/step, loss=0.77261, avg_loss=0.78247]\n",
            "Step 24256   [1.713 sec/step, loss=0.78848, avg_loss=0.78243]\n",
            "Generated 32 batches of size 8 in 7.839 sec\n",
            "Step 24257   [1.717 sec/step, loss=0.78147, avg_loss=0.78242]\n",
            "Step 24258   [1.714 sec/step, loss=0.74535, avg_loss=0.78234]\n",
            "Step 24259   [1.712 sec/step, loss=0.75821, avg_loss=0.78194]\n",
            "Step 24260   [1.705 sec/step, loss=0.69444, avg_loss=0.78106]\n",
            "Step 24261   [1.710 sec/step, loss=0.76684, avg_loss=0.78074]\n",
            "Step 24262   [1.705 sec/step, loss=0.79196, avg_loss=0.78105]\n",
            "Step 24263   [1.688 sec/step, loss=0.80934, avg_loss=0.78012]\n",
            "Step 24264   [1.690 sec/step, loss=0.80511, avg_loss=0.78064]\n",
            "Step 24265   [1.690 sec/step, loss=0.77790, avg_loss=0.78062]\n",
            "Step 24266   [1.687 sec/step, loss=0.77207, avg_loss=0.78020]\n",
            "Step 24267   [1.689 sec/step, loss=0.78442, avg_loss=0.78014]\n",
            "Step 24268   [1.690 sec/step, loss=0.78987, avg_loss=0.78020]\n",
            "Step 24269   [1.692 sec/step, loss=0.78740, avg_loss=0.77993]\n",
            "Step 24270   [1.694 sec/step, loss=0.79122, avg_loss=0.78026]\n",
            "Step 24271   [1.700 sec/step, loss=0.76386, avg_loss=0.77993]\n",
            "Step 24272   [1.697 sec/step, loss=0.77584, avg_loss=0.77952]\n",
            "Step 24273   [1.695 sec/step, loss=0.75450, avg_loss=0.77911]\n",
            "Step 24274   [1.658 sec/step, loss=0.76774, avg_loss=0.77954]\n",
            "Step 24275   [1.655 sec/step, loss=0.80578, avg_loss=0.77969]\n",
            "Step 24276   [1.657 sec/step, loss=0.78531, avg_loss=0.78012]\n",
            "Step 24277   [1.655 sec/step, loss=0.81746, avg_loss=0.78020]\n",
            "Step 24278   [1.652 sec/step, loss=0.78899, avg_loss=0.77949]\n",
            "Step 24279   [1.649 sec/step, loss=0.77932, avg_loss=0.77949]\n",
            "Step 24280   [1.643 sec/step, loss=0.73635, avg_loss=0.77929]\n",
            "Step 24281   [1.641 sec/step, loss=0.75924, avg_loss=0.77884]\n",
            "Step 24282   [1.685 sec/step, loss=0.70691, avg_loss=0.77809]\n",
            "Step 24283   [1.689 sec/step, loss=0.76708, avg_loss=0.77803]\n",
            "Step 24284   [1.684 sec/step, loss=0.80058, avg_loss=0.77873]\n",
            "Step 24285   [1.684 sec/step, loss=0.79562, avg_loss=0.77850]\n",
            "Step 24286   [1.695 sec/step, loss=0.80140, avg_loss=0.77832]\n",
            "Step 24287   [1.706 sec/step, loss=0.77342, avg_loss=0.77815]\n",
            "Generated 32 batches of size 8 in 8.215 sec\n",
            "Step 24288   [1.718 sec/step, loss=0.71490, avg_loss=0.77777]\n",
            "Step 24289   [1.701 sec/step, loss=0.83241, avg_loss=0.77862]\n",
            "Step 24290   [1.698 sec/step, loss=0.80134, avg_loss=0.77904]\n",
            "Step 24291   [1.685 sec/step, loss=0.73976, avg_loss=0.77824]\n",
            "Step 24292   [1.677 sec/step, loss=0.73691, avg_loss=0.77793]\n",
            "Step 24293   [1.669 sec/step, loss=0.75694, avg_loss=0.77722]\n",
            "Step 24294   [1.670 sec/step, loss=0.74499, avg_loss=0.77667]\n",
            "Step 24295   [1.667 sec/step, loss=0.78606, avg_loss=0.77675]\n",
            "Step 24296   [1.662 sec/step, loss=0.81512, avg_loss=0.77666]\n",
            "Step 24297   [1.658 sec/step, loss=0.81744, avg_loss=0.77732]\n",
            "Step 24298   [1.661 sec/step, loss=0.78745, avg_loss=0.77745]\n",
            "Step 24299   [1.660 sec/step, loss=0.75563, avg_loss=0.77723]\n",
            "Step 24300   [1.660 sec/step, loss=0.77129, avg_loss=0.77694]\n",
            "Writing summary at step: 24300\n",
            "Step 24301   [1.651 sec/step, loss=0.73679, avg_loss=0.77628]\n",
            "Step 24302   [1.647 sec/step, loss=0.76177, avg_loss=0.77616]\n",
            "Step 24303   [1.653 sec/step, loss=0.77972, avg_loss=0.77642]\n",
            "Step 24304   [1.667 sec/step, loss=0.82167, avg_loss=0.77692]\n",
            "Step 24305   [1.663 sec/step, loss=0.79579, avg_loss=0.77683]\n",
            "Step 24306   [1.664 sec/step, loss=0.81020, avg_loss=0.77689]\n",
            "Step 24307   [1.665 sec/step, loss=0.77184, avg_loss=0.77697]\n",
            "Step 24308   [1.666 sec/step, loss=0.76160, avg_loss=0.77678]\n",
            "Step 24309   [1.664 sec/step, loss=0.74055, avg_loss=0.77627]\n",
            "Step 24310   [1.660 sec/step, loss=0.74595, avg_loss=0.77618]\n",
            "Step 24311   [1.648 sec/step, loss=0.75227, avg_loss=0.77568]\n",
            "Step 24312   [1.612 sec/step, loss=0.82073, avg_loss=0.77672]\n",
            "Step 24313   [1.609 sec/step, loss=0.77817, avg_loss=0.77681]\n",
            "Step 24314   [1.645 sec/step, loss=0.76903, avg_loss=0.77661]\n",
            "Step 24315   [1.643 sec/step, loss=0.73359, avg_loss=0.77613]\n",
            "Step 24316   [1.645 sec/step, loss=0.78481, avg_loss=0.77665]\n",
            "Step 24317   [1.657 sec/step, loss=0.79106, avg_loss=0.77650]\n",
            "Step 24318   [1.633 sec/step, loss=0.79483, avg_loss=0.77616]\n",
            "Step 24319   [1.634 sec/step, loss=0.77266, avg_loss=0.77641]\n",
            "Step 24320   [1.626 sec/step, loss=0.80441, avg_loss=0.77675]\n",
            "Generated 32 batches of size 8 in 8.321 sec\n",
            "Step 24321   [1.626 sec/step, loss=0.74320, avg_loss=0.77667]\n",
            "Step 24322   [1.624 sec/step, loss=0.74601, avg_loss=0.77635]\n",
            "Step 24323   [1.610 sec/step, loss=0.82857, avg_loss=0.77667]\n",
            "Step 24324   [1.609 sec/step, loss=0.74039, avg_loss=0.77598]\n",
            "Step 24325   [1.609 sec/step, loss=0.76887, avg_loss=0.77575]\n",
            "Step 24326   [1.609 sec/step, loss=0.79988, avg_loss=0.77584]\n",
            "Step 24327   [1.597 sec/step, loss=0.75693, avg_loss=0.77513]\n",
            "Step 24328   [1.591 sec/step, loss=0.80799, avg_loss=0.77587]\n",
            "Step 24329   [1.588 sec/step, loss=0.82980, avg_loss=0.77634]\n",
            "Step 24330   [1.589 sec/step, loss=0.77680, avg_loss=0.77634]\n",
            "Step 24331   [1.591 sec/step, loss=0.80261, avg_loss=0.77616]\n",
            "Step 24332   [1.587 sec/step, loss=0.79176, avg_loss=0.77590]\n",
            "Step 24333   [1.594 sec/step, loss=0.71907, avg_loss=0.77466]\n",
            "Step 24334   [1.598 sec/step, loss=0.74167, avg_loss=0.77463]\n",
            "Step 24335   [1.602 sec/step, loss=0.81275, avg_loss=0.77509]\n",
            "Step 24336   [1.601 sec/step, loss=0.80570, avg_loss=0.77554]\n",
            "Step 24337   [1.601 sec/step, loss=0.82100, avg_loss=0.77581]\n",
            "Step 24338   [1.627 sec/step, loss=0.78223, avg_loss=0.77590]\n",
            "Step 24339   [1.632 sec/step, loss=0.78592, avg_loss=0.77628]\n",
            "Step 24340   [1.648 sec/step, loss=0.83677, avg_loss=0.77682]\n",
            "Step 24341   [1.640 sec/step, loss=0.79944, avg_loss=0.77666]\n",
            "Step 24342   [1.606 sec/step, loss=0.76650, avg_loss=0.77806]\n",
            "Step 24343   [1.606 sec/step, loss=0.77265, avg_loss=0.77836]\n",
            "Step 24344   [1.605 sec/step, loss=0.80085, avg_loss=0.77864]\n",
            "Step 24345   [1.608 sec/step, loss=0.77392, avg_loss=0.77822]\n",
            "Step 24346   [1.613 sec/step, loss=0.81527, avg_loss=0.77845]\n",
            "Step 24347   [1.607 sec/step, loss=0.81274, avg_loss=0.77869]\n",
            "Step 24348   [1.613 sec/step, loss=0.79657, avg_loss=0.77882]\n",
            "Step 24349   [1.611 sec/step, loss=0.74322, avg_loss=0.77807]\n",
            "Step 24350   [1.601 sec/step, loss=0.82158, avg_loss=0.77897]\n",
            "Step 24351   [1.609 sec/step, loss=0.66985, avg_loss=0.77785]\n",
            "Step 24352   [1.601 sec/step, loss=0.73898, avg_loss=0.77765]\n",
            "Generated 32 batches of size 8 in 9.910 sec\n",
            "Step 24353   [1.593 sec/step, loss=0.79612, avg_loss=0.77757]\n",
            "Step 24354   [1.595 sec/step, loss=0.76036, avg_loss=0.77750]\n",
            "Step 24355   [1.583 sec/step, loss=0.77396, avg_loss=0.77751]\n",
            "Step 24356   [1.588 sec/step, loss=0.77021, avg_loss=0.77733]\n",
            "Step 24357   [1.592 sec/step, loss=0.77162, avg_loss=0.77723]\n",
            "Step 24358   [1.589 sec/step, loss=0.71015, avg_loss=0.77688]\n",
            "Step 24359   [1.590 sec/step, loss=0.73889, avg_loss=0.77668]\n",
            "Step 24360   [1.612 sec/step, loss=0.78590, avg_loss=0.77760]\n",
            "Step 24361   [1.619 sec/step, loss=0.75648, avg_loss=0.77750]\n",
            "Step 24362   [1.615 sec/step, loss=0.76877, avg_loss=0.77726]\n",
            "Step 24363   [1.614 sec/step, loss=0.75031, avg_loss=0.77667]\n",
            "Step 24364   [1.617 sec/step, loss=0.78208, avg_loss=0.77644]\n",
            "Step 24365   [1.622 sec/step, loss=0.79575, avg_loss=0.77662]\n",
            "Step 24366   [1.624 sec/step, loss=0.82712, avg_loss=0.77717]\n",
            "Step 24367   [1.620 sec/step, loss=0.81722, avg_loss=0.77750]\n",
            "Step 24368   [1.617 sec/step, loss=0.71127, avg_loss=0.77671]\n",
            "Step 24369   [1.628 sec/step, loss=0.79283, avg_loss=0.77677]\n",
            "Step 24370   [1.630 sec/step, loss=0.83711, avg_loss=0.77723]\n",
            "Step 24371   [1.614 sec/step, loss=0.74548, avg_loss=0.77704]\n",
            "Step 24372   [1.607 sec/step, loss=0.76915, avg_loss=0.77698]\n",
            "Step 24373   [1.609 sec/step, loss=0.80117, avg_loss=0.77744]\n",
            "Step 24374   [1.609 sec/step, loss=0.74665, avg_loss=0.77723]\n",
            "Step 24375   [1.606 sec/step, loss=0.78862, avg_loss=0.77706]\n",
            "Step 24376   [1.606 sec/step, loss=0.80800, avg_loss=0.77729]\n",
            "Step 24377   [1.604 sec/step, loss=0.75540, avg_loss=0.77667]\n",
            "Step 24378   [1.649 sec/step, loss=0.70849, avg_loss=0.77586]\n",
            "Step 24379   [1.652 sec/step, loss=0.73454, avg_loss=0.77541]\n",
            "Step 24380   [1.656 sec/step, loss=0.74116, avg_loss=0.77546]\n",
            "Step 24381   [1.663 sec/step, loss=0.80822, avg_loss=0.77595]\n",
            "Step 24382   [1.630 sec/step, loss=0.77924, avg_loss=0.77668]\n",
            "Step 24383   [1.643 sec/step, loss=0.79061, avg_loss=0.77691]\n",
            "Generated 32 batches of size 8 in 10.241 sec\n",
            "Step 24384   [1.648 sec/step, loss=0.77192, avg_loss=0.77662]\n",
            "Step 24385   [1.645 sec/step, loss=0.76968, avg_loss=0.77636]\n",
            "Step 24386   [1.633 sec/step, loss=0.80722, avg_loss=0.77642]\n",
            "Step 24387   [1.618 sec/step, loss=0.81069, avg_loss=0.77680]\n",
            "Step 24388   [1.600 sec/step, loss=0.78592, avg_loss=0.77751]\n",
            "Step 24389   [1.592 sec/step, loss=0.81430, avg_loss=0.77732]\n",
            "Step 24390   [1.600 sec/step, loss=0.80274, avg_loss=0.77734]\n",
            "Step 24391   [1.597 sec/step, loss=0.81647, avg_loss=0.77811]\n",
            "Step 24392   [1.598 sec/step, loss=0.81412, avg_loss=0.77888]\n",
            "Step 24393   [1.602 sec/step, loss=0.82057, avg_loss=0.77951]\n",
            "Step 24394   [1.603 sec/step, loss=0.82792, avg_loss=0.78034]\n",
            "Step 24395   [1.619 sec/step, loss=0.80709, avg_loss=0.78055]\n",
            "Step 24396   [1.617 sec/step, loss=0.80635, avg_loss=0.78047]\n",
            "Step 24397   [1.613 sec/step, loss=0.80856, avg_loss=0.78038]\n",
            "Step 24398   [1.613 sec/step, loss=0.74637, avg_loss=0.77997]\n",
            "Step 24399   [1.617 sec/step, loss=0.71581, avg_loss=0.77957]\n",
            "Step 24400   [1.619 sec/step, loss=0.73632, avg_loss=0.77922]\n",
            "Writing summary at step: 24400\n",
            "Step 24401   [1.623 sec/step, loss=0.79002, avg_loss=0.77975]\n",
            "Step 24402   [1.621 sec/step, loss=0.74872, avg_loss=0.77962]\n",
            "Step 24403   [1.613 sec/step, loss=0.78535, avg_loss=0.77968]\n",
            "Step 24404   [1.599 sec/step, loss=0.77845, avg_loss=0.77924]\n",
            "Step 24405   [1.597 sec/step, loss=0.74719, avg_loss=0.77876]\n",
            "Step 24406   [1.609 sec/step, loss=0.81170, avg_loss=0.77877]\n",
            "Step 24407   [1.615 sec/step, loss=0.70055, avg_loss=0.77806]\n",
            "Step 24408   [1.611 sec/step, loss=0.80056, avg_loss=0.77845]\n",
            "Step 24409   [1.612 sec/step, loss=0.79160, avg_loss=0.77896]\n",
            "Step 24410   [1.614 sec/step, loss=0.75587, avg_loss=0.77906]\n",
            "Step 24411   [1.610 sec/step, loss=0.79006, avg_loss=0.77944]\n",
            "Step 24412   [1.612 sec/step, loss=0.78423, avg_loss=0.77907]\n",
            "Step 24413   [1.660 sec/step, loss=0.73562, avg_loss=0.77865]\n",
            "Generated 32 batches of size 8 in 10.317 sec\n",
            "Step 24414   [1.630 sec/step, loss=0.71917, avg_loss=0.77815]\n",
            "Step 24415   [1.618 sec/step, loss=0.76368, avg_loss=0.77845]\n",
            "Step 24416   [1.620 sec/step, loss=0.76112, avg_loss=0.77821]\n",
            "Step 24417   [1.606 sec/step, loss=0.81768, avg_loss=0.77848]\n",
            "Step 24418   [1.606 sec/step, loss=0.82158, avg_loss=0.77875]\n",
            "Step 24419   [1.618 sec/step, loss=0.73205, avg_loss=0.77834]\n",
            "Step 24420   [1.619 sec/step, loss=0.80874, avg_loss=0.77838]\n",
            "Step 24421   [1.611 sec/step, loss=0.75353, avg_loss=0.77849]\n",
            "Step 24422   [1.619 sec/step, loss=0.79126, avg_loss=0.77894]\n",
            "Step 24423   [1.623 sec/step, loss=0.73849, avg_loss=0.77804]\n",
            "Step 24424   [1.629 sec/step, loss=0.74932, avg_loss=0.77813]\n",
            "Step 24425   [1.630 sec/step, loss=0.80605, avg_loss=0.77850]\n",
            "Step 24426   [1.631 sec/step, loss=0.83225, avg_loss=0.77882]\n",
            "Step 24427   [1.631 sec/step, loss=0.76936, avg_loss=0.77895]\n",
            "Step 24428   [1.637 sec/step, loss=0.75961, avg_loss=0.77846]\n",
            "Step 24429   [1.651 sec/step, loss=0.78712, avg_loss=0.77804]\n",
            "Step 24430   [1.667 sec/step, loss=0.77919, avg_loss=0.77806]\n",
            "Step 24431   [1.666 sec/step, loss=0.79647, avg_loss=0.77800]\n",
            "Step 24432   [1.677 sec/step, loss=0.81392, avg_loss=0.77822]\n",
            "Step 24433   [1.665 sec/step, loss=0.78434, avg_loss=0.77887]\n",
            "Step 24434   [1.701 sec/step, loss=0.77081, avg_loss=0.77917]\n",
            "Step 24435   [1.697 sec/step, loss=0.75000, avg_loss=0.77854]\n",
            "Step 24436   [1.703 sec/step, loss=0.74603, avg_loss=0.77794]\n",
            "Step 24437   [1.703 sec/step, loss=0.78633, avg_loss=0.77759]\n",
            "Step 24438   [1.678 sec/step, loss=0.81269, avg_loss=0.77790]\n",
            "Step 24439   [1.670 sec/step, loss=0.78529, avg_loss=0.77789]\n",
            "Step 24440   [1.653 sec/step, loss=0.75417, avg_loss=0.77707]\n",
            "Step 24441   [1.656 sec/step, loss=0.76855, avg_loss=0.77676]\n",
            "Step 24442   [1.651 sec/step, loss=0.78946, avg_loss=0.77699]\n",
            "Step 24443   [1.649 sec/step, loss=0.86590, avg_loss=0.77792]\n",
            "Step 24444   [1.655 sec/step, loss=0.76789, avg_loss=0.77759]\n",
            "Step 24445   [1.682 sec/step, loss=0.79734, avg_loss=0.77782]\n",
            "Step 24446   [1.678 sec/step, loss=0.83312, avg_loss=0.77800]\n",
            "Step 24447   [1.679 sec/step, loss=0.72931, avg_loss=0.77717]\n",
            "Generated 32 batches of size 8 in 9.928 sec\n",
            "Step 24448   [1.678 sec/step, loss=0.81829, avg_loss=0.77739]\n",
            "Step 24449   [1.677 sec/step, loss=0.77059, avg_loss=0.77766]\n",
            "Step 24450   [1.676 sec/step, loss=0.80018, avg_loss=0.77745]\n",
            "Step 24451   [1.676 sec/step, loss=0.78580, avg_loss=0.77861]\n",
            "Step 24452   [1.666 sec/step, loss=0.72506, avg_loss=0.77847]\n",
            "Step 24453   [1.671 sec/step, loss=0.77911, avg_loss=0.77830]\n",
            "Step 24454   [1.668 sec/step, loss=0.78430, avg_loss=0.77854]\n",
            "Step 24455   [1.661 sec/step, loss=0.75424, avg_loss=0.77834]\n",
            "Step 24456   [1.657 sec/step, loss=0.81402, avg_loss=0.77878]\n",
            "Step 24457   [1.654 sec/step, loss=0.78149, avg_loss=0.77887]\n",
            "Step 24458   [1.649 sec/step, loss=0.78174, avg_loss=0.77959]\n",
            "Step 24459   [1.646 sec/step, loss=0.79147, avg_loss=0.78012]\n",
            "Step 24460   [1.627 sec/step, loss=0.79253, avg_loss=0.78018]\n",
            "Step 24461   [1.616 sec/step, loss=0.82222, avg_loss=0.78084]\n",
            "Step 24462   [1.618 sec/step, loss=0.75180, avg_loss=0.78067]\n",
            "Step 24463   [1.612 sec/step, loss=0.79659, avg_loss=0.78113]\n",
            "Step 24464   [1.639 sec/step, loss=0.73036, avg_loss=0.78062]\n",
            "Step 24465   [1.633 sec/step, loss=0.73900, avg_loss=0.78005]\n",
            "Step 24466   [1.630 sec/step, loss=0.76797, avg_loss=0.77946]\n",
            "Step 24467   [1.630 sec/step, loss=0.82285, avg_loss=0.77951]\n",
            "Step 24468   [1.635 sec/step, loss=0.73853, avg_loss=0.77979]\n",
            "Step 24469   [1.630 sec/step, loss=0.77554, avg_loss=0.77961]\n",
            "Step 24470   [1.628 sec/step, loss=0.79155, avg_loss=0.77916]\n",
            "Step 24471   [1.623 sec/step, loss=0.77421, avg_loss=0.77944]\n",
            "Step 24472   [1.632 sec/step, loss=0.80746, avg_loss=0.77983]\n",
            "Step 24473   [1.633 sec/step, loss=0.81266, avg_loss=0.77994]\n",
            "Step 24474   [1.627 sec/step, loss=0.81339, avg_loss=0.78061]\n",
            "Step 24475   [1.634 sec/step, loss=0.73532, avg_loss=0.78008]\n",
            "Step 24476   [1.651 sec/step, loss=0.83234, avg_loss=0.78032]\n",
            "Step 24477   [1.648 sec/step, loss=0.82441, avg_loss=0.78101]\n",
            "Step 24478   [1.622 sec/step, loss=0.76293, avg_loss=0.78155]\n",
            "Generated 32 batches of size 8 in 10.525 sec\n",
            "Step 24479   [1.626 sec/step, loss=0.75363, avg_loss=0.78175]\n",
            "Step 24480   [1.642 sec/step, loss=0.77944, avg_loss=0.78213]\n",
            "Step 24481   [1.635 sec/step, loss=0.81366, avg_loss=0.78218]\n",
            "Step 24482   [1.628 sec/step, loss=0.76263, avg_loss=0.78202]\n",
            "Step 24483   [1.617 sec/step, loss=0.75589, avg_loss=0.78167]\n",
            "Step 24484   [1.618 sec/step, loss=0.78484, avg_loss=0.78180]\n",
            "Step 24485   [1.622 sec/step, loss=0.75925, avg_loss=0.78169]\n",
            "Step 24486   [1.634 sec/step, loss=0.75201, avg_loss=0.78114]\n",
            "Step 24487   [1.641 sec/step, loss=0.78093, avg_loss=0.78084]\n",
            "Step 24488   [1.646 sec/step, loss=0.73246, avg_loss=0.78031]\n",
            "Step 24489   [1.648 sec/step, loss=0.73485, avg_loss=0.77952]\n",
            "Step 24490   [1.654 sec/step, loss=0.71585, avg_loss=0.77865]\n",
            "Step 24491   [1.655 sec/step, loss=0.75127, avg_loss=0.77799]\n",
            "Step 24492   [1.651 sec/step, loss=0.75771, avg_loss=0.77743]\n",
            "Step 24493   [1.650 sec/step, loss=0.73008, avg_loss=0.77653]\n",
            "Step 24494   [1.657 sec/step, loss=0.79062, avg_loss=0.77615]\n",
            "Step 24495   [1.642 sec/step, loss=0.78036, avg_loss=0.77589]\n",
            "Step 24496   [1.643 sec/step, loss=0.78092, avg_loss=0.77563]\n",
            "Step 24497   [1.637 sec/step, loss=0.79432, avg_loss=0.77549]\n",
            "Step 24498   [1.639 sec/step, loss=0.78153, avg_loss=0.77584]\n",
            "Step 24499   [1.639 sec/step, loss=0.77941, avg_loss=0.77648]\n",
            "Step 24500   [1.634 sec/step, loss=0.74731, avg_loss=0.77659]\n",
            "Writing summary at step: 24500\n",
            "Step 24501   [1.668 sec/step, loss=0.70928, avg_loss=0.77578]\n",
            "Step 24502   [1.666 sec/step, loss=0.80380, avg_loss=0.77633]\n",
            "Step 24503   [1.674 sec/step, loss=0.74058, avg_loss=0.77588]\n",
            "Step 24504   [1.671 sec/step, loss=0.79416, avg_loss=0.77604]\n",
            "Step 24505   [1.675 sec/step, loss=0.78584, avg_loss=0.77643]\n",
            "Step 24506   [1.671 sec/step, loss=0.74362, avg_loss=0.77574]\n",
            "Step 24507   [1.688 sec/step, loss=0.74975, avg_loss=0.77624]\n",
            "Step 24508   [1.697 sec/step, loss=0.73594, avg_loss=0.77559]\n",
            "Step 24509   [1.692 sec/step, loss=0.77510, avg_loss=0.77543]\n",
            "Step 24510   [1.691 sec/step, loss=0.78164, avg_loss=0.77568]\n",
            "Generated 32 batches of size 8 in 10.416 sec\n",
            "Step 24511   [1.689 sec/step, loss=0.76433, avg_loss=0.77543]\n",
            "Step 24512   [1.685 sec/step, loss=0.76731, avg_loss=0.77526]\n",
            "Step 24513   [1.639 sec/step, loss=0.75560, avg_loss=0.77546]\n",
            "Step 24514   [1.659 sec/step, loss=0.69186, avg_loss=0.77518]\n",
            "Step 24515   [1.662 sec/step, loss=0.82448, avg_loss=0.77579]\n",
            "Step 24516   [1.656 sec/step, loss=0.77005, avg_loss=0.77588]\n",
            "Step 24517   [1.658 sec/step, loss=0.75107, avg_loss=0.77522]\n",
            "Step 24518   [1.656 sec/step, loss=0.76580, avg_loss=0.77466]\n",
            "Step 24519   [1.639 sec/step, loss=0.72881, avg_loss=0.77462]\n",
            "Step 24520   [1.642 sec/step, loss=0.74334, avg_loss=0.77397]\n",
            "Step 24521   [1.651 sec/step, loss=0.80295, avg_loss=0.77446]\n",
            "Step 24522   [1.646 sec/step, loss=0.80798, avg_loss=0.77463]\n",
            "Step 24523   [1.645 sec/step, loss=0.77075, avg_loss=0.77495]\n",
            "Step 24524   [1.645 sec/step, loss=0.75585, avg_loss=0.77502]\n",
            "Step 24525   [1.649 sec/step, loss=0.78093, avg_loss=0.77477]\n",
            "Step 24526   [1.651 sec/step, loss=0.84584, avg_loss=0.77490]\n",
            "Step 24527   [1.693 sec/step, loss=0.71687, avg_loss=0.77438]\n",
            "Step 24528   [1.686 sec/step, loss=0.77946, avg_loss=0.77458]\n",
            "Step 24529   [1.688 sec/step, loss=0.75711, avg_loss=0.77428]\n",
            "Step 24530   [1.670 sec/step, loss=0.76410, avg_loss=0.77413]\n",
            "Step 24531   [1.670 sec/step, loss=0.81195, avg_loss=0.77428]\n",
            "Step 24532   [1.668 sec/step, loss=0.79983, avg_loss=0.77414]\n",
            "Step 24533   [1.668 sec/step, loss=0.79934, avg_loss=0.77429]\n",
            "Step 24534   [1.628 sec/step, loss=0.78655, avg_loss=0.77445]\n",
            "Step 24535   [1.630 sec/step, loss=0.75387, avg_loss=0.77449]\n",
            "Step 24536   [1.631 sec/step, loss=0.76562, avg_loss=0.77468]\n",
            "Step 24537   [1.632 sec/step, loss=0.80439, avg_loss=0.77486]\n",
            "Step 24538   [1.633 sec/step, loss=0.75414, avg_loss=0.77428]\n",
            "Step 24539   [1.639 sec/step, loss=0.75815, avg_loss=0.77401]\n",
            "Step 24540   [1.649 sec/step, loss=0.79354, avg_loss=0.77440]\n",
            "Step 24541   [1.647 sec/step, loss=0.82084, avg_loss=0.77492]\n",
            "Step 24542   [1.665 sec/step, loss=0.72455, avg_loss=0.77427]\n",
            "Generated 32 batches of size 8 in 9.825 sec\n",
            "Step 24543   [1.684 sec/step, loss=0.75660, avg_loss=0.77318]\n",
            "Step 24544   [1.678 sec/step, loss=0.78233, avg_loss=0.77333]\n",
            "Step 24545   [1.657 sec/step, loss=0.79527, avg_loss=0.77331]\n",
            "Step 24546   [1.652 sec/step, loss=0.76801, avg_loss=0.77265]\n",
            "Step 24547   [1.654 sec/step, loss=0.82935, avg_loss=0.77365]\n",
            "Step 24548   [1.668 sec/step, loss=0.74666, avg_loss=0.77294]\n",
            "Step 24549   [1.669 sec/step, loss=0.78450, avg_loss=0.77308]\n",
            "Step 24550   [1.671 sec/step, loss=0.77035, avg_loss=0.77278]\n",
            "Step 24551   [1.671 sec/step, loss=0.74198, avg_loss=0.77234]\n",
            "Step 24552   [1.666 sec/step, loss=0.75863, avg_loss=0.77268]\n",
            "Step 24553   [1.670 sec/step, loss=0.76542, avg_loss=0.77254]\n",
            "Step 24554   [1.672 sec/step, loss=0.78753, avg_loss=0.77257]\n",
            "Step 24555   [1.677 sec/step, loss=0.82112, avg_loss=0.77324]\n",
            "Step 24556   [1.676 sec/step, loss=0.78694, avg_loss=0.77297]\n",
            "Step 24557   [1.684 sec/step, loss=0.76975, avg_loss=0.77285]\n",
            "Step 24558   [1.690 sec/step, loss=0.78039, avg_loss=0.77284]\n",
            "Step 24559   [1.688 sec/step, loss=0.78450, avg_loss=0.77277]\n",
            "Step 24560   [1.729 sec/step, loss=0.73179, avg_loss=0.77216]\n",
            "Step 24561   [1.726 sec/step, loss=0.77123, avg_loss=0.77165]\n",
            "Step 24562   [1.730 sec/step, loss=0.77714, avg_loss=0.77191]\n",
            "Step 24563   [1.732 sec/step, loss=0.70106, avg_loss=0.77095]\n",
            "Step 24564   [1.702 sec/step, loss=0.80539, avg_loss=0.77170]\n",
            "Step 24565   [1.705 sec/step, loss=0.81064, avg_loss=0.77242]\n",
            "Step 24566   [1.707 sec/step, loss=0.80480, avg_loss=0.77279]\n",
            "Step 24567   [1.706 sec/step, loss=0.78356, avg_loss=0.77239]\n",
            "Step 24568   [1.704 sec/step, loss=0.78589, avg_loss=0.77287]\n",
            "Step 24569   [1.703 sec/step, loss=0.74861, avg_loss=0.77260]\n",
            "Step 24570   [1.704 sec/step, loss=0.75668, avg_loss=0.77225]\n",
            "Step 24571   [1.708 sec/step, loss=0.76042, avg_loss=0.77211]\n",
            "Step 24572   [1.723 sec/step, loss=0.73090, avg_loss=0.77134]\n",
            "Generated 32 batches of size 8 in 7.129 sec\n",
            "Step 24573   [1.727 sec/step, loss=0.72367, avg_loss=0.77045]\n",
            "Step 24574   [1.735 sec/step, loss=0.76544, avg_loss=0.76998]\n",
            "Step 24575   [1.733 sec/step, loss=0.77727, avg_loss=0.77040]\n",
            "Step 24576   [1.710 sec/step, loss=0.74990, avg_loss=0.76957]\n",
            "Step 24577   [1.719 sec/step, loss=0.77912, avg_loss=0.76912]\n",
            "Step 24578   [1.711 sec/step, loss=0.80104, avg_loss=0.76950]\n",
            "Step 24579   [1.723 sec/step, loss=0.74740, avg_loss=0.76944]\n",
            "Step 24580   [1.701 sec/step, loss=0.75590, avg_loss=0.76920]\n",
            "Step 24581   [1.698 sec/step, loss=0.75888, avg_loss=0.76865]\n",
            "Step 24582   [1.692 sec/step, loss=0.83250, avg_loss=0.76935]\n",
            "Step 24583   [1.688 sec/step, loss=0.77905, avg_loss=0.76958]\n",
            "Step 24584   [1.681 sec/step, loss=0.76154, avg_loss=0.76935]\n",
            "Step 24585   [1.679 sec/step, loss=0.77916, avg_loss=0.76955]\n",
            "Step 24586   [1.680 sec/step, loss=0.80234, avg_loss=0.77005]\n",
            "Step 24587   [1.676 sec/step, loss=0.73824, avg_loss=0.76963]\n",
            "Step 24588   [1.675 sec/step, loss=0.75869, avg_loss=0.76989]\n",
            "Step 24589   [1.672 sec/step, loss=0.77692, avg_loss=0.77031]\n",
            "Step 24590   [1.660 sec/step, loss=0.78826, avg_loss=0.77103]\n",
            "Step 24591   [1.663 sec/step, loss=0.75996, avg_loss=0.77112]\n",
            "Step 24592   [1.664 sec/step, loss=0.77100, avg_loss=0.77125]\n",
            "Step 24593   [1.667 sec/step, loss=0.78566, avg_loss=0.77181]\n",
            "Step 24594   [1.658 sec/step, loss=0.81011, avg_loss=0.77200]\n",
            "Step 24595   [1.663 sec/step, loss=0.80619, avg_loss=0.77226]\n",
            "Step 24596   [1.667 sec/step, loss=0.79249, avg_loss=0.77238]\n",
            "Step 24597   [1.662 sec/step, loss=0.80315, avg_loss=0.77247]\n",
            "Step 24598   [1.656 sec/step, loss=0.81755, avg_loss=0.77283]\n",
            "Step 24599   [1.651 sec/step, loss=0.79123, avg_loss=0.77294]\n",
            "Step 24600   [1.656 sec/step, loss=0.79296, avg_loss=0.77340]\n",
            "Writing summary at step: 24600\n",
            "Step 24601   [1.622 sec/step, loss=0.81981, avg_loss=0.77451]\n",
            "Generated 32 batches of size 8 in 6.887 sec\n",
            "Step 24602   [1.674 sec/step, loss=0.73530, avg_loss=0.77382]\n",
            "Step 24603   [1.666 sec/step, loss=0.77516, avg_loss=0.77417]\n",
            "Step 24604   [1.672 sec/step, loss=0.79108, avg_loss=0.77414]\n",
            "Step 24605   [1.673 sec/step, loss=0.76777, avg_loss=0.77396]\n",
            "Step 24606   [1.682 sec/step, loss=0.79524, avg_loss=0.77447]\n",
            "Step 24607   [1.668 sec/step, loss=0.76447, avg_loss=0.77462]\n",
            "Step 24608   [1.662 sec/step, loss=0.72663, avg_loss=0.77453]\n",
            "Step 24609   [1.669 sec/step, loss=0.76637, avg_loss=0.77444]\n",
            "Step 24610   [1.668 sec/step, loss=0.83500, avg_loss=0.77497]\n",
            "Step 24611   [1.673 sec/step, loss=0.78842, avg_loss=0.77521]\n",
            "Step 24612   [1.671 sec/step, loss=0.73947, avg_loss=0.77493]\n",
            "Step 24613   [1.677 sec/step, loss=0.79049, avg_loss=0.77528]\n",
            "Step 24614   [1.663 sec/step, loss=0.79341, avg_loss=0.77630]\n",
            "Step 24615   [1.679 sec/step, loss=0.76796, avg_loss=0.77573]\n",
            "Step 24616   [1.679 sec/step, loss=0.80482, avg_loss=0.77608]\n",
            "Step 24617   [1.676 sec/step, loss=0.75570, avg_loss=0.77613]\n",
            "Step 24618   [1.678 sec/step, loss=0.81054, avg_loss=0.77657]\n",
            "Step 24619   [1.679 sec/step, loss=0.77293, avg_loss=0.77702]\n",
            "Step 24620   [1.678 sec/step, loss=0.73729, avg_loss=0.77696]\n",
            "Step 24621   [1.678 sec/step, loss=0.75712, avg_loss=0.77650]\n",
            "Step 24622   [1.678 sec/step, loss=0.76954, avg_loss=0.77611]\n",
            "Step 24623   [1.677 sec/step, loss=0.71466, avg_loss=0.77555]\n",
            "Step 24624   [1.669 sec/step, loss=0.77585, avg_loss=0.77575]\n",
            "Step 24625   [1.662 sec/step, loss=0.81191, avg_loss=0.77606]\n",
            "Step 24626   [1.659 sec/step, loss=0.78705, avg_loss=0.77547]\n",
            "Step 24627   [1.619 sec/step, loss=0.80048, avg_loss=0.77631]\n",
            "Step 24628   [1.661 sec/step, loss=0.73379, avg_loss=0.77585]\n",
            "Step 24629   [1.647 sec/step, loss=0.80521, avg_loss=0.77633]\n",
            "Step 24630   [1.670 sec/step, loss=0.79950, avg_loss=0.77669]\n",
            "Step 24631   [1.677 sec/step, loss=0.80957, avg_loss=0.77666]\n",
            "Step 24632   [1.674 sec/step, loss=0.82105, avg_loss=0.77688]\n",
            "Step 24633   [1.675 sec/step, loss=0.80081, avg_loss=0.77689]\n",
            "Step 24634   [1.682 sec/step, loss=0.75663, avg_loss=0.77659]\n",
            "Step 24635   [1.684 sec/step, loss=0.83450, avg_loss=0.77740]\n",
            "Generated 32 batches of size 8 in 7.127 sec\n",
            "Step 24636   [1.694 sec/step, loss=0.77423, avg_loss=0.77748]\n",
            "Step 24637   [1.691 sec/step, loss=0.76995, avg_loss=0.77714]\n",
            "Step 24638   [1.690 sec/step, loss=0.77111, avg_loss=0.77731]\n",
            "Step 24639   [1.688 sec/step, loss=0.80927, avg_loss=0.77782]\n",
            "Step 24640   [1.678 sec/step, loss=0.80641, avg_loss=0.77795]\n",
            "Step 24641   [1.675 sec/step, loss=0.77487, avg_loss=0.77749]\n",
            "Step 24642   [1.655 sec/step, loss=0.77126, avg_loss=0.77796]\n",
            "Step 24643   [1.635 sec/step, loss=0.71279, avg_loss=0.77752]\n",
            "Step 24644   [1.637 sec/step, loss=0.70170, avg_loss=0.77671]\n",
            "Step 24645   [1.629 sec/step, loss=0.75575, avg_loss=0.77632]\n",
            "Step 24646   [1.631 sec/step, loss=0.81999, avg_loss=0.77684]\n",
            "Step 24647   [1.627 sec/step, loss=0.80881, avg_loss=0.77663]\n",
            "Step 24648   [1.614 sec/step, loss=0.76693, avg_loss=0.77683]\n",
            "Step 24649   [1.613 sec/step, loss=0.76538, avg_loss=0.77664]\n",
            "Step 24650   [1.613 sec/step, loss=0.71328, avg_loss=0.77607]\n",
            "Step 24651   [1.612 sec/step, loss=0.68373, avg_loss=0.77549]\n",
            "Step 24652   [1.620 sec/step, loss=0.75795, avg_loss=0.77548]\n",
            "Step 24653   [1.615 sec/step, loss=0.77334, avg_loss=0.77556]\n",
            "Step 24654   [1.624 sec/step, loss=0.77709, avg_loss=0.77546]\n",
            "Step 24655   [1.624 sec/step, loss=0.77769, avg_loss=0.77502]\n",
            "Step 24656   [1.629 sec/step, loss=0.80557, avg_loss=0.77521]\n",
            "Step 24657   [1.618 sec/step, loss=0.78411, avg_loss=0.77535]\n",
            "Step 24658   [1.612 sec/step, loss=0.77206, avg_loss=0.77527]\n",
            "Step 24659   [1.618 sec/step, loss=0.81529, avg_loss=0.77558]\n",
            "Step 24660   [1.613 sec/step, loss=0.79397, avg_loss=0.77620]\n",
            "Step 24661   [1.616 sec/step, loss=0.81912, avg_loss=0.77668]\n",
            "Step 24662   [1.630 sec/step, loss=0.71710, avg_loss=0.77608]\n",
            "Step 24663   [1.626 sec/step, loss=0.78936, avg_loss=0.77696]\n",
            "Step 24664   [1.637 sec/step, loss=0.79160, avg_loss=0.77682]\n",
            "Step 24665   [1.643 sec/step, loss=0.75471, avg_loss=0.77626]\n",
            "Step 24666   [1.643 sec/step, loss=0.71756, avg_loss=0.77539]\n",
            "Step 24667   [1.649 sec/step, loss=0.76654, avg_loss=0.77522]\n",
            "Step 24668   [1.650 sec/step, loss=0.75595, avg_loss=0.77492]\n",
            "Step 24669   [1.648 sec/step, loss=0.79508, avg_loss=0.77539]\n",
            "Generated 32 batches of size 8 in 8.228 sec\n",
            "Step 24670   [1.662 sec/step, loss=0.80213, avg_loss=0.77584]\n",
            "Step 24671   [1.667 sec/step, loss=0.77504, avg_loss=0.77599]\n",
            "Step 24672   [1.642 sec/step, loss=0.82242, avg_loss=0.77690]\n",
            "Step 24673   [1.638 sec/step, loss=0.79139, avg_loss=0.77758]\n",
            "Step 24674   [1.631 sec/step, loss=0.79640, avg_loss=0.77789]\n",
            "Step 24675   [1.641 sec/step, loss=0.80491, avg_loss=0.77817]\n",
            "Step 24676   [1.645 sec/step, loss=0.78655, avg_loss=0.77853]\n",
            "Step 24677   [1.646 sec/step, loss=0.80456, avg_loss=0.77879]\n",
            "Step 24678   [1.645 sec/step, loss=0.76748, avg_loss=0.77845]\n",
            "Step 24679   [1.628 sec/step, loss=0.79253, avg_loss=0.77890]\n",
            "Step 24680   [1.631 sec/step, loss=0.79737, avg_loss=0.77932]\n",
            "Step 24681   [1.666 sec/step, loss=0.71767, avg_loss=0.77891]\n",
            "Step 24682   [1.666 sec/step, loss=0.77524, avg_loss=0.77833]\n",
            "Step 24683   [1.671 sec/step, loss=0.80943, avg_loss=0.77864]\n",
            "Step 24684   [1.681 sec/step, loss=0.81945, avg_loss=0.77922]\n",
            "Step 24685   [1.703 sec/step, loss=0.71736, avg_loss=0.77860]\n",
            "Step 24686   [1.698 sec/step, loss=0.73125, avg_loss=0.77789]\n",
            "Step 24687   [1.702 sec/step, loss=0.74194, avg_loss=0.77792]\n",
            "Step 24688   [1.697 sec/step, loss=0.78586, avg_loss=0.77820]\n",
            "Step 24689   [1.698 sec/step, loss=0.74057, avg_loss=0.77783]\n",
            "Step 24690   [1.696 sec/step, loss=0.77217, avg_loss=0.77767]\n",
            "Step 24691   [1.698 sec/step, loss=0.81368, avg_loss=0.77821]\n",
            "Step 24692   [1.697 sec/step, loss=0.75860, avg_loss=0.77808]\n",
            "Step 24693   [1.707 sec/step, loss=0.75208, avg_loss=0.77775]\n",
            "Step 24694   [1.706 sec/step, loss=0.79827, avg_loss=0.77763]\n",
            "Step 24695   [1.701 sec/step, loss=0.73815, avg_loss=0.77695]\n",
            "Step 24696   [1.700 sec/step, loss=0.76598, avg_loss=0.77668]\n",
            "Step 24697   [1.708 sec/step, loss=0.74729, avg_loss=0.77613]\n",
            "Step 24698   [1.714 sec/step, loss=0.77310, avg_loss=0.77568]\n",
            "Step 24699   [1.730 sec/step, loss=0.73488, avg_loss=0.77512]\n",
            "Step 24700   [1.732 sec/step, loss=0.83086, avg_loss=0.77550]\n",
            "Writing summary at step: 24700\n",
            "Generated 32 batches of size 8 in 8.420 sec\n",
            "Step 24701   [1.726 sec/step, loss=0.76237, avg_loss=0.77492]\n",
            "Step 24702   [1.684 sec/step, loss=0.77261, avg_loss=0.77530]\n",
            "Step 24703   [1.689 sec/step, loss=0.76894, avg_loss=0.77523]\n",
            "Step 24704   [1.685 sec/step, loss=0.79032, avg_loss=0.77523]\n",
            "Step 24705   [1.697 sec/step, loss=0.78535, avg_loss=0.77540]\n",
            "Step 24706   [1.725 sec/step, loss=0.67331, avg_loss=0.77418]\n",
            "Step 24707   [1.716 sec/step, loss=0.75757, avg_loss=0.77411]\n",
            "Step 24708   [1.719 sec/step, loss=0.80766, avg_loss=0.77492]\n",
            "Step 24709   [1.708 sec/step, loss=0.78308, avg_loss=0.77509]\n",
            "Step 24710   [1.716 sec/step, loss=0.74797, avg_loss=0.77422]\n",
            "Step 24711   [1.715 sec/step, loss=0.74315, avg_loss=0.77377]\n",
            "Step 24712   [1.714 sec/step, loss=0.80992, avg_loss=0.77447]\n",
            "Step 24713   [1.702 sec/step, loss=0.80267, avg_loss=0.77459]\n",
            "Step 24714   [1.691 sec/step, loss=0.79597, avg_loss=0.77462]\n",
            "Step 24715   [1.681 sec/step, loss=0.85135, avg_loss=0.77545]\n",
            "Step 24716   [1.683 sec/step, loss=0.82797, avg_loss=0.77568]\n",
            "Step 24717   [1.687 sec/step, loss=0.83877, avg_loss=0.77652]\n",
            "Step 24718   [1.690 sec/step, loss=0.74996, avg_loss=0.77591]\n",
            "Step 24719   [1.692 sec/step, loss=0.76457, avg_loss=0.77583]\n",
            "Step 24720   [1.687 sec/step, loss=0.79753, avg_loss=0.77643]\n",
            "Step 24721   [1.685 sec/step, loss=0.75215, avg_loss=0.77638]\n",
            "Step 24722   [1.682 sec/step, loss=0.72459, avg_loss=0.77593]\n",
            "Step 24723   [1.676 sec/step, loss=0.78496, avg_loss=0.77663]\n",
            "Step 24724   [1.684 sec/step, loss=0.80890, avg_loss=0.77696]\n",
            "Step 24725   [1.688 sec/step, loss=0.82600, avg_loss=0.77710]\n",
            "Step 24726   [1.693 sec/step, loss=0.73582, avg_loss=0.77659]\n",
            "Step 24727   [1.702 sec/step, loss=0.75535, avg_loss=0.77614]\n",
            "Step 24728   [1.670 sec/step, loss=0.73400, avg_loss=0.77614]\n",
            "Step 24729   [1.669 sec/step, loss=0.78825, avg_loss=0.77597]\n",
            "Step 24730   [1.650 sec/step, loss=0.77059, avg_loss=0.77568]\n",
            "Step 24731   [1.650 sec/step, loss=0.81462, avg_loss=0.77573]\n",
            "Step 24732   [1.646 sec/step, loss=0.75203, avg_loss=0.77504]\n",
            "Step 24733   [1.647 sec/step, loss=0.79031, avg_loss=0.77494]\n",
            "Generated 32 batches of size 8 in 9.609 sec\n",
            "Step 24734   [1.639 sec/step, loss=0.77715, avg_loss=0.77514]\n",
            "Step 24735   [1.653 sec/step, loss=0.78119, avg_loss=0.77461]\n",
            "Step 24736   [1.637 sec/step, loss=0.77062, avg_loss=0.77457]\n",
            "Step 24737   [1.644 sec/step, loss=0.80127, avg_loss=0.77489]\n",
            "Step 24738   [1.640 sec/step, loss=0.79665, avg_loss=0.77514]\n",
            "Step 24739   [1.638 sec/step, loss=0.79703, avg_loss=0.77502]\n",
            "Step 24740   [1.642 sec/step, loss=0.74731, avg_loss=0.77443]\n",
            "Step 24741   [1.644 sec/step, loss=0.75773, avg_loss=0.77426]\n",
            "Step 24742   [1.648 sec/step, loss=0.75898, avg_loss=0.77414]\n",
            "Step 24743   [1.651 sec/step, loss=0.74331, avg_loss=0.77444]\n",
            "Step 24744   [1.648 sec/step, loss=0.76156, avg_loss=0.77504]\n",
            "Step 24745   [1.663 sec/step, loss=0.74539, avg_loss=0.77494]\n",
            "Step 24746   [1.677 sec/step, loss=0.78384, avg_loss=0.77457]\n",
            "Step 24747   [1.684 sec/step, loss=0.81288, avg_loss=0.77461]\n",
            "Step 24748   [1.684 sec/step, loss=0.75738, avg_loss=0.77452]\n",
            "Step 24749   [1.682 sec/step, loss=0.80617, avg_loss=0.77493]\n",
            "Step 24750   [1.685 sec/step, loss=0.70656, avg_loss=0.77486]\n",
            "Step 24751   [1.680 sec/step, loss=0.81208, avg_loss=0.77614]\n",
            "Step 24752   [1.679 sec/step, loss=0.79816, avg_loss=0.77655]\n",
            "Step 24753   [1.672 sec/step, loss=0.81771, avg_loss=0.77699]\n",
            "Step 24754   [1.663 sec/step, loss=0.80223, avg_loss=0.77724]\n",
            "Step 24755   [1.657 sec/step, loss=0.82849, avg_loss=0.77775]\n",
            "Step 24756   [1.660 sec/step, loss=0.80203, avg_loss=0.77771]\n",
            "Step 24757   [1.659 sec/step, loss=0.78860, avg_loss=0.77776]\n",
            "Step 24758   [1.676 sec/step, loss=0.80048, avg_loss=0.77804]\n",
            "Step 24759   [1.710 sec/step, loss=0.70103, avg_loss=0.77690]\n",
            "Step 24760   [1.679 sec/step, loss=0.77993, avg_loss=0.77676]\n",
            "Step 24761   [1.684 sec/step, loss=0.79586, avg_loss=0.77653]\n",
            "Step 24762   [1.668 sec/step, loss=0.77793, avg_loss=0.77714]\n",
            "Step 24763   [1.678 sec/step, loss=0.81745, avg_loss=0.77742]\n",
            "Step 24764   [1.674 sec/step, loss=0.77377, avg_loss=0.77724]\n",
            "Generated 32 batches of size 8 in 10.027 sec\n",
            "Step 24765   [1.668 sec/step, loss=0.76869, avg_loss=0.77738]\n",
            "Step 24766   [1.689 sec/step, loss=0.79591, avg_loss=0.77816]\n",
            "Step 24767   [1.684 sec/step, loss=0.80389, avg_loss=0.77853]\n",
            "Step 24768   [1.680 sec/step, loss=0.80883, avg_loss=0.77906]\n",
            "Step 24769   [1.679 sec/step, loss=0.77937, avg_loss=0.77891]\n",
            "Step 24770   [1.684 sec/step, loss=0.78026, avg_loss=0.77869]\n",
            "Step 24771   [1.682 sec/step, loss=0.74368, avg_loss=0.77837]\n",
            "Step 24772   [1.681 sec/step, loss=0.73848, avg_loss=0.77753]\n",
            "Step 24773   [1.680 sec/step, loss=0.79017, avg_loss=0.77752]\n",
            "Step 24774   [1.686 sec/step, loss=0.75700, avg_loss=0.77713]\n",
            "Step 24775   [1.672 sec/step, loss=0.72594, avg_loss=0.77634]\n",
            "Step 24776   [1.672 sec/step, loss=0.81816, avg_loss=0.77666]\n",
            "Step 24777   [1.671 sec/step, loss=0.76502, avg_loss=0.77626]\n",
            "Step 24778   [1.677 sec/step, loss=0.75542, avg_loss=0.77614]\n",
            "Step 24779   [1.678 sec/step, loss=0.79245, avg_loss=0.77614]\n",
            "Step 24780   [1.678 sec/step, loss=0.76317, avg_loss=0.77580]\n",
            "Step 24781   [1.685 sec/step, loss=0.70770, avg_loss=0.77570]\n",
            "Step 24782   [1.690 sec/step, loss=0.77985, avg_loss=0.77574]\n",
            "Step 24783   [1.683 sec/step, loss=0.82034, avg_loss=0.77585]\n",
            "Step 24784   [1.670 sec/step, loss=0.80435, avg_loss=0.77570]\n",
            "Step 24785   [1.650 sec/step, loss=0.77514, avg_loss=0.77628]\n",
            "Step 24786   [1.652 sec/step, loss=0.73083, avg_loss=0.77627]\n",
            "Step 24787   [1.653 sec/step, loss=0.74772, avg_loss=0.77633]\n",
            "Step 24788   [1.653 sec/step, loss=0.80464, avg_loss=0.77652]\n",
            "Step 24789   [1.657 sec/step, loss=0.77952, avg_loss=0.77691]\n",
            "Step 24790   [1.659 sec/step, loss=0.80466, avg_loss=0.77723]\n",
            "Step 24791   [1.656 sec/step, loss=0.77026, avg_loss=0.77680]\n",
            "Step 24792   [1.668 sec/step, loss=0.77269, avg_loss=0.77694]\n",
            "Step 24793   [1.665 sec/step, loss=0.75098, avg_loss=0.77693]\n",
            "Step 24794   [1.672 sec/step, loss=0.80805, avg_loss=0.77703]\n",
            "Step 24795   [1.689 sec/step, loss=0.81530, avg_loss=0.77780]\n",
            "Generated 32 batches of size 8 in 9.907 sec\n",
            "Step 24796   [1.690 sec/step, loss=0.75391, avg_loss=0.77768]\n",
            "Step 24797   [1.687 sec/step, loss=0.74429, avg_loss=0.77765]\n",
            "Step 24798   [1.686 sec/step, loss=0.83873, avg_loss=0.77831]\n",
            "Step 24799   [1.672 sec/step, loss=0.80528, avg_loss=0.77901]\n",
            "Step 24800   [1.666 sec/step, loss=0.78307, avg_loss=0.77853]\n",
            "Writing summary at step: 24800\n",
            "Step 24801   [1.681 sec/step, loss=0.82323, avg_loss=0.77914]\n",
            "Step 24802   [1.675 sec/step, loss=0.80294, avg_loss=0.77944]\n",
            "Step 24803   [1.690 sec/step, loss=0.74640, avg_loss=0.77922]\n",
            "Step 24804   [1.689 sec/step, loss=0.82475, avg_loss=0.77956]\n",
            "Step 24805   [1.676 sec/step, loss=0.77144, avg_loss=0.77942]\n",
            "Step 24806   [1.632 sec/step, loss=0.79714, avg_loss=0.78066]\n",
            "Step 24807   [1.632 sec/step, loss=0.80429, avg_loss=0.78113]\n",
            "Step 24808   [1.634 sec/step, loss=0.75067, avg_loss=0.78056]\n",
            "Step 24809   [1.638 sec/step, loss=0.77613, avg_loss=0.78049]\n",
            "Step 24810   [1.631 sec/step, loss=0.80992, avg_loss=0.78111]\n",
            "Step 24811   [1.631 sec/step, loss=0.71465, avg_loss=0.78082]\n",
            "Step 24812   [1.638 sec/step, loss=0.82581, avg_loss=0.78098]\n",
            "Step 24813   [1.641 sec/step, loss=0.82494, avg_loss=0.78121]\n",
            "Step 24814   [1.644 sec/step, loss=0.81951, avg_loss=0.78144]\n",
            "Step 24815   [1.681 sec/step, loss=0.57973, avg_loss=0.77872]\n",
            "Step 24816   [1.677 sec/step, loss=0.77144, avg_loss=0.77816]\n",
            "Step 24817   [1.675 sec/step, loss=0.72844, avg_loss=0.77706]\n",
            "Step 24818   [1.671 sec/step, loss=0.80952, avg_loss=0.77765]\n",
            "Step 24819   [1.669 sec/step, loss=0.77486, avg_loss=0.77775]\n",
            "Step 24820   [1.675 sec/step, loss=0.73624, avg_loss=0.77714]\n",
            "Step 24821   [1.674 sec/step, loss=0.69389, avg_loss=0.77656]\n",
            "Step 24822   [1.674 sec/step, loss=0.76063, avg_loss=0.77692]\n",
            "Step 24823   [1.690 sec/step, loss=0.85371, avg_loss=0.77761]\n",
            "Step 24824   [1.682 sec/step, loss=0.72963, avg_loss=0.77681]\n",
            "Step 24825   [1.689 sec/step, loss=0.77225, avg_loss=0.77628]\n",
            "Step 24826   [1.688 sec/step, loss=0.76577, avg_loss=0.77658]\n",
            "Step 24827   [1.680 sec/step, loss=0.79985, avg_loss=0.77702]\n",
            "Step 24828   [1.677 sec/step, loss=0.77032, avg_loss=0.77738]\n",
            "Generated 32 batches of size 8 in 10.330 sec\n",
            "Step 24829   [1.683 sec/step, loss=0.78741, avg_loss=0.77738]\n",
            "Step 24830   [1.679 sec/step, loss=0.71713, avg_loss=0.77684]\n",
            "Step 24831   [1.677 sec/step, loss=0.74576, avg_loss=0.77615]\n",
            "Step 24832   [1.674 sec/step, loss=0.72173, avg_loss=0.77585]\n",
            "Step 24833   [1.671 sec/step, loss=0.79042, avg_loss=0.77585]\n",
            "Step 24834   [1.677 sec/step, loss=0.74913, avg_loss=0.77557]\n",
            "Step 24835   [1.661 sec/step, loss=0.78613, avg_loss=0.77562]\n",
            "Step 24836   [1.661 sec/step, loss=0.75106, avg_loss=0.77542]\n",
            "Step 24837   [1.661 sec/step, loss=0.76745, avg_loss=0.77509]\n",
            "Step 24838   [1.664 sec/step, loss=0.78272, avg_loss=0.77495]\n",
            "Step 24839   [1.674 sec/step, loss=0.76899, avg_loss=0.77467]\n",
            "Step 24840   [1.673 sec/step, loss=0.79752, avg_loss=0.77517]\n",
            "Step 24841   [1.672 sec/step, loss=0.79350, avg_loss=0.77553]\n",
            "Step 24842   [1.669 sec/step, loss=0.76527, avg_loss=0.77559]\n",
            "Step 24843   [1.667 sec/step, loss=0.78778, avg_loss=0.77603]\n",
            "Step 24844   [1.665 sec/step, loss=0.77610, avg_loss=0.77618]\n",
            "Step 24845   [1.651 sec/step, loss=0.80683, avg_loss=0.77679]\n",
            "Step 24846   [1.636 sec/step, loss=0.74725, avg_loss=0.77643]\n",
            "Step 24847   [1.637 sec/step, loss=0.80610, avg_loss=0.77636]\n",
            "Step 24848   [1.630 sec/step, loss=0.76350, avg_loss=0.77642]\n",
            "Step 24849   [1.634 sec/step, loss=0.78445, avg_loss=0.77620]\n",
            "Step 24850   [1.629 sec/step, loss=0.77436, avg_loss=0.77688]\n",
            "Step 24851   [1.652 sec/step, loss=0.79455, avg_loss=0.77671]\n",
            "Step 24852   [1.654 sec/step, loss=0.81370, avg_loss=0.77686]\n",
            "Step 24853   [1.655 sec/step, loss=0.80459, avg_loss=0.77673]\n",
            "Step 24854   [1.663 sec/step, loss=0.77143, avg_loss=0.77642]\n",
            "Step 24855   [1.671 sec/step, loss=0.76659, avg_loss=0.77580]\n",
            "Step 24856   [1.663 sec/step, loss=0.76337, avg_loss=0.77542]\n",
            "Step 24857   [1.672 sec/step, loss=0.71511, avg_loss=0.77468]\n",
            "Step 24858   [1.677 sec/step, loss=0.70457, avg_loss=0.77372]\n",
            "Step 24859   [1.640 sec/step, loss=0.75501, avg_loss=0.77426]\n",
            "Generated 32 batches of size 8 in 9.918 sec\n",
            "Step 24860   [1.675 sec/step, loss=0.78646, avg_loss=0.77433]\n",
            "Step 24861   [1.667 sec/step, loss=0.79270, avg_loss=0.77430]\n",
            "Step 24862   [1.661 sec/step, loss=0.75469, avg_loss=0.77406]\n",
            "Step 24863   [1.663 sec/step, loss=0.73202, avg_loss=0.77321]\n",
            "Step 24864   [1.652 sec/step, loss=0.79759, avg_loss=0.77345]\n",
            "Step 24865   [1.654 sec/step, loss=0.79887, avg_loss=0.77375]\n",
            "Step 24866   [1.632 sec/step, loss=0.77626, avg_loss=0.77355]\n",
            "Step 24867   [1.628 sec/step, loss=0.77179, avg_loss=0.77323]\n",
            "Step 24868   [1.632 sec/step, loss=0.77891, avg_loss=0.77293]\n",
            "Step 24869   [1.639 sec/step, loss=0.78592, avg_loss=0.77300]\n",
            "Step 24870   [1.618 sec/step, loss=0.73373, avg_loss=0.77253]\n",
            "Step 24871   [1.620 sec/step, loss=0.76649, avg_loss=0.77276]\n",
            "Step 24872   [1.630 sec/step, loss=0.76893, avg_loss=0.77307]\n",
            "Step 24873   [1.640 sec/step, loss=0.82008, avg_loss=0.77337]\n",
            "Step 24874   [1.637 sec/step, loss=0.78818, avg_loss=0.77368]\n",
            "Step 24875   [1.634 sec/step, loss=0.76436, avg_loss=0.77406]\n",
            "Step 24876   [1.633 sec/step, loss=0.77552, avg_loss=0.77363]\n",
            "Step 24877   [1.636 sec/step, loss=0.82144, avg_loss=0.77420]\n",
            "Step 24878   [1.623 sec/step, loss=0.80309, avg_loss=0.77468]\n",
            "Step 24879   [1.624 sec/step, loss=0.82443, avg_loss=0.77500]\n",
            "Step 24880   [1.625 sec/step, loss=0.75717, avg_loss=0.77494]\n",
            "Step 24881   [1.589 sec/step, loss=0.75607, avg_loss=0.77542]\n",
            "Step 24882   [1.584 sec/step, loss=0.76964, avg_loss=0.77532]\n",
            "Step 24883   [1.586 sec/step, loss=0.70446, avg_loss=0.77416]\n",
            "Step 24884   [1.591 sec/step, loss=0.76538, avg_loss=0.77377]\n",
            "Step 24885   [1.598 sec/step, loss=0.78591, avg_loss=0.77388]\n",
            "Step 24886   [1.590 sec/step, loss=0.74181, avg_loss=0.77399]\n",
            "Step 24887   [1.583 sec/step, loss=0.76890, avg_loss=0.77420]\n",
            "Step 24888   [1.592 sec/step, loss=0.76844, avg_loss=0.77384]\n",
            "Step 24889   [1.597 sec/step, loss=0.76414, avg_loss=0.77368]\n",
            "Step 24890   [1.598 sec/step, loss=0.80243, avg_loss=0.77366]\n",
            "Step 24891   [1.596 sec/step, loss=0.74101, avg_loss=0.77337]\n",
            "Step 24892   [1.585 sec/step, loss=0.74547, avg_loss=0.77309]\n",
            "Generated 32 batches of size 8 in 10.238 sec\n",
            "Step 24893   [1.601 sec/step, loss=0.68792, avg_loss=0.77246]\n",
            "Step 24894   [1.598 sec/step, loss=0.73679, avg_loss=0.77175]\n",
            "Step 24895   [1.619 sec/step, loss=0.68638, avg_loss=0.77046]\n",
            "Step 24896   [1.621 sec/step, loss=0.70226, avg_loss=0.76995]\n",
            "Step 24897   [1.614 sec/step, loss=0.80944, avg_loss=0.77060]\n",
            "Step 24898   [1.609 sec/step, loss=0.69666, avg_loss=0.76918]\n",
            "Step 24899   [1.615 sec/step, loss=0.81891, avg_loss=0.76931]\n",
            "Step 24900   [1.615 sec/step, loss=0.76810, avg_loss=0.76916]\n",
            "Writing summary at step: 24900\n",
            "Step 24901   [1.604 sec/step, loss=0.81248, avg_loss=0.76906]\n",
            "Step 24902   [1.610 sec/step, loss=0.82301, avg_loss=0.76926]\n",
            "Step 24903   [1.602 sec/step, loss=0.74691, avg_loss=0.76926]\n",
            "Step 24904   [1.605 sec/step, loss=0.80202, avg_loss=0.76903]\n",
            "Step 24905   [1.606 sec/step, loss=0.79713, avg_loss=0.76929]\n",
            "Step 24906   [1.609 sec/step, loss=0.79212, avg_loss=0.76924]\n",
            "Step 24907   [1.609 sec/step, loss=0.78012, avg_loss=0.76900]\n",
            "Step 24908   [1.601 sec/step, loss=0.81242, avg_loss=0.76962]\n",
            "Step 24909   [1.614 sec/step, loss=0.77671, avg_loss=0.76962]\n",
            "Step 24910   [1.635 sec/step, loss=0.86682, avg_loss=0.77019]\n",
            "Step 24911   [1.639 sec/step, loss=0.78427, avg_loss=0.77089]\n",
            "Step 24912   [1.633 sec/step, loss=0.68109, avg_loss=0.76944]\n",
            "Step 24913   [1.635 sec/step, loss=0.76520, avg_loss=0.76884]\n",
            "Step 24914   [1.636 sec/step, loss=0.79299, avg_loss=0.76858]\n",
            "Step 24915   [1.591 sec/step, loss=0.76616, avg_loss=0.77044]\n",
            "Step 24916   [1.595 sec/step, loss=0.80187, avg_loss=0.77075]\n",
            "Step 24917   [1.593 sec/step, loss=0.76342, avg_loss=0.77110]\n",
            "Step 24918   [1.594 sec/step, loss=0.77113, avg_loss=0.77071]\n",
            "Step 24919   [1.596 sec/step, loss=0.79505, avg_loss=0.77091]\n",
            "Step 24920   [1.638 sec/step, loss=0.78025, avg_loss=0.77135]\n",
            "Step 24921   [1.632 sec/step, loss=0.74957, avg_loss=0.77191]\n",
            "Generated 32 batches of size 8 in 10.524 sec\n",
            "Step 24922   [1.641 sec/step, loss=0.78728, avg_loss=0.77218]\n",
            "Step 24923   [1.635 sec/step, loss=0.75450, avg_loss=0.77119]\n",
            "Step 24924   [1.647 sec/step, loss=0.79603, avg_loss=0.77185]\n",
            "Step 24925   [1.637 sec/step, loss=0.72851, avg_loss=0.77141]\n",
            "Step 24926   [1.636 sec/step, loss=0.74082, avg_loss=0.77116]\n",
            "Step 24927   [1.638 sec/step, loss=0.77690, avg_loss=0.77093]\n",
            "Step 24928   [1.635 sec/step, loss=0.80573, avg_loss=0.77129]\n",
            "Step 24929   [1.630 sec/step, loss=0.83904, avg_loss=0.77180]\n",
            "Step 24930   [1.631 sec/step, loss=0.83172, avg_loss=0.77295]\n",
            "Step 24931   [1.633 sec/step, loss=0.78964, avg_loss=0.77339]\n",
            "Step 24932   [1.656 sec/step, loss=0.81582, avg_loss=0.77433]\n",
            "Step 24933   [1.656 sec/step, loss=0.81139, avg_loss=0.77454]\n",
            "Step 24934   [1.658 sec/step, loss=0.84401, avg_loss=0.77549]\n",
            "Step 24935   [1.667 sec/step, loss=0.86633, avg_loss=0.77629]\n",
            "Step 24936   [1.666 sec/step, loss=0.80937, avg_loss=0.77687]\n",
            "Step 24937   [1.658 sec/step, loss=0.75709, avg_loss=0.77677]\n",
            "Step 24938   [1.657 sec/step, loss=0.79953, avg_loss=0.77694]\n",
            "Step 24939   [1.652 sec/step, loss=0.81432, avg_loss=0.77739]\n",
            "Step 24940   [1.648 sec/step, loss=0.79511, avg_loss=0.77737]\n",
            "Step 24941   [1.644 sec/step, loss=0.75563, avg_loss=0.77699]\n",
            "Step 24942   [1.643 sec/step, loss=0.75000, avg_loss=0.77684]\n",
            "Step 24943   [1.646 sec/step, loss=0.77214, avg_loss=0.77668]\n",
            "Step 24944   [1.646 sec/step, loss=0.77936, avg_loss=0.77671]\n",
            "Step 24945   [1.665 sec/step, loss=0.76063, avg_loss=0.77625]\n",
            "Step 24946   [1.671 sec/step, loss=0.81621, avg_loss=0.77694]\n",
            "Step 24947   [1.667 sec/step, loss=0.77650, avg_loss=0.77664]\n",
            "Step 24948   [1.668 sec/step, loss=0.79524, avg_loss=0.77696]\n",
            "Step 24949   [1.668 sec/step, loss=0.76005, avg_loss=0.77672]\n",
            "Step 24950   [1.722 sec/step, loss=0.69752, avg_loss=0.77595]\n",
            "Step 24951   [1.701 sec/step, loss=0.78731, avg_loss=0.77588]\n",
            "Generated 32 batches of size 8 in 8.952 sec\n",
            "Step 24952   [1.707 sec/step, loss=0.80906, avg_loss=0.77583]\n",
            "Step 24953   [1.707 sec/step, loss=0.77961, avg_loss=0.77558]\n",
            "Step 24954   [1.697 sec/step, loss=0.76486, avg_loss=0.77551]\n",
            "Step 24955   [1.699 sec/step, loss=0.80090, avg_loss=0.77586]\n",
            "Step 24956   [1.699 sec/step, loss=0.75925, avg_loss=0.77582]\n",
            "Step 24957   [1.706 sec/step, loss=0.78880, avg_loss=0.77655]\n",
            "Step 24958   [1.691 sec/step, loss=0.78350, avg_loss=0.77734]\n",
            "Step 24959   [1.689 sec/step, loss=0.77211, avg_loss=0.77751]\n",
            "Step 24960   [1.651 sec/step, loss=0.72375, avg_loss=0.77689]\n",
            "Step 24961   [1.650 sec/step, loss=0.79678, avg_loss=0.77693]\n",
            "Step 24962   [1.657 sec/step, loss=0.75310, avg_loss=0.77691]\n",
            "Step 24963   [1.656 sec/step, loss=0.71913, avg_loss=0.77678]\n",
            "Step 24964   [1.672 sec/step, loss=0.78666, avg_loss=0.77667]\n",
            "Step 24965   [1.671 sec/step, loss=0.79102, avg_loss=0.77659]\n",
            "Step 24966   [1.676 sec/step, loss=0.68077, avg_loss=0.77564]\n",
            "Step 24967   [1.679 sec/step, loss=0.75653, avg_loss=0.77549]\n",
            "Step 24968   [1.674 sec/step, loss=0.83954, avg_loss=0.77609]\n",
            "Step 24969   [1.666 sec/step, loss=0.75548, avg_loss=0.77579]\n",
            "Step 24970   [1.663 sec/step, loss=0.80663, avg_loss=0.77652]\n",
            "Step 24971   [1.656 sec/step, loss=0.76858, avg_loss=0.77654]\n",
            "Step 24972   [1.666 sec/step, loss=0.79195, avg_loss=0.77677]\n",
            "Step 24973   [1.657 sec/step, loss=0.73878, avg_loss=0.77596]\n",
            "Step 24974   [1.696 sec/step, loss=0.70026, avg_loss=0.77508]\n",
            "Step 24975   [1.698 sec/step, loss=0.76116, avg_loss=0.77504]\n",
            "Step 24976   [1.699 sec/step, loss=0.78462, avg_loss=0.77514]\n",
            "Step 24977   [1.698 sec/step, loss=0.79572, avg_loss=0.77488]\n",
            "Step 24978   [1.697 sec/step, loss=0.79136, avg_loss=0.77476]\n",
            "Step 24979   [1.700 sec/step, loss=0.73412, avg_loss=0.77386]\n",
            "Step 24980   [1.699 sec/step, loss=0.74525, avg_loss=0.77374]\n",
            "Step 24981   [1.699 sec/step, loss=0.74586, avg_loss=0.77364]\n",
            "Step 24982   [1.704 sec/step, loss=0.72995, avg_loss=0.77324]\n",
            "Step 24983   [1.717 sec/step, loss=0.76232, avg_loss=0.77382]\n",
            "Step 24984   [1.718 sec/step, loss=0.77731, avg_loss=0.77394]\n",
            "Generated 32 batches of size 8 in 7.156 sec\n",
            "Step 24985   [1.715 sec/step, loss=0.78628, avg_loss=0.77394]\n",
            "Step 24986   [1.722 sec/step, loss=0.74196, avg_loss=0.77394]\n",
            "Step 24987   [1.724 sec/step, loss=0.82211, avg_loss=0.77448]\n",
            "Step 24988   [1.721 sec/step, loss=0.76602, avg_loss=0.77445]\n",
            "Step 24989   [1.717 sec/step, loss=0.78817, avg_loss=0.77469]\n",
            "Step 24990   [1.712 sec/step, loss=0.78601, avg_loss=0.77453]\n",
            "Step 24991   [1.720 sec/step, loss=0.81126, avg_loss=0.77523]\n",
            "Step 24992   [1.724 sec/step, loss=0.81803, avg_loss=0.77595]\n",
            "Step 24993   [1.699 sec/step, loss=0.80485, avg_loss=0.77712]\n",
            "Step 24994   [1.705 sec/step, loss=0.76972, avg_loss=0.77745]\n",
            "Step 24995   [1.668 sec/step, loss=0.74938, avg_loss=0.77808]\n",
            "Step 24996   [1.664 sec/step, loss=0.79026, avg_loss=0.77896]\n",
            "Step 24997   [1.672 sec/step, loss=0.70055, avg_loss=0.77787]\n",
            "Step 24998   [1.679 sec/step, loss=0.77399, avg_loss=0.77865]\n",
            "Step 24999   [1.674 sec/step, loss=0.85049, avg_loss=0.77896]\n",
            "Step 25000   [1.672 sec/step, loss=0.73767, avg_loss=0.77866]\n",
            "Writing summary at step: 25000\n",
            "Saving checkpoint to: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/model.ckpt-25000\n",
            "Saving audio and alignment...\n",
            "  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n",
            " [*] Plot saved: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/train-step-000025000-align000.png\n",
            "100% 1/1 [00:11<00:00, 11.30s/it]\n",
            "Test finished for step 25000.\n",
            "  0% 0/4 [00:00<?, ?it/s]Training korean : Use jamo\n",
            " [*] Plot saved: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/test-step-000025000-align000.png\n",
            " 25% 1/4 [00:10<00:32, 10.96s/it]Training korean : Use jamo\n",
            " [*] Plot saved: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/test-step-000025000-align001.png\n",
            " 50% 2/4 [00:21<00:21, 10.98s/it]Training korean : Use jamo\n",
            " [*] Plot saved: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/test-step-000025000-align002.png\n",
            " 75% 3/4 [00:33<00:11, 11.00s/it]Training korean : Use jamo\n",
            " [*] Plot saved: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/test-step-000025000-align003.png\n",
            "100% 4/4 [00:44<00:00, 11.08s/it]\n",
            "Test finished for step 25000.\n",
            "Step 25001   [1.672 sec/step, loss=0.80912, avg_loss=0.77863]\n",
            "Step 25002   [1.672 sec/step, loss=0.81709, avg_loss=0.77857]\n",
            "Step 25003   [1.662 sec/step, loss=0.76803, avg_loss=0.77878]\n",
            "Step 25004   [1.657 sec/step, loss=0.80826, avg_loss=0.77884]\n",
            "Step 25005   [1.670 sec/step, loss=0.73994, avg_loss=0.77827]\n",
            "Step 25006   [1.676 sec/step, loss=0.74443, avg_loss=0.77779]\n",
            "Step 25007   [1.688 sec/step, loss=0.80394, avg_loss=0.77803]\n",
            "Step 25008   [1.690 sec/step, loss=0.77388, avg_loss=0.77764]\n",
            "Step 25009   [1.679 sec/step, loss=0.82394, avg_loss=0.77812]\n",
            "Step 25010   [1.657 sec/step, loss=0.73997, avg_loss=0.77685]\n",
            "Step 25011   [1.652 sec/step, loss=0.72478, avg_loss=0.77625]\n",
            "Step 25012   [1.657 sec/step, loss=0.76854, avg_loss=0.77713]\n",
            "Step 25013   [1.655 sec/step, loss=0.78764, avg_loss=0.77735]\n",
            "Generated 32 batches of size 8 in 7.137 sec\n",
            "Step 25014   [1.689 sec/step, loss=0.76949, avg_loss=0.77712]\n",
            "Step 25015   [1.687 sec/step, loss=0.76997, avg_loss=0.77716]\n",
            "Step 25016   [1.684 sec/step, loss=0.82484, avg_loss=0.77738]\n",
            "Step 25017   [1.686 sec/step, loss=0.78323, avg_loss=0.77758]\n",
            "Step 25018   [1.683 sec/step, loss=0.76598, avg_loss=0.77753]\n",
            "Step 25019   [1.681 sec/step, loss=0.81072, avg_loss=0.77769]\n",
            "Step 25020   [1.644 sec/step, loss=0.75147, avg_loss=0.77740]\n",
            "Step 25021   [1.653 sec/step, loss=0.74009, avg_loss=0.77731]\n",
            "Step 25022   [1.644 sec/step, loss=0.79783, avg_loss=0.77741]\n",
            "Step 25023   [1.635 sec/step, loss=0.82259, avg_loss=0.77809]\n",
            "Step 25024   [1.624 sec/step, loss=0.78026, avg_loss=0.77793]\n",
            "Step 25025   [1.622 sec/step, loss=0.77862, avg_loss=0.77844]\n",
            "Step 25026   [1.618 sec/step, loss=0.75507, avg_loss=0.77858]\n",
            "Step 25027   [1.616 sec/step, loss=0.77113, avg_loss=0.77852]\n",
            "Step 25028   [1.619 sec/step, loss=0.77730, avg_loss=0.77824]\n",
            "Step 25029   [1.620 sec/step, loss=0.75515, avg_loss=0.77740]\n",
            "Step 25030   [1.624 sec/step, loss=0.84302, avg_loss=0.77751]\n",
            "Step 25031   [1.621 sec/step, loss=0.78437, avg_loss=0.77746]\n",
            "Step 25032   [1.608 sec/step, loss=0.84971, avg_loss=0.77780]\n",
            "Step 25033   [1.613 sec/step, loss=0.79093, avg_loss=0.77759]\n",
            "Step 25034   [1.610 sec/step, loss=0.75444, avg_loss=0.77670]\n",
            "Step 25035   [1.617 sec/step, loss=0.71624, avg_loss=0.77519]\n",
            "Step 25036   [1.621 sec/step, loss=0.80112, avg_loss=0.77511]\n",
            "Step 25037   [1.626 sec/step, loss=0.71925, avg_loss=0.77473]\n",
            "Step 25038   [1.626 sec/step, loss=0.77070, avg_loss=0.77445]\n",
            "Step 25039   [1.623 sec/step, loss=0.80018, avg_loss=0.77430]\n",
            "Step 25040   [1.630 sec/step, loss=0.72836, avg_loss=0.77364]\n",
            "Step 25041   [1.630 sec/step, loss=0.79152, avg_loss=0.77400]\n",
            "Step 25042   [1.635 sec/step, loss=0.81909, avg_loss=0.77469]\n",
            "Step 25043   [1.658 sec/step, loss=0.78081, avg_loss=0.77477]\n",
            "Step 25044   [1.664 sec/step, loss=0.72020, avg_loss=0.77418]\n",
            "Step 25045   [1.649 sec/step, loss=0.82470, avg_loss=0.77482]\n",
            "Step 25046   [1.668 sec/step, loss=0.79140, avg_loss=0.77457]\n",
            "Generated 32 batches of size 8 in 7.577 sec\n",
            "Step 25047   [1.670 sec/step, loss=0.80940, avg_loss=0.77490]\n",
            "Step 25048   [1.679 sec/step, loss=0.76376, avg_loss=0.77459]\n",
            "Step 25049   [1.692 sec/step, loss=0.72898, avg_loss=0.77428]\n",
            "Step 25050   [1.641 sec/step, loss=0.74683, avg_loss=0.77477]\n",
            "Step 25051   [1.641 sec/step, loss=0.76745, avg_loss=0.77457]\n",
            "Step 25052   [1.626 sec/step, loss=0.78920, avg_loss=0.77437]\n",
            "Step 25053   [1.625 sec/step, loss=0.76929, avg_loss=0.77427]\n",
            "Step 25054   [1.635 sec/step, loss=0.76806, avg_loss=0.77430]\n",
            "Step 25055   [1.631 sec/step, loss=0.81610, avg_loss=0.77445]\n",
            "Step 25056   [1.634 sec/step, loss=0.75723, avg_loss=0.77443]\n",
            "Step 25057   [1.621 sec/step, loss=0.78973, avg_loss=0.77444]\n",
            "Step 25058   [1.635 sec/step, loss=0.77543, avg_loss=0.77436]\n",
            "Step 25059   [1.642 sec/step, loss=0.79023, avg_loss=0.77454]\n",
            "Step 25060   [1.641 sec/step, loss=0.75908, avg_loss=0.77490]\n",
            "Step 25061   [1.678 sec/step, loss=0.64673, avg_loss=0.77340]\n",
            "Step 25062   [1.682 sec/step, loss=0.84173, avg_loss=0.77428]\n",
            "Step 25063   [1.674 sec/step, loss=0.69836, avg_loss=0.77408]\n",
            "Step 25064   [1.658 sec/step, loss=0.74850, avg_loss=0.77369]\n",
            "Step 25065   [1.657 sec/step, loss=0.80818, avg_loss=0.77387]\n",
            "Step 25066   [1.666 sec/step, loss=0.78206, avg_loss=0.77488]\n",
            "Step 25067   [1.676 sec/step, loss=0.78173, avg_loss=0.77513]\n",
            "Step 25068   [1.680 sec/step, loss=0.76818, avg_loss=0.77442]\n",
            "Step 25069   [1.685 sec/step, loss=0.75215, avg_loss=0.77438]\n",
            "Step 25070   [1.687 sec/step, loss=0.79615, avg_loss=0.77428]\n",
            "Step 25071   [1.695 sec/step, loss=0.72954, avg_loss=0.77389]\n",
            "Step 25072   [1.675 sec/step, loss=0.73791, avg_loss=0.77335]\n",
            "Step 25073   [1.676 sec/step, loss=0.79399, avg_loss=0.77390]\n",
            "Step 25074   [1.634 sec/step, loss=0.78515, avg_loss=0.77475]\n",
            "Step 25075   [1.633 sec/step, loss=0.76613, avg_loss=0.77480]\n",
            "Step 25076   [1.637 sec/step, loss=0.78973, avg_loss=0.77485]\n",
            "Step 25077   [1.628 sec/step, loss=0.79484, avg_loss=0.77484]\n",
            "Step 25078   [1.629 sec/step, loss=0.74292, avg_loss=0.77436]\n",
            "Step 25079   [1.633 sec/step, loss=0.73819, avg_loss=0.77440]\n",
            "Step 25080   [1.636 sec/step, loss=0.80087, avg_loss=0.77495]\n",
            "Generated 32 batches of size 8 in 8.287 sec\n",
            "Step 25081   [1.631 sec/step, loss=0.78627, avg_loss=0.77536]\n",
            "Step 25082   [1.630 sec/step, loss=0.75320, avg_loss=0.77559]\n",
            "Step 25083   [1.615 sec/step, loss=0.76748, avg_loss=0.77564]\n",
            "Step 25084   [1.610 sec/step, loss=0.77681, avg_loss=0.77564]\n",
            "Step 25085   [1.611 sec/step, loss=0.82841, avg_loss=0.77606]\n",
            "Step 25086   [1.609 sec/step, loss=0.76309, avg_loss=0.77627]\n",
            "Step 25087   [1.613 sec/step, loss=0.73703, avg_loss=0.77542]\n",
            "Step 25088   [1.608 sec/step, loss=0.73579, avg_loss=0.77512]\n",
            "Step 25089   [1.605 sec/step, loss=0.75594, avg_loss=0.77479]\n",
            "Step 25090   [1.609 sec/step, loss=0.77073, avg_loss=0.77464]\n",
            "Step 25091   [1.603 sec/step, loss=0.82111, avg_loss=0.77474]\n",
            "Step 25092   [1.636 sec/step, loss=0.64191, avg_loss=0.77298]\n",
            "Step 25093   [1.645 sec/step, loss=0.79920, avg_loss=0.77292]\n",
            "Step 25094   [1.638 sec/step, loss=0.76047, avg_loss=0.77283]\n",
            "Step 25095   [1.638 sec/step, loss=0.81338, avg_loss=0.77347]\n",
            "Step 25096   [1.640 sec/step, loss=0.73390, avg_loss=0.77291]\n",
            "Step 25097   [1.653 sec/step, loss=0.79185, avg_loss=0.77382]\n",
            "Step 25098   [1.648 sec/step, loss=0.76389, avg_loss=0.77372]\n",
            "Step 25099   [1.651 sec/step, loss=0.76844, avg_loss=0.77290]\n",
            "Step 25100   [1.656 sec/step, loss=0.74553, avg_loss=0.77298]\n",
            "Writing summary at step: 25100\n",
            "Step 25101   [1.657 sec/step, loss=0.73913, avg_loss=0.77228]\n",
            "Step 25102   [1.652 sec/step, loss=0.84392, avg_loss=0.77254]\n",
            "Step 25103   [1.651 sec/step, loss=0.76133, avg_loss=0.77248]\n",
            "Step 25104   [1.651 sec/step, loss=0.80264, avg_loss=0.77242]\n",
            "Step 25105   [1.634 sec/step, loss=0.78805, avg_loss=0.77290]\n",
            "Step 25106   [1.625 sec/step, loss=0.79619, avg_loss=0.77342]\n",
            "Step 25107   [1.620 sec/step, loss=0.80082, avg_loss=0.77339]\n",
            "Step 25108   [1.637 sec/step, loss=0.72495, avg_loss=0.77290]\n",
            "Step 25109   [1.642 sec/step, loss=0.69364, avg_loss=0.77160]\n",
            "Step 25110   [1.645 sec/step, loss=0.78284, avg_loss=0.77203]\n",
            "Generated 32 batches of size 8 in 9.017 sec\n",
            "Step 25111   [1.643 sec/step, loss=0.76904, avg_loss=0.77247]\n",
            "Step 25112   [1.635 sec/step, loss=0.78719, avg_loss=0.77265]\n",
            "Step 25113   [1.632 sec/step, loss=0.75385, avg_loss=0.77232]\n",
            "Step 25114   [1.602 sec/step, loss=0.70100, avg_loss=0.77163]\n",
            "Step 25115   [1.613 sec/step, loss=0.76822, avg_loss=0.77161]\n",
            "Step 25116   [1.617 sec/step, loss=0.78365, avg_loss=0.77120]\n",
            "Step 25117   [1.629 sec/step, loss=0.72578, avg_loss=0.77063]\n",
            "Step 25118   [1.635 sec/step, loss=0.81785, avg_loss=0.77115]\n",
            "Step 25119   [1.681 sec/step, loss=0.58144, avg_loss=0.76885]\n",
            "Step 25120   [1.671 sec/step, loss=0.77913, avg_loss=0.76913]\n",
            "Step 25121   [1.666 sec/step, loss=0.75969, avg_loss=0.76933]\n",
            "Step 25122   [1.666 sec/step, loss=0.75626, avg_loss=0.76891]\n",
            "Step 25123   [1.668 sec/step, loss=0.75295, avg_loss=0.76821]\n",
            "Step 25124   [1.671 sec/step, loss=0.76782, avg_loss=0.76809]\n",
            "Step 25125   [1.672 sec/step, loss=0.75948, avg_loss=0.76790]\n",
            "Step 25126   [1.689 sec/step, loss=0.75027, avg_loss=0.76785]\n",
            "Step 25127   [1.687 sec/step, loss=0.75201, avg_loss=0.76766]\n",
            "Step 25128   [1.689 sec/step, loss=0.81757, avg_loss=0.76806]\n",
            "Step 25129   [1.695 sec/step, loss=0.79184, avg_loss=0.76843]\n",
            "Step 25130   [1.696 sec/step, loss=0.73053, avg_loss=0.76730]\n",
            "Step 25131   [1.695 sec/step, loss=0.79809, avg_loss=0.76744]\n",
            "Step 25132   [1.697 sec/step, loss=0.75471, avg_loss=0.76649]\n",
            "Step 25133   [1.691 sec/step, loss=0.78793, avg_loss=0.76646]\n",
            "Step 25134   [1.684 sec/step, loss=0.78283, avg_loss=0.76674]\n",
            "Step 25135   [1.664 sec/step, loss=0.72636, avg_loss=0.76685]\n",
            "Step 25136   [1.667 sec/step, loss=0.78515, avg_loss=0.76669]\n",
            "Step 25137   [1.664 sec/step, loss=0.78664, avg_loss=0.76736]\n",
            "Step 25138   [1.663 sec/step, loss=0.77579, avg_loss=0.76741]\n",
            "Step 25139   [1.668 sec/step, loss=0.78892, avg_loss=0.76730]\n",
            "Step 25140   [1.664 sec/step, loss=0.73777, avg_loss=0.76739]\n",
            "Step 25141   [1.676 sec/step, loss=0.78477, avg_loss=0.76733]\n",
            "Step 25142   [1.675 sec/step, loss=0.76265, avg_loss=0.76676]\n",
            "Step 25143   [1.662 sec/step, loss=0.73794, avg_loss=0.76633]\n",
            "Generated 32 batches of size 8 in 9.458 sec\n",
            "Step 25144   [1.659 sec/step, loss=0.81204, avg_loss=0.76725]\n",
            "Step 25145   [1.658 sec/step, loss=0.76798, avg_loss=0.76668]\n",
            "Step 25146   [1.639 sec/step, loss=0.76594, avg_loss=0.76643]\n",
            "Step 25147   [1.635 sec/step, loss=0.80240, avg_loss=0.76636]\n",
            "Step 25148   [1.630 sec/step, loss=0.77126, avg_loss=0.76643]\n",
            "Step 25149   [1.611 sec/step, loss=0.74963, avg_loss=0.76664]\n",
            "Step 25150   [1.610 sec/step, loss=0.77592, avg_loss=0.76693]\n",
            "Step 25151   [1.616 sec/step, loss=0.79222, avg_loss=0.76718]\n",
            "Step 25152   [1.620 sec/step, loss=0.81624, avg_loss=0.76745]\n",
            "Step 25153   [1.645 sec/step, loss=0.75562, avg_loss=0.76731]\n",
            "Step 25154   [1.639 sec/step, loss=0.79453, avg_loss=0.76758]\n",
            "Step 25155   [1.638 sec/step, loss=0.78414, avg_loss=0.76726]\n",
            "Step 25156   [1.639 sec/step, loss=0.78149, avg_loss=0.76750]\n",
            "Step 25157   [1.635 sec/step, loss=0.78829, avg_loss=0.76749]\n",
            "Step 25158   [1.617 sec/step, loss=0.75196, avg_loss=0.76725]\n",
            "Step 25159   [1.612 sec/step, loss=0.81381, avg_loss=0.76749]\n",
            "Step 25160   [1.613 sec/step, loss=0.74650, avg_loss=0.76736]\n",
            "Step 25161   [1.577 sec/step, loss=0.74479, avg_loss=0.76834]\n",
            "Step 25162   [1.568 sec/step, loss=0.79595, avg_loss=0.76788]\n",
            "Step 25163   [1.571 sec/step, loss=0.75809, avg_loss=0.76848]\n",
            "Step 25164   [1.576 sec/step, loss=0.73723, avg_loss=0.76837]\n",
            "Step 25165   [1.583 sec/step, loss=0.81726, avg_loss=0.76846]\n",
            "Step 25166   [1.604 sec/step, loss=0.76851, avg_loss=0.76832]\n",
            "Step 25167   [1.591 sec/step, loss=0.77738, avg_loss=0.76828]\n",
            "Step 25168   [1.589 sec/step, loss=0.78864, avg_loss=0.76848]\n",
            "Step 25169   [1.603 sec/step, loss=0.73115, avg_loss=0.76827]\n",
            "Step 25170   [1.601 sec/step, loss=0.75713, avg_loss=0.76788]\n",
            "Step 25171   [1.600 sec/step, loss=0.79116, avg_loss=0.76850]\n",
            "Step 25172   [1.607 sec/step, loss=0.76897, avg_loss=0.76881]\n",
            "Step 25173   [1.613 sec/step, loss=0.78546, avg_loss=0.76873]\n",
            "Step 25174   [1.616 sec/step, loss=0.81520, avg_loss=0.76903]\n",
            "Step 25175   [1.618 sec/step, loss=0.79897, avg_loss=0.76936]\n",
            "Step 25176   [1.621 sec/step, loss=0.81682, avg_loss=0.76963]\n",
            "Generated 32 batches of size 8 in 10.112 sec\n",
            "Step 25177   [1.619 sec/step, loss=0.78402, avg_loss=0.76952]\n",
            "Step 25178   [1.631 sec/step, loss=0.72224, avg_loss=0.76931]\n",
            "Step 25179   [1.627 sec/step, loss=0.79396, avg_loss=0.76987]\n",
            "Step 25180   [1.626 sec/step, loss=0.76421, avg_loss=0.76950]\n",
            "Step 25181   [1.624 sec/step, loss=0.76546, avg_loss=0.76929]\n",
            "Step 25182   [1.625 sec/step, loss=0.75960, avg_loss=0.76936]\n",
            "Step 25183   [1.624 sec/step, loss=0.82205, avg_loss=0.76990]\n",
            "Step 25184   [1.623 sec/step, loss=0.76866, avg_loss=0.76982]\n",
            "Step 25185   [1.641 sec/step, loss=0.75715, avg_loss=0.76911]\n",
            "Step 25186   [1.640 sec/step, loss=0.71287, avg_loss=0.76861]\n",
            "Step 25187   [1.672 sec/step, loss=0.78139, avg_loss=0.76905]\n",
            "Step 25188   [1.673 sec/step, loss=0.77212, avg_loss=0.76941]\n",
            "Step 25189   [1.679 sec/step, loss=0.80376, avg_loss=0.76989]\n",
            "Step 25190   [1.683 sec/step, loss=0.79151, avg_loss=0.77010]\n",
            "Step 25191   [1.693 sec/step, loss=0.81071, avg_loss=0.77000]\n",
            "Step 25192   [1.654 sec/step, loss=0.77491, avg_loss=0.77133]\n",
            "Step 25193   [1.653 sec/step, loss=0.75408, avg_loss=0.77088]\n",
            "Step 25194   [1.657 sec/step, loss=0.80281, avg_loss=0.77130]\n",
            "Step 25195   [1.657 sec/step, loss=0.76087, avg_loss=0.77077]\n",
            "Step 25196   [1.668 sec/step, loss=0.75394, avg_loss=0.77097]\n",
            "Step 25197   [1.654 sec/step, loss=0.81170, avg_loss=0.77117]\n",
            "Step 25198   [1.656 sec/step, loss=0.74900, avg_loss=0.77102]\n",
            "Step 25199   [1.657 sec/step, loss=0.79908, avg_loss=0.77133]\n",
            "Step 25200   [1.658 sec/step, loss=0.82568, avg_loss=0.77213]\n",
            "Writing summary at step: 25200\n",
            "Step 25201   [1.652 sec/step, loss=0.75330, avg_loss=0.77227]\n",
            "Step 25202   [1.660 sec/step, loss=0.80390, avg_loss=0.77187]\n",
            "Step 25203   [1.665 sec/step, loss=0.73672, avg_loss=0.77163]\n",
            "Step 25204   [1.670 sec/step, loss=0.78010, avg_loss=0.77140]\n",
            "Step 25205   [1.673 sec/step, loss=0.76949, avg_loss=0.77122]\n",
            "Step 25206   [1.678 sec/step, loss=0.78039, avg_loss=0.77106]\n",
            "Step 25207   [1.670 sec/step, loss=0.71489, avg_loss=0.77020]\n",
            "Step 25208   [1.653 sec/step, loss=0.82264, avg_loss=0.77118]\n",
            "Generated 32 batches of size 8 in 10.488 sec\n",
            "Step 25209   [1.661 sec/step, loss=0.79971, avg_loss=0.77224]\n",
            "Step 25210   [1.659 sec/step, loss=0.82024, avg_loss=0.77261]\n",
            "Step 25211   [1.662 sec/step, loss=0.75868, avg_loss=0.77251]\n",
            "Step 25212   [1.672 sec/step, loss=0.74651, avg_loss=0.77210]\n",
            "Step 25213   [1.680 sec/step, loss=0.82334, avg_loss=0.77279]\n",
            "Step 25214   [1.684 sec/step, loss=0.79212, avg_loss=0.77371]\n",
            "Step 25215   [1.674 sec/step, loss=0.77428, avg_loss=0.77377]\n",
            "Step 25216   [1.712 sec/step, loss=0.74376, avg_loss=0.77337]\n",
            "Step 25217   [1.716 sec/step, loss=0.79131, avg_loss=0.77402]\n",
            "Step 25218   [1.716 sec/step, loss=0.80823, avg_loss=0.77393]\n",
            "Step 25219   [1.671 sec/step, loss=0.77608, avg_loss=0.77587]\n",
            "Step 25220   [1.679 sec/step, loss=0.79659, avg_loss=0.77605]\n",
            "Step 25221   [1.678 sec/step, loss=0.77468, avg_loss=0.77620]\n",
            "Step 25222   [1.679 sec/step, loss=0.79581, avg_loss=0.77659]\n",
            "Step 25223   [1.677 sec/step, loss=0.78241, avg_loss=0.77689]\n",
            "Step 25224   [1.671 sec/step, loss=0.81147, avg_loss=0.77732]\n",
            "Step 25225   [1.670 sec/step, loss=0.75211, avg_loss=0.77725]\n",
            "Step 25226   [1.658 sec/step, loss=0.73801, avg_loss=0.77713]\n",
            "Step 25227   [1.663 sec/step, loss=0.77846, avg_loss=0.77739]\n",
            "Step 25228   [1.656 sec/step, loss=0.83491, avg_loss=0.77757]\n",
            "Step 25229   [1.647 sec/step, loss=0.79009, avg_loss=0.77755]\n",
            "Step 25230   [1.650 sec/step, loss=0.80974, avg_loss=0.77834]\n",
            "Step 25231   [1.655 sec/step, loss=0.74482, avg_loss=0.77781]\n",
            "Step 25232   [1.663 sec/step, loss=0.77765, avg_loss=0.77804]\n",
            "Step 25233   [1.666 sec/step, loss=0.74634, avg_loss=0.77762]\n",
            "Step 25234   [1.673 sec/step, loss=0.78629, avg_loss=0.77766]\n",
            "Step 25235   [1.677 sec/step, loss=0.74971, avg_loss=0.77789]\n",
            "Step 25236   [1.674 sec/step, loss=0.78967, avg_loss=0.77793]\n",
            "Step 25237   [1.674 sec/step, loss=0.77909, avg_loss=0.77786]\n",
            "Step 25238   [1.695 sec/step, loss=0.83373, avg_loss=0.77844]\n",
            "Generated 32 batches of size 8 in 10.505 sec\n",
            "Step 25239   [1.698 sec/step, loss=0.80002, avg_loss=0.77855]\n",
            "Step 25240   [1.700 sec/step, loss=0.82300, avg_loss=0.77940]\n",
            "Step 25241   [1.691 sec/step, loss=0.75632, avg_loss=0.77912]\n",
            "Step 25242   [1.693 sec/step, loss=0.77917, avg_loss=0.77928]\n",
            "Step 25243   [1.684 sec/step, loss=0.74848, avg_loss=0.77939]\n",
            "Step 25244   [1.684 sec/step, loss=0.84126, avg_loss=0.77968]\n",
            "Step 25245   [1.685 sec/step, loss=0.79420, avg_loss=0.77994]\n",
            "Step 25246   [1.680 sec/step, loss=0.81705, avg_loss=0.78045]\n",
            "Step 25247   [1.692 sec/step, loss=0.76015, avg_loss=0.78003]\n",
            "Step 25248   [1.727 sec/step, loss=0.70451, avg_loss=0.77936]\n",
            "Step 25249   [1.728 sec/step, loss=0.77845, avg_loss=0.77965]\n",
            "Step 25250   [1.733 sec/step, loss=0.80656, avg_loss=0.77996]\n",
            "Step 25251   [1.740 sec/step, loss=0.75920, avg_loss=0.77963]\n",
            "Step 25252   [1.740 sec/step, loss=0.79470, avg_loss=0.77941]\n",
            "Step 25253   [1.727 sec/step, loss=0.75620, avg_loss=0.77942]\n",
            "Step 25254   [1.729 sec/step, loss=0.75011, avg_loss=0.77897]\n",
            "Step 25255   [1.727 sec/step, loss=0.78727, avg_loss=0.77900]\n",
            "Step 25256   [1.719 sec/step, loss=0.80005, avg_loss=0.77919]\n",
            "Step 25257   [1.724 sec/step, loss=0.79268, avg_loss=0.77923]\n",
            "Step 25258   [1.721 sec/step, loss=0.78279, avg_loss=0.77954]\n",
            "Step 25259   [1.722 sec/step, loss=0.82175, avg_loss=0.77962]\n",
            "Step 25260   [1.721 sec/step, loss=0.72511, avg_loss=0.77941]\n",
            "Step 25261   [1.721 sec/step, loss=0.76318, avg_loss=0.77959]\n",
            "Step 25262   [1.727 sec/step, loss=0.80794, avg_loss=0.77971]\n",
            "Step 25263   [1.742 sec/step, loss=0.76045, avg_loss=0.77974]\n",
            "Step 25264   [1.739 sec/step, loss=0.77049, avg_loss=0.78007]\n",
            "Step 25265   [1.733 sec/step, loss=0.75509, avg_loss=0.77945]\n",
            "Step 25266   [1.722 sec/step, loss=0.78158, avg_loss=0.77958]\n",
            "Step 25267   [1.733 sec/step, loss=0.74032, avg_loss=0.77921]\n",
            "Step 25268   [1.748 sec/step, loss=0.77758, avg_loss=0.77910]\n",
            "Step 25269   [1.732 sec/step, loss=0.77706, avg_loss=0.77955]\n",
            "Generated 32 batches of size 8 in 10.312 sec\n",
            "Step 25270   [1.746 sec/step, loss=0.79116, avg_loss=0.77989]\n",
            "Step 25271   [1.748 sec/step, loss=0.77487, avg_loss=0.77973]\n",
            "Step 25272   [1.743 sec/step, loss=0.77838, avg_loss=0.77983]\n",
            "Step 25273   [1.733 sec/step, loss=0.75581, avg_loss=0.77953]\n",
            "Step 25274   [1.731 sec/step, loss=0.74797, avg_loss=0.77886]\n",
            "Step 25275   [1.732 sec/step, loss=0.79264, avg_loss=0.77879]\n",
            "Step 25276   [1.735 sec/step, loss=0.80873, avg_loss=0.77871]\n",
            "Step 25277   [1.733 sec/step, loss=0.74739, avg_loss=0.77835]\n",
            "Step 25278   [1.730 sec/step, loss=0.71804, avg_loss=0.77830]\n",
            "Step 25279   [1.722 sec/step, loss=0.75204, avg_loss=0.77789]\n",
            "Step 25280   [1.726 sec/step, loss=0.79169, avg_loss=0.77816]\n",
            "Step 25281   [1.733 sec/step, loss=0.77479, avg_loss=0.77825]\n",
            "Step 25282   [1.737 sec/step, loss=0.77749, avg_loss=0.77843]\n",
            "Step 25283   [1.745 sec/step, loss=0.72930, avg_loss=0.77751]\n",
            "Step 25284   [1.749 sec/step, loss=0.74302, avg_loss=0.77725]\n",
            "Step 25285   [1.728 sec/step, loss=0.83523, avg_loss=0.77803]\n",
            "Step 25286   [1.726 sec/step, loss=0.82071, avg_loss=0.77911]\n",
            "Step 25287   [1.686 sec/step, loss=0.78815, avg_loss=0.77918]\n",
            "Step 25288   [1.685 sec/step, loss=0.77525, avg_loss=0.77921]\n",
            "Step 25289   [1.683 sec/step, loss=0.78614, avg_loss=0.77903]\n",
            "Step 25290   [1.714 sec/step, loss=0.70857, avg_loss=0.77820]\n",
            "Step 25291   [1.701 sec/step, loss=0.77931, avg_loss=0.77789]\n",
            "Step 25292   [1.702 sec/step, loss=0.74802, avg_loss=0.77762]\n",
            "Step 25293   [1.696 sec/step, loss=0.79807, avg_loss=0.77806]\n",
            "Step 25294   [1.707 sec/step, loss=0.72142, avg_loss=0.77724]\n",
            "Step 25295   [1.706 sec/step, loss=0.74207, avg_loss=0.77706]\n",
            "Step 25296   [1.694 sec/step, loss=0.79425, avg_loss=0.77746]\n",
            "Step 25297   [1.691 sec/step, loss=0.80799, avg_loss=0.77742]\n",
            "Step 25298   [1.691 sec/step, loss=0.75385, avg_loss=0.77747]\n",
            "Step 25299   [1.688 sec/step, loss=0.77343, avg_loss=0.77721]\n",
            "Step 25300   [1.688 sec/step, loss=0.79202, avg_loss=0.77688]\n",
            "Writing summary at step: 25300\n",
            "Step 25301   [1.693 sec/step, loss=0.77026, avg_loss=0.77705]\n",
            "Generated 32 batches of size 8 in 10.211 sec\n",
            "Step 25302   [1.712 sec/step, loss=0.84994, avg_loss=0.77751]\n",
            "Step 25303   [1.713 sec/step, loss=0.75773, avg_loss=0.77772]\n",
            "Step 25304   [1.724 sec/step, loss=0.76887, avg_loss=0.77761]\n",
            "Step 25305   [1.726 sec/step, loss=0.77788, avg_loss=0.77769]\n",
            "Step 25306   [1.726 sec/step, loss=0.73224, avg_loss=0.77721]\n",
            "Step 25307   [1.728 sec/step, loss=0.77441, avg_loss=0.77780]\n",
            "Step 25308   [1.725 sec/step, loss=0.77040, avg_loss=0.77728]\n",
            "Step 25309   [1.707 sec/step, loss=0.73316, avg_loss=0.77661]\n",
            "Step 25310   [1.727 sec/step, loss=0.69164, avg_loss=0.77533]\n",
            "Step 25311   [1.727 sec/step, loss=0.74665, avg_loss=0.77521]\n",
            "Step 25312   [1.718 sec/step, loss=0.76818, avg_loss=0.77543]\n",
            "Step 25313   [1.711 sec/step, loss=0.74857, avg_loss=0.77468]\n",
            "Step 25314   [1.699 sec/step, loss=0.77351, avg_loss=0.77449]\n",
            "Step 25315   [1.701 sec/step, loss=0.76710, avg_loss=0.77442]\n",
            "Step 25316   [1.674 sec/step, loss=0.78767, avg_loss=0.77486]\n",
            "Step 25317   [1.669 sec/step, loss=0.81014, avg_loss=0.77505]\n",
            "Step 25318   [1.665 sec/step, loss=0.81619, avg_loss=0.77513]\n",
            "Step 25319   [1.664 sec/step, loss=0.78941, avg_loss=0.77526]\n",
            "Step 25320   [1.663 sec/step, loss=0.69298, avg_loss=0.77422]\n",
            "Step 25321   [1.658 sec/step, loss=0.73916, avg_loss=0.77387]\n",
            "Step 25322   [1.663 sec/step, loss=0.72626, avg_loss=0.77317]\n",
            "Step 25323   [1.665 sec/step, loss=0.79986, avg_loss=0.77335]\n",
            "Step 25324   [1.668 sec/step, loss=0.75989, avg_loss=0.77283]\n",
            "Step 25325   [1.675 sec/step, loss=0.76543, avg_loss=0.77297]\n",
            "Step 25326   [1.679 sec/step, loss=0.82265, avg_loss=0.77381]\n",
            "Step 25327   [1.682 sec/step, loss=0.74233, avg_loss=0.77345]\n",
            "Step 25328   [1.685 sec/step, loss=0.74981, avg_loss=0.77260]\n",
            "Step 25329   [1.689 sec/step, loss=0.79270, avg_loss=0.77263]\n",
            "Step 25330   [1.686 sec/step, loss=0.78045, avg_loss=0.77233]\n",
            "Step 25331   [1.694 sec/step, loss=0.74708, avg_loss=0.77236]\n",
            "Step 25332   [1.676 sec/step, loss=0.79553, avg_loss=0.77253]\n",
            "Step 25333   [1.676 sec/step, loss=0.75316, avg_loss=0.77260]\n",
            "Generated 32 batches of size 8 in 10.132 sec\n",
            "Step 25334   [1.679 sec/step, loss=0.73655, avg_loss=0.77211]\n",
            "Step 25335   [1.715 sec/step, loss=0.73004, avg_loss=0.77191]\n",
            "Step 25336   [1.718 sec/step, loss=0.82809, avg_loss=0.77229]\n",
            "Step 25337   [1.716 sec/step, loss=0.74756, avg_loss=0.77198]\n",
            "Step 25338   [1.706 sec/step, loss=0.78272, avg_loss=0.77147]\n",
            "Step 25339   [1.696 sec/step, loss=0.79683, avg_loss=0.77144]\n",
            "Step 25340   [1.734 sec/step, loss=0.70573, avg_loss=0.77026]\n",
            "Step 25341   [1.733 sec/step, loss=0.71658, avg_loss=0.76987]\n",
            "Step 25342   [1.728 sec/step, loss=0.77500, avg_loss=0.76982]\n",
            "Step 25343   [1.748 sec/step, loss=0.80283, avg_loss=0.77037]\n",
            "Step 25344   [1.748 sec/step, loss=0.80900, avg_loss=0.77004]\n",
            "Step 25345   [1.751 sec/step, loss=0.75689, avg_loss=0.76967]\n",
            "Step 25346   [1.751 sec/step, loss=0.78956, avg_loss=0.76940]\n",
            "Step 25347   [1.756 sec/step, loss=0.78765, avg_loss=0.76967]\n",
            "Step 25348   [1.716 sec/step, loss=0.83098, avg_loss=0.77094]\n",
            "Step 25349   [1.725 sec/step, loss=0.79006, avg_loss=0.77105]\n",
            "Step 25350   [1.722 sec/step, loss=0.76558, avg_loss=0.77064]\n",
            "Step 25351   [1.708 sec/step, loss=0.81445, avg_loss=0.77119]\n",
            "Step 25352   [1.710 sec/step, loss=0.79636, avg_loss=0.77121]\n",
            "Step 25353   [1.705 sec/step, loss=0.72503, avg_loss=0.77090]\n",
            "Step 25354   [1.704 sec/step, loss=0.78800, avg_loss=0.77128]\n",
            "Step 25355   [1.705 sec/step, loss=0.77922, avg_loss=0.77120]\n",
            "Step 25356   [1.707 sec/step, loss=0.76693, avg_loss=0.77087]\n",
            "Step 25357   [1.702 sec/step, loss=0.79833, avg_loss=0.77092]\n",
            "Step 25358   [1.712 sec/step, loss=0.75147, avg_loss=0.77061]\n",
            "Step 25359   [1.718 sec/step, loss=0.80038, avg_loss=0.77040]\n",
            "Step 25360   [1.725 sec/step, loss=0.73884, avg_loss=0.77053]\n",
            "Step 25361   [1.726 sec/step, loss=0.73859, avg_loss=0.77029]\n",
            "Step 25362   [1.722 sec/step, loss=0.82031, avg_loss=0.77041]\n",
            "Step 25363   [1.703 sec/step, loss=0.74650, avg_loss=0.77027]\n",
            "Step 25364   [1.707 sec/step, loss=0.78047, avg_loss=0.77037]\n",
            "Step 25365   [1.705 sec/step, loss=0.76974, avg_loss=0.77052]\n",
            "Generated 32 batches of size 8 in 7.619 sec\n",
            "Step 25366   [1.684 sec/step, loss=0.80102, avg_loss=0.77071]\n",
            "Step 25367   [1.679 sec/step, loss=0.75625, avg_loss=0.77087]\n",
            "Step 25368   [1.669 sec/step, loss=0.79339, avg_loss=0.77103]\n",
            "Step 25369   [1.668 sec/step, loss=0.76763, avg_loss=0.77094]\n",
            "Step 25370   [1.663 sec/step, loss=0.77354, avg_loss=0.77076]\n",
            "Step 25371   [1.656 sec/step, loss=0.79742, avg_loss=0.77099]\n",
            "Step 25372   [1.658 sec/step, loss=0.84624, avg_loss=0.77166]\n",
            "Step 25373   [1.662 sec/step, loss=0.78427, avg_loss=0.77195]\n",
            "Step 25374   [1.669 sec/step, loss=0.81127, avg_loss=0.77258]\n",
            "Step 25375   [1.668 sec/step, loss=0.78243, avg_loss=0.77248]\n",
            "Step 25376   [1.657 sec/step, loss=0.79431, avg_loss=0.77234]\n",
            "Step 25377   [1.660 sec/step, loss=0.75259, avg_loss=0.77239]\n",
            "Step 25378   [1.650 sec/step, loss=0.82450, avg_loss=0.77345]\n",
            "Step 25379   [1.657 sec/step, loss=0.76562, avg_loss=0.77359]\n",
            "Step 25380   [1.657 sec/step, loss=0.71555, avg_loss=0.77283]\n",
            "Step 25381   [1.657 sec/step, loss=0.80266, avg_loss=0.77310]\n",
            "Step 25382   [1.648 sec/step, loss=0.78753, avg_loss=0.77321]\n",
            "Step 25383   [1.640 sec/step, loss=0.80093, avg_loss=0.77392]\n",
            "Step 25384   [1.648 sec/step, loss=0.78615, avg_loss=0.77435]\n",
            "Step 25385   [1.643 sec/step, loss=0.81117, avg_loss=0.77411]\n",
            "Step 25386   [1.642 sec/step, loss=0.82531, avg_loss=0.77416]\n",
            "Step 25387   [1.645 sec/step, loss=0.81287, avg_loss=0.77441]\n",
            "Step 25388   [1.645 sec/step, loss=0.78906, avg_loss=0.77454]\n",
            "Step 25389   [1.643 sec/step, loss=0.75399, avg_loss=0.77422]\n",
            "Step 25390   [1.613 sec/step, loss=0.80136, avg_loss=0.77515]\n",
            "Step 25391   [1.615 sec/step, loss=0.76646, avg_loss=0.77502]\n",
            "Step 25392   [1.619 sec/step, loss=0.71084, avg_loss=0.77465]\n",
            "Step 25393   [1.631 sec/step, loss=0.80114, avg_loss=0.77468]\n",
            "Step 25394   [1.634 sec/step, loss=0.77034, avg_loss=0.77517]\n",
            "Step 25395   [1.634 sec/step, loss=0.76341, avg_loss=0.77538]\n",
            "Generated 32 batches of size 8 in 7.143 sec\n",
            "Step 25396   [1.677 sec/step, loss=0.60021, avg_loss=0.77344]\n",
            "Step 25397   [1.675 sec/step, loss=0.69843, avg_loss=0.77235]\n",
            "Step 25398   [1.690 sec/step, loss=0.79533, avg_loss=0.77276]\n",
            "Step 25399   [1.688 sec/step, loss=0.79805, avg_loss=0.77301]\n",
            "Step 25400   [1.698 sec/step, loss=0.78038, avg_loss=0.77289]\n",
            "Writing summary at step: 25400\n",
            "Step 25401   [1.695 sec/step, loss=0.74118, avg_loss=0.77260]\n",
            "Step 25402   [1.670 sec/step, loss=0.75148, avg_loss=0.77162]\n",
            "Step 25403   [1.666 sec/step, loss=0.80578, avg_loss=0.77210]\n",
            "Step 25404   [1.659 sec/step, loss=0.78072, avg_loss=0.77222]\n",
            "Step 25405   [1.657 sec/step, loss=0.73098, avg_loss=0.77175]\n",
            "Step 25406   [1.657 sec/step, loss=0.76300, avg_loss=0.77205]\n",
            "Step 25407   [1.652 sec/step, loss=0.72836, avg_loss=0.77159]\n",
            "Step 25408   [1.695 sec/step, loss=0.64733, avg_loss=0.77036]\n",
            "Step 25409   [1.697 sec/step, loss=0.76041, avg_loss=0.77063]\n",
            "Step 25410   [1.687 sec/step, loss=0.80135, avg_loss=0.77173]\n",
            "Step 25411   [1.683 sec/step, loss=0.77757, avg_loss=0.77204]\n",
            "Step 25412   [1.680 sec/step, loss=0.76812, avg_loss=0.77204]\n",
            "Step 25413   [1.682 sec/step, loss=0.79960, avg_loss=0.77255]\n",
            "Step 25414   [1.689 sec/step, loss=0.81207, avg_loss=0.77294]\n",
            "Step 25415   [1.688 sec/step, loss=0.75584, avg_loss=0.77282]\n",
            "Step 25416   [1.673 sec/step, loss=0.75083, avg_loss=0.77246]\n",
            "Step 25417   [1.667 sec/step, loss=0.77215, avg_loss=0.77208]\n",
            "Step 25418   [1.671 sec/step, loss=0.74500, avg_loss=0.77136]\n",
            "Step 25419   [1.685 sec/step, loss=0.82227, avg_loss=0.77169]\n",
            "Step 25420   [1.694 sec/step, loss=0.68598, avg_loss=0.77162]\n",
            "Step 25421   [1.695 sec/step, loss=0.78060, avg_loss=0.77204]\n",
            "Step 25422   [1.709 sec/step, loss=0.80693, avg_loss=0.77284]\n",
            "Step 25423   [1.711 sec/step, loss=0.82567, avg_loss=0.77310]\n",
            "Step 25424   [1.712 sec/step, loss=0.82641, avg_loss=0.77377]\n",
            "Step 25425   [1.721 sec/step, loss=0.80573, avg_loss=0.77417]\n",
            "Step 25426   [1.716 sec/step, loss=0.81535, avg_loss=0.77410]\n",
            "Step 25427   [1.715 sec/step, loss=0.81489, avg_loss=0.77482]\n",
            "Generated 32 batches of size 8 in 8.236 sec\n",
            "Step 25428   [1.715 sec/step, loss=0.78991, avg_loss=0.77522]\n",
            "Step 25429   [1.714 sec/step, loss=0.75598, avg_loss=0.77486]\n",
            "Step 25430   [1.711 sec/step, loss=0.77785, avg_loss=0.77483]\n",
            "Step 25431   [1.695 sec/step, loss=0.80928, avg_loss=0.77545]\n",
            "Step 25432   [1.695 sec/step, loss=0.77169, avg_loss=0.77521]\n",
            "Step 25433   [1.690 sec/step, loss=0.80852, avg_loss=0.77577]\n",
            "Step 25434   [1.678 sec/step, loss=0.78106, avg_loss=0.77621]\n",
            "Step 25435   [1.639 sec/step, loss=0.80098, avg_loss=0.77692]\n",
            "Step 25436   [1.648 sec/step, loss=0.83253, avg_loss=0.77697]\n",
            "Step 25437   [1.659 sec/step, loss=0.75401, avg_loss=0.77703]\n",
            "Step 25438   [1.652 sec/step, loss=0.77838, avg_loss=0.77699]\n",
            "Step 25439   [1.662 sec/step, loss=0.83446, avg_loss=0.77736]\n",
            "Step 25440   [1.626 sec/step, loss=0.74643, avg_loss=0.77777]\n",
            "Step 25441   [1.635 sec/step, loss=0.79009, avg_loss=0.77851]\n",
            "Step 25442   [1.633 sec/step, loss=0.78833, avg_loss=0.77864]\n",
            "Step 25443   [1.609 sec/step, loss=0.79162, avg_loss=0.77853]\n",
            "Step 25444   [1.614 sec/step, loss=0.80866, avg_loss=0.77852]\n",
            "Step 25445   [1.638 sec/step, loss=0.77228, avg_loss=0.77868]\n",
            "Step 25446   [1.639 sec/step, loss=0.78094, avg_loss=0.77859]\n",
            "Step 25447   [1.631 sec/step, loss=0.75170, avg_loss=0.77823]\n",
            "Step 25448   [1.630 sec/step, loss=0.73657, avg_loss=0.77729]\n",
            "Step 25449   [1.623 sec/step, loss=0.79458, avg_loss=0.77733]\n",
            "Step 25450   [1.635 sec/step, loss=0.78023, avg_loss=0.77748]\n",
            "Step 25451   [1.631 sec/step, loss=0.79410, avg_loss=0.77728]\n",
            "Step 25452   [1.634 sec/step, loss=0.83701, avg_loss=0.77768]\n",
            "Step 25453   [1.636 sec/step, loss=0.80943, avg_loss=0.77853]\n",
            "Step 25454   [1.636 sec/step, loss=0.79928, avg_loss=0.77864]\n",
            "Step 25455   [1.634 sec/step, loss=0.81212, avg_loss=0.77897]\n",
            "Step 25456   [1.637 sec/step, loss=0.80001, avg_loss=0.77930]\n",
            "Step 25457   [1.646 sec/step, loss=0.74718, avg_loss=0.77879]\n",
            "Step 25458   [1.645 sec/step, loss=0.83049, avg_loss=0.77958]\n",
            "Generated 32 batches of size 8 in 7.952 sec\n",
            "Step 25459   [1.663 sec/step, loss=0.81106, avg_loss=0.77968]\n",
            "Step 25460   [1.657 sec/step, loss=0.75994, avg_loss=0.77990]\n",
            "Step 25461   [1.662 sec/step, loss=0.79159, avg_loss=0.78043]\n",
            "Step 25462   [1.658 sec/step, loss=0.76285, avg_loss=0.77985]\n",
            "Step 25463   [1.657 sec/step, loss=0.77326, avg_loss=0.78012]\n",
            "Step 25464   [1.658 sec/step, loss=0.77468, avg_loss=0.78006]\n",
            "Step 25465   [1.659 sec/step, loss=0.77161, avg_loss=0.78008]\n",
            "Step 25466   [1.655 sec/step, loss=0.80265, avg_loss=0.78010]\n",
            "Step 25467   [1.650 sec/step, loss=0.78913, avg_loss=0.78042]\n",
            "Step 25468   [1.649 sec/step, loss=0.73515, avg_loss=0.77984]\n",
            "Step 25469   [1.651 sec/step, loss=0.81736, avg_loss=0.78034]\n",
            "Step 25470   [1.689 sec/step, loss=0.71049, avg_loss=0.77971]\n",
            "Step 25471   [1.692 sec/step, loss=0.78337, avg_loss=0.77957]\n",
            "Step 25472   [1.697 sec/step, loss=0.78098, avg_loss=0.77892]\n",
            "Step 25473   [1.711 sec/step, loss=0.73907, avg_loss=0.77846]\n",
            "Step 25474   [1.714 sec/step, loss=0.76902, avg_loss=0.77804]\n",
            "Step 25475   [1.712 sec/step, loss=0.82333, avg_loss=0.77845]\n",
            "Step 25476   [1.712 sec/step, loss=0.75448, avg_loss=0.77805]\n",
            "Step 25477   [1.713 sec/step, loss=0.78279, avg_loss=0.77835]\n",
            "Step 25478   [1.715 sec/step, loss=0.82615, avg_loss=0.77837]\n",
            "Step 25479   [1.739 sec/step, loss=0.81179, avg_loss=0.77883]\n",
            "Step 25480   [1.734 sec/step, loss=0.80047, avg_loss=0.77968]\n",
            "Step 25481   [1.730 sec/step, loss=0.76670, avg_loss=0.77932]\n",
            "Step 25482   [1.734 sec/step, loss=0.79480, avg_loss=0.77939]\n",
            "Step 25483   [1.742 sec/step, loss=0.80162, avg_loss=0.77940]\n",
            "Step 25484   [1.731 sec/step, loss=0.80975, avg_loss=0.77964]\n",
            "Step 25485   [1.731 sec/step, loss=0.73909, avg_loss=0.77892]\n",
            "Step 25486   [1.740 sec/step, loss=0.84264, avg_loss=0.77909]\n",
            "Step 25487   [1.738 sec/step, loss=0.76310, avg_loss=0.77859]\n",
            "Step 25488   [1.743 sec/step, loss=0.81964, avg_loss=0.77890]\n",
            "Step 25489   [1.742 sec/step, loss=0.83626, avg_loss=0.77972]\n",
            "Step 25490   [1.736 sec/step, loss=0.75898, avg_loss=0.77930]\n",
            "Step 25491   [1.739 sec/step, loss=0.78694, avg_loss=0.77950]\n",
            "Step 25492   [1.740 sec/step, loss=0.76184, avg_loss=0.78001]\n",
            "Generated 32 batches of size 8 in 8.636 sec\n",
            "Step 25493   [1.734 sec/step, loss=0.75029, avg_loss=0.77950]\n",
            "Step 25494   [1.718 sec/step, loss=0.80334, avg_loss=0.77983]\n",
            "Step 25495   [1.715 sec/step, loss=0.78956, avg_loss=0.78009]\n",
            "Step 25496   [1.674 sec/step, loss=0.80140, avg_loss=0.78211]\n",
            "Step 25497   [1.677 sec/step, loss=0.76265, avg_loss=0.78275]\n",
            "Step 25498   [1.661 sec/step, loss=0.76577, avg_loss=0.78245]\n",
            "Step 25499   [1.664 sec/step, loss=0.70902, avg_loss=0.78156]\n",
            "Step 25500   [1.656 sec/step, loss=0.83553, avg_loss=0.78211]\n",
            "Writing summary at step: 25500\n",
            "Step 25501   [1.657 sec/step, loss=0.79065, avg_loss=0.78261]\n",
            "Step 25502   [1.654 sec/step, loss=0.80715, avg_loss=0.78317]\n",
            "Step 25503   [1.653 sec/step, loss=0.78426, avg_loss=0.78295]\n",
            "Step 25504   [1.655 sec/step, loss=0.78119, avg_loss=0.78296]\n",
            "Step 25505   [1.666 sec/step, loss=0.80481, avg_loss=0.78369]\n",
            "Step 25506   [1.671 sec/step, loss=0.76093, avg_loss=0.78367]\n",
            "Step 25507   [1.686 sec/step, loss=0.80396, avg_loss=0.78443]\n",
            "Step 25508   [1.650 sec/step, loss=0.80428, avg_loss=0.78600]\n",
            "Step 25509   [1.650 sec/step, loss=0.79886, avg_loss=0.78638]\n",
            "Step 25510   [1.641 sec/step, loss=0.82165, avg_loss=0.78659]\n",
            "Step 25511   [1.681 sec/step, loss=0.67615, avg_loss=0.78557]\n",
            "Step 25512   [1.688 sec/step, loss=0.74989, avg_loss=0.78539]\n",
            "Step 25513   [1.688 sec/step, loss=0.77025, avg_loss=0.78510]\n",
            "Step 25514   [1.686 sec/step, loss=0.70968, avg_loss=0.78407]\n",
            "Step 25515   [1.687 sec/step, loss=0.80596, avg_loss=0.78457]\n",
            "Step 25516   [1.688 sec/step, loss=0.80627, avg_loss=0.78513]\n",
            "Step 25517   [1.682 sec/step, loss=0.79695, avg_loss=0.78538]\n",
            "Step 25518   [1.697 sec/step, loss=0.78743, avg_loss=0.78580]\n",
            "Step 25519   [1.689 sec/step, loss=0.79170, avg_loss=0.78549]\n",
            "Step 25520   [1.677 sec/step, loss=0.80006, avg_loss=0.78664]\n",
            "Step 25521   [1.683 sec/step, loss=0.75531, avg_loss=0.78638]\n",
            "Step 25522   [1.670 sec/step, loss=0.76552, avg_loss=0.78597]\n",
            "Step 25523   [1.678 sec/step, loss=0.71402, avg_loss=0.78485]\n",
            "Generated 32 batches of size 8 in 9.229 sec\n",
            "Step 25524   [1.681 sec/step, loss=0.79526, avg_loss=0.78454]\n",
            "Step 25525   [1.666 sec/step, loss=0.79928, avg_loss=0.78448]\n",
            "Step 25526   [1.661 sec/step, loss=0.73831, avg_loss=0.78371]\n",
            "Step 25527   [1.665 sec/step, loss=0.77139, avg_loss=0.78327]\n",
            "Step 25528   [1.662 sec/step, loss=0.79725, avg_loss=0.78334]\n",
            "Step 25529   [1.665 sec/step, loss=0.75442, avg_loss=0.78333]\n",
            "Step 25530   [1.700 sec/step, loss=0.77414, avg_loss=0.78329]\n",
            "Step 25531   [1.712 sec/step, loss=0.77106, avg_loss=0.78291]\n",
            "Step 25532   [1.709 sec/step, loss=0.78326, avg_loss=0.78302]\n",
            "Step 25533   [1.713 sec/step, loss=0.81289, avg_loss=0.78307]\n",
            "Step 25534   [1.721 sec/step, loss=0.83807, avg_loss=0.78364]\n",
            "Step 25535   [1.719 sec/step, loss=0.75282, avg_loss=0.78316]\n",
            "Step 25536   [1.722 sec/step, loss=0.72816, avg_loss=0.78211]\n",
            "Step 25537   [1.713 sec/step, loss=0.78291, avg_loss=0.78240]\n",
            "Step 25538   [1.711 sec/step, loss=0.80561, avg_loss=0.78267]\n",
            "Step 25539   [1.706 sec/step, loss=0.75979, avg_loss=0.78193]\n",
            "Step 25540   [1.709 sec/step, loss=0.76887, avg_loss=0.78215]\n",
            "Step 25541   [1.698 sec/step, loss=0.78932, avg_loss=0.78214]\n",
            "Step 25542   [1.696 sec/step, loss=0.76286, avg_loss=0.78189]\n",
            "Step 25543   [1.708 sec/step, loss=0.80646, avg_loss=0.78204]\n",
            "Step 25544   [1.703 sec/step, loss=0.78834, avg_loss=0.78183]\n",
            "Step 25545   [1.673 sec/step, loss=0.79929, avg_loss=0.78210]\n",
            "Step 25546   [1.673 sec/step, loss=0.85074, avg_loss=0.78280]\n",
            "Step 25547   [1.665 sec/step, loss=0.77051, avg_loss=0.78299]\n",
            "Step 25548   [1.667 sec/step, loss=0.78415, avg_loss=0.78347]\n",
            "Step 25549   [1.678 sec/step, loss=0.79989, avg_loss=0.78352]\n",
            "Step 25550   [1.663 sec/step, loss=0.82452, avg_loss=0.78396]\n",
            "Step 25551   [1.696 sec/step, loss=0.85313, avg_loss=0.78455]\n",
            "Step 25552   [1.689 sec/step, loss=0.74555, avg_loss=0.78364]\n",
            "Step 25553   [1.690 sec/step, loss=0.73622, avg_loss=0.78291]\n",
            "Step 25554   [1.692 sec/step, loss=0.75045, avg_loss=0.78242]\n",
            "Generated 32 batches of size 8 in 9.516 sec\n",
            "Step 25555   [1.693 sec/step, loss=0.74693, avg_loss=0.78177]\n",
            "Step 25556   [1.695 sec/step, loss=0.78424, avg_loss=0.78161]\n",
            "Step 25557   [1.689 sec/step, loss=0.76896, avg_loss=0.78183]\n",
            "Step 25558   [1.682 sec/step, loss=0.79742, avg_loss=0.78150]\n",
            "Step 25559   [1.664 sec/step, loss=0.70634, avg_loss=0.78045]\n",
            "Step 25560   [1.662 sec/step, loss=0.72874, avg_loss=0.78014]\n",
            "Step 25561   [1.656 sec/step, loss=0.78047, avg_loss=0.78002]\n",
            "Step 25562   [1.657 sec/step, loss=0.77224, avg_loss=0.78012]\n",
            "Step 25563   [1.674 sec/step, loss=0.75015, avg_loss=0.77989]\n",
            "Step 25564   [1.672 sec/step, loss=0.78857, avg_loss=0.78003]\n",
            "Step 25565   [1.673 sec/step, loss=0.76281, avg_loss=0.77994]\n",
            "Step 25566   [1.688 sec/step, loss=0.78881, avg_loss=0.77980]\n",
            "Step 25567   [1.688 sec/step, loss=0.74236, avg_loss=0.77933]\n",
            "Step 25568   [1.685 sec/step, loss=0.80568, avg_loss=0.78004]\n",
            "Step 25569   [1.691 sec/step, loss=0.71695, avg_loss=0.77903]\n",
            "Step 25570   [1.647 sec/step, loss=0.72382, avg_loss=0.77917]\n",
            "Step 25571   [1.646 sec/step, loss=0.77607, avg_loss=0.77909]\n",
            "Step 25572   [1.639 sec/step, loss=0.85240, avg_loss=0.77981]\n",
            "Step 25573   [1.620 sec/step, loss=0.77746, avg_loss=0.78019]\n",
            "Step 25574   [1.613 sec/step, loss=0.79573, avg_loss=0.78046]\n",
            "Step 25575   [1.651 sec/step, loss=0.70079, avg_loss=0.77923]\n",
            "Step 25576   [1.651 sec/step, loss=0.79318, avg_loss=0.77962]\n",
            "Step 25577   [1.651 sec/step, loss=0.81011, avg_loss=0.77989]\n",
            "Step 25578   [1.658 sec/step, loss=0.80387, avg_loss=0.77967]\n",
            "Step 25579   [1.628 sec/step, loss=0.79334, avg_loss=0.77949]\n",
            "Step 25580   [1.628 sec/step, loss=0.73327, avg_loss=0.77881]\n",
            "Step 25581   [1.642 sec/step, loss=0.76945, avg_loss=0.77884]\n",
            "Step 25582   [1.641 sec/step, loss=0.72134, avg_loss=0.77811]\n",
            "Step 25583   [1.636 sec/step, loss=0.78485, avg_loss=0.77794]\n",
            "Step 25584   [1.645 sec/step, loss=0.80949, avg_loss=0.77794]\n",
            "Step 25585   [1.650 sec/step, loss=0.73623, avg_loss=0.77791]\n",
            "Step 25586   [1.647 sec/step, loss=0.79097, avg_loss=0.77739]\n",
            "Step 25587   [1.657 sec/step, loss=0.72962, avg_loss=0.77706]\n",
            "Generated 32 batches of size 8 in 10.123 sec\n",
            "Step 25588   [1.658 sec/step, loss=0.74913, avg_loss=0.77635]\n",
            "Step 25589   [1.653 sec/step, loss=0.80530, avg_loss=0.77604]\n",
            "Step 25590   [1.660 sec/step, loss=0.77723, avg_loss=0.77622]\n",
            "Step 25591   [1.666 sec/step, loss=0.74484, avg_loss=0.77580]\n",
            "Step 25592   [1.669 sec/step, loss=0.75468, avg_loss=0.77573]\n",
            "Step 25593   [1.666 sec/step, loss=0.76344, avg_loss=0.77586]\n",
            "Step 25594   [1.660 sec/step, loss=0.71360, avg_loss=0.77497]\n",
            "Step 25595   [1.668 sec/step, loss=0.77386, avg_loss=0.77481]\n",
            "Step 25596   [1.666 sec/step, loss=0.84407, avg_loss=0.77524]\n",
            "Step 25597   [1.687 sec/step, loss=0.72918, avg_loss=0.77490]\n",
            "Step 25598   [1.687 sec/step, loss=0.78353, avg_loss=0.77508]\n",
            "Step 25599   [1.692 sec/step, loss=0.80112, avg_loss=0.77600]\n",
            "Step 25600   [1.686 sec/step, loss=0.76983, avg_loss=0.77534]\n",
            "Writing summary at step: 25600\n",
            "Step 25601   [1.691 sec/step, loss=0.73805, avg_loss=0.77482]\n",
            "Step 25602   [1.693 sec/step, loss=0.78393, avg_loss=0.77458]\n",
            "Step 25603   [1.698 sec/step, loss=0.75012, avg_loss=0.77424]\n",
            "Step 25604   [1.689 sec/step, loss=0.74943, avg_loss=0.77393]\n",
            "Step 25605   [1.687 sec/step, loss=0.85242, avg_loss=0.77440]\n",
            "Step 25606   [1.680 sec/step, loss=0.72793, avg_loss=0.77407]\n",
            "Step 25607   [1.668 sec/step, loss=0.80578, avg_loss=0.77409]\n",
            "Step 25608   [1.660 sec/step, loss=0.78407, avg_loss=0.77389]\n",
            "Step 25609   [1.660 sec/step, loss=0.73911, avg_loss=0.77329]\n",
            "Step 25610   [1.699 sec/step, loss=0.76489, avg_loss=0.77272]\n",
            "Step 25611   [1.658 sec/step, loss=0.81586, avg_loss=0.77412]\n",
            "Step 25612   [1.655 sec/step, loss=0.82720, avg_loss=0.77489]\n",
            "Step 25613   [1.654 sec/step, loss=0.77566, avg_loss=0.77495]\n",
            "Step 25614   [1.657 sec/step, loss=0.77263, avg_loss=0.77558]\n",
            "Step 25615   [1.658 sec/step, loss=0.77474, avg_loss=0.77526]\n",
            "Step 25616   [1.675 sec/step, loss=0.81708, avg_loss=0.77537]\n",
            "Step 25617   [1.681 sec/step, loss=0.76984, avg_loss=0.77510]\n",
            "Step 25618   [1.662 sec/step, loss=0.79738, avg_loss=0.77520]\n",
            "Generated 32 batches of size 8 in 10.107 sec\n",
            "Step 25619   [1.664 sec/step, loss=0.81494, avg_loss=0.77543]\n",
            "Step 25620   [1.665 sec/step, loss=0.83884, avg_loss=0.77582]\n",
            "Step 25621   [1.661 sec/step, loss=0.79034, avg_loss=0.77617]\n",
            "Step 25622   [1.667 sec/step, loss=0.83210, avg_loss=0.77684]\n",
            "Step 25623   [1.660 sec/step, loss=0.76139, avg_loss=0.77731]\n",
            "Step 25624   [1.671 sec/step, loss=0.78929, avg_loss=0.77725]\n",
            "Step 25625   [1.679 sec/step, loss=0.77231, avg_loss=0.77698]\n",
            "Step 25626   [1.680 sec/step, loss=0.77537, avg_loss=0.77735]\n",
            "Step 25627   [1.675 sec/step, loss=0.75986, avg_loss=0.77724]\n",
            "Step 25628   [1.672 sec/step, loss=0.76830, avg_loss=0.77695]\n",
            "Step 25629   [1.680 sec/step, loss=0.82838, avg_loss=0.77769]\n",
            "Step 25630   [1.645 sec/step, loss=0.80570, avg_loss=0.77800]\n",
            "Step 25631   [1.636 sec/step, loss=0.76858, avg_loss=0.77798]\n",
            "Step 25632   [1.637 sec/step, loss=0.79406, avg_loss=0.77809]\n",
            "Step 25633   [1.630 sec/step, loss=0.77448, avg_loss=0.77770]\n",
            "Step 25634   [1.629 sec/step, loss=0.79848, avg_loss=0.77731]\n",
            "Step 25635   [1.628 sec/step, loss=0.80219, avg_loss=0.77780]\n",
            "Step 25636   [1.613 sec/step, loss=0.78323, avg_loss=0.77835]\n",
            "Step 25637   [1.612 sec/step, loss=0.78145, avg_loss=0.77834]\n",
            "Step 25638   [1.611 sec/step, loss=0.79095, avg_loss=0.77819]\n",
            "Step 25639   [1.645 sec/step, loss=0.76215, avg_loss=0.77821]\n",
            "Step 25640   [1.643 sec/step, loss=0.74002, avg_loss=0.77792]\n",
            "Step 25641   [1.662 sec/step, loss=0.76172, avg_loss=0.77765]\n",
            "Step 25642   [1.669 sec/step, loss=0.78563, avg_loss=0.77788]\n",
            "Step 25643   [1.663 sec/step, loss=0.74553, avg_loss=0.77727]\n",
            "Step 25644   [1.663 sec/step, loss=0.78667, avg_loss=0.77725]\n",
            "Step 25645   [1.675 sec/step, loss=0.80550, avg_loss=0.77731]\n",
            "Step 25646   [1.673 sec/step, loss=0.78384, avg_loss=0.77664]\n",
            "Step 25647   [1.677 sec/step, loss=0.79625, avg_loss=0.77690]\n",
            "Step 25648   [1.675 sec/step, loss=0.77410, avg_loss=0.77680]\n",
            "Step 25649   [1.666 sec/step, loss=0.78961, avg_loss=0.77670]\n",
            "Step 25650   [1.668 sec/step, loss=0.79757, avg_loss=0.77643]\n",
            "Step 25651   [1.640 sec/step, loss=0.75060, avg_loss=0.77540]\n",
            "Generated 32 batches of size 8 in 9.925 sec\n",
            "Step 25652   [1.654 sec/step, loss=0.73139, avg_loss=0.77526]\n",
            "Step 25653   [1.645 sec/step, loss=0.72792, avg_loss=0.77518]\n",
            "Step 25654   [1.644 sec/step, loss=0.76673, avg_loss=0.77534]\n",
            "Step 25655   [1.643 sec/step, loss=0.81591, avg_loss=0.77603]\n",
            "Step 25656   [1.645 sec/step, loss=0.80706, avg_loss=0.77626]\n",
            "Step 25657   [1.647 sec/step, loss=0.79766, avg_loss=0.77655]\n",
            "Step 25658   [1.653 sec/step, loss=0.74029, avg_loss=0.77597]\n",
            "Step 25659   [1.644 sec/step, loss=0.74753, avg_loss=0.77639]\n",
            "Step 25660   [1.648 sec/step, loss=0.70026, avg_loss=0.77610]\n",
            "Step 25661   [1.650 sec/step, loss=0.78509, avg_loss=0.77615]\n",
            "Step 25662   [1.693 sec/step, loss=0.67755, avg_loss=0.77520]\n",
            "Step 25663   [1.677 sec/step, loss=0.78805, avg_loss=0.77558]\n",
            "Step 25664   [1.675 sec/step, loss=0.79974, avg_loss=0.77569]\n",
            "Step 25665   [1.686 sec/step, loss=0.77392, avg_loss=0.77580]\n",
            "Step 25666   [1.670 sec/step, loss=0.82075, avg_loss=0.77612]\n",
            "Step 25667   [1.670 sec/step, loss=0.78778, avg_loss=0.77658]\n",
            "Step 25668   [1.667 sec/step, loss=0.78828, avg_loss=0.77640]\n",
            "Step 25669   [1.657 sec/step, loss=0.74919, avg_loss=0.77672]\n",
            "Step 25670   [1.659 sec/step, loss=0.77978, avg_loss=0.77728]\n",
            "Step 25671   [1.658 sec/step, loss=0.76871, avg_loss=0.77721]\n",
            "Step 25672   [1.657 sec/step, loss=0.80896, avg_loss=0.77678]\n",
            "Step 25673   [1.661 sec/step, loss=0.74447, avg_loss=0.77645]\n",
            "Step 25674   [1.674 sec/step, loss=0.74124, avg_loss=0.77590]\n",
            "Step 25675   [1.641 sec/step, loss=0.74252, avg_loss=0.77632]\n",
            "Step 25676   [1.644 sec/step, loss=0.78865, avg_loss=0.77627]\n",
            "Step 25677   [1.641 sec/step, loss=0.78049, avg_loss=0.77598]\n",
            "Step 25678   [1.641 sec/step, loss=0.67147, avg_loss=0.77465]\n",
            "Step 25679   [1.646 sec/step, loss=0.74550, avg_loss=0.77417]\n",
            "Step 25680   [1.647 sec/step, loss=0.81961, avg_loss=0.77504]\n",
            "Step 25681   [1.635 sec/step, loss=0.76714, avg_loss=0.77502]\n",
            "Step 25682   [1.635 sec/step, loss=0.80653, avg_loss=0.77587]\n",
            "Generated 32 batches of size 8 in 10.216 sec\n",
            "Step 25683   [1.656 sec/step, loss=0.72766, avg_loss=0.77530]\n",
            "Step 25684   [1.658 sec/step, loss=0.83339, avg_loss=0.77553]\n",
            "Step 25685   [1.654 sec/step, loss=0.77027, avg_loss=0.77587]\n",
            "Step 25686   [1.654 sec/step, loss=0.72386, avg_loss=0.77520]\n",
            "Step 25687   [1.681 sec/step, loss=0.71737, avg_loss=0.77508]\n",
            "Step 25688   [1.680 sec/step, loss=0.78334, avg_loss=0.77542]\n",
            "Step 25689   [1.692 sec/step, loss=0.75686, avg_loss=0.77494]\n",
            "Step 25690   [1.690 sec/step, loss=0.76402, avg_loss=0.77481]\n",
            "Step 25691   [1.682 sec/step, loss=0.77464, avg_loss=0.77510]\n",
            "Step 25692   [1.675 sec/step, loss=0.76157, avg_loss=0.77517]\n",
            "Step 25693   [1.673 sec/step, loss=0.79135, avg_loss=0.77545]\n",
            "Step 25694   [1.671 sec/step, loss=0.74880, avg_loss=0.77580]\n",
            "Step 25695   [1.674 sec/step, loss=0.76106, avg_loss=0.77568]\n",
            "Step 25696   [1.673 sec/step, loss=0.72817, avg_loss=0.77452]\n",
            "Step 25697   [1.654 sec/step, loss=0.72376, avg_loss=0.77446]\n",
            "Step 25698   [1.658 sec/step, loss=0.76386, avg_loss=0.77427]\n",
            "Step 25699   [1.652 sec/step, loss=0.75827, avg_loss=0.77384]\n",
            "Step 25700   [1.650 sec/step, loss=0.77268, avg_loss=0.77387]\n",
            "Writing summary at step: 25700\n",
            "Step 25701   [1.653 sec/step, loss=0.76921, avg_loss=0.77418]\n",
            "Step 25702   [1.653 sec/step, loss=0.77196, avg_loss=0.77406]\n",
            "Step 25703   [1.651 sec/step, loss=0.79293, avg_loss=0.77449]\n",
            "Step 25704   [1.652 sec/step, loss=0.80100, avg_loss=0.77500]\n",
            "Step 25705   [1.649 sec/step, loss=0.72867, avg_loss=0.77376]\n",
            "Step 25706   [1.660 sec/step, loss=0.67374, avg_loss=0.77322]\n",
            "Step 25707   [1.663 sec/step, loss=0.77370, avg_loss=0.77290]\n",
            "Step 25708   [1.668 sec/step, loss=0.78364, avg_loss=0.77290]\n",
            "Step 25709   [1.668 sec/step, loss=0.77400, avg_loss=0.77325]\n",
            "Step 25710   [1.638 sec/step, loss=0.75118, avg_loss=0.77311]\n",
            "Step 25711   [1.639 sec/step, loss=0.81627, avg_loss=0.77311]\n",
            "Step 25712   [1.637 sec/step, loss=0.77237, avg_loss=0.77257]\n",
            "Step 25713   [1.641 sec/step, loss=0.79870, avg_loss=0.77280]\n",
            "Step 25714   [1.637 sec/step, loss=0.77258, avg_loss=0.77280]\n",
            "Generated 32 batches of size 8 in 10.533 sec\n",
            "Step 25715   [1.658 sec/step, loss=0.75503, avg_loss=0.77260]\n",
            "Step 25716   [1.642 sec/step, loss=0.78131, avg_loss=0.77224]\n",
            "Step 25717   [1.635 sec/step, loss=0.79688, avg_loss=0.77251]\n",
            "Step 25718   [1.646 sec/step, loss=0.78522, avg_loss=0.77239]\n",
            "Step 25719   [1.646 sec/step, loss=0.75950, avg_loss=0.77184]\n",
            "Step 25720   [1.643 sec/step, loss=0.80888, avg_loss=0.77154]\n",
            "Step 25721   [1.646 sec/step, loss=0.78776, avg_loss=0.77151]\n",
            "Step 25722   [1.651 sec/step, loss=0.72057, avg_loss=0.77039]\n",
            "Step 25723   [1.651 sec/step, loss=0.74220, avg_loss=0.77020]\n",
            "Step 25724   [1.635 sec/step, loss=0.80025, avg_loss=0.77031]\n",
            "Step 25725   [1.629 sec/step, loss=0.76002, avg_loss=0.77019]\n",
            "Step 25726   [1.638 sec/step, loss=0.78395, avg_loss=0.77027]\n",
            "Step 25727   [1.634 sec/step, loss=0.82839, avg_loss=0.77096]\n",
            "Step 25728   [1.679 sec/step, loss=0.81661, avg_loss=0.77144]\n",
            "Step 25729   [1.672 sec/step, loss=0.79903, avg_loss=0.77115]\n",
            "Step 25730   [1.668 sec/step, loss=0.79313, avg_loss=0.77102]\n",
            "Step 25731   [1.670 sec/step, loss=0.68467, avg_loss=0.77018]\n",
            "Step 25732   [1.677 sec/step, loss=0.78563, avg_loss=0.77010]\n",
            "Step 25733   [1.678 sec/step, loss=0.78590, avg_loss=0.77021]\n",
            "Step 25734   [1.673 sec/step, loss=0.83128, avg_loss=0.77054]\n",
            "Step 25735   [1.672 sec/step, loss=0.72236, avg_loss=0.76974]\n",
            "Step 25736   [1.696 sec/step, loss=0.70427, avg_loss=0.76896]\n",
            "Step 25737   [1.701 sec/step, loss=0.83692, avg_loss=0.76951]\n",
            "Step 25738   [1.698 sec/step, loss=0.75894, avg_loss=0.76919]\n",
            "Step 25739   [1.662 sec/step, loss=0.75700, avg_loss=0.76914]\n",
            "Step 25740   [1.659 sec/step, loss=0.77189, avg_loss=0.76946]\n",
            "Step 25741   [1.658 sec/step, loss=0.77243, avg_loss=0.76956]\n",
            "Step 25742   [1.657 sec/step, loss=0.79354, avg_loss=0.76964]\n",
            "Step 25743   [1.650 sec/step, loss=0.78712, avg_loss=0.77006]\n",
            "Step 25744   [1.652 sec/step, loss=0.82437, avg_loss=0.77044]\n",
            "Generated 32 batches of size 8 in 8.908 sec\n",
            "Step 25745   [1.654 sec/step, loss=0.79935, avg_loss=0.77037]\n",
            "Step 25746   [1.658 sec/step, loss=0.77096, avg_loss=0.77025]\n",
            "Step 25747   [1.655 sec/step, loss=0.76178, avg_loss=0.76990]\n",
            "Step 25748   [1.656 sec/step, loss=0.77110, avg_loss=0.76987]\n",
            "Step 25749   [1.656 sec/step, loss=0.73128, avg_loss=0.76929]\n",
            "Step 25750   [1.656 sec/step, loss=0.78833, avg_loss=0.76920]\n",
            "Step 25751   [1.666 sec/step, loss=0.81752, avg_loss=0.76986]\n",
            "Step 25752   [1.656 sec/step, loss=0.83166, avg_loss=0.77087]\n",
            "Step 25753   [1.656 sec/step, loss=0.80961, avg_loss=0.77168]\n",
            "Step 25754   [1.651 sec/step, loss=0.77634, avg_loss=0.77178]\n",
            "Step 25755   [1.659 sec/step, loss=0.76591, avg_loss=0.77128]\n",
            "Step 25756   [1.653 sec/step, loss=0.81161, avg_loss=0.77133]\n",
            "Step 25757   [1.646 sec/step, loss=0.72476, avg_loss=0.77060]\n",
            "Step 25758   [1.650 sec/step, loss=0.76010, avg_loss=0.77079]\n",
            "Step 25759   [1.651 sec/step, loss=0.77672, avg_loss=0.77109]\n",
            "Step 25760   [1.652 sec/step, loss=0.73021, avg_loss=0.77139]\n",
            "Step 25761   [1.651 sec/step, loss=0.77676, avg_loss=0.77130]\n",
            "Step 25762   [1.615 sec/step, loss=0.76419, avg_loss=0.77217]\n",
            "Step 25763   [1.628 sec/step, loss=0.75654, avg_loss=0.77185]\n",
            "Step 25764   [1.633 sec/step, loss=0.76687, avg_loss=0.77153]\n",
            "Step 25765   [1.618 sec/step, loss=0.82867, avg_loss=0.77207]\n",
            "Step 25766   [1.621 sec/step, loss=0.79405, avg_loss=0.77181]\n",
            "Step 25767   [1.622 sec/step, loss=0.76971, avg_loss=0.77163]\n",
            "Step 25768   [1.629 sec/step, loss=0.76837, avg_loss=0.77143]\n",
            "Step 25769   [1.638 sec/step, loss=0.77913, avg_loss=0.77173]\n",
            "Step 25770   [1.635 sec/step, loss=0.80105, avg_loss=0.77194]\n",
            "Step 25771   [1.630 sec/step, loss=0.74616, avg_loss=0.77171]\n",
            "Step 25772   [1.638 sec/step, loss=0.76077, avg_loss=0.77123]\n",
            "Step 25773   [1.643 sec/step, loss=0.75211, avg_loss=0.77131]\n",
            "Step 25774   [1.637 sec/step, loss=0.74688, avg_loss=0.77136]\n",
            "Step 25775   [1.638 sec/step, loss=0.71008, avg_loss=0.77104]\n",
            "Generated 32 batches of size 8 in 6.538 sec\n",
            "Step 25776   [1.640 sec/step, loss=0.75237, avg_loss=0.77068]\n",
            "Step 25777   [1.683 sec/step, loss=0.79470, avg_loss=0.77082]\n",
            "Step 25778   [1.680 sec/step, loss=0.77491, avg_loss=0.77185]\n",
            "Step 25779   [1.684 sec/step, loss=0.80207, avg_loss=0.77242]\n",
            "Step 25780   [1.680 sec/step, loss=0.75921, avg_loss=0.77181]\n",
            "Step 25781   [1.702 sec/step, loss=0.72522, avg_loss=0.77140]\n",
            "Step 25782   [1.707 sec/step, loss=0.79430, avg_loss=0.77127]\n",
            "Step 25783   [1.690 sec/step, loss=0.83135, avg_loss=0.77231]\n",
            "Step 25784   [1.685 sec/step, loss=0.75318, avg_loss=0.77151]\n",
            "Step 25785   [1.687 sec/step, loss=0.80698, avg_loss=0.77188]\n",
            "Step 25786   [1.719 sec/step, loss=0.69660, avg_loss=0.77160]\n",
            "Step 25787   [1.692 sec/step, loss=0.77727, avg_loss=0.77220]\n",
            "Step 25788   [1.684 sec/step, loss=0.73907, avg_loss=0.77176]\n",
            "Step 25789   [1.682 sec/step, loss=0.73660, avg_loss=0.77156]\n",
            "Step 25790   [1.678 sec/step, loss=0.74915, avg_loss=0.77141]\n",
            "Step 25791   [1.679 sec/step, loss=0.79512, avg_loss=0.77161]\n",
            "Step 25792   [1.680 sec/step, loss=0.78150, avg_loss=0.77181]\n",
            "Step 25793   [1.691 sec/step, loss=0.73542, avg_loss=0.77125]\n",
            "Step 25794   [1.695 sec/step, loss=0.80671, avg_loss=0.77183]\n",
            "Step 25795   [1.687 sec/step, loss=0.78519, avg_loss=0.77207]\n",
            "Step 25796   [1.686 sec/step, loss=0.79675, avg_loss=0.77276]\n",
            "Step 25797   [1.701 sec/step, loss=0.80685, avg_loss=0.77359]\n",
            "Step 25798   [1.697 sec/step, loss=0.79665, avg_loss=0.77392]\n",
            "Step 25799   [1.695 sec/step, loss=0.74880, avg_loss=0.77382]\n",
            "Step 25800   [1.698 sec/step, loss=0.71941, avg_loss=0.77329]\n",
            "Writing summary at step: 25800\n",
            "Step 25801   [1.695 sec/step, loss=0.78851, avg_loss=0.77348]\n",
            "Step 25802   [1.693 sec/step, loss=0.80368, avg_loss=0.77380]\n",
            "Step 25803   [1.697 sec/step, loss=0.79584, avg_loss=0.77383]\n",
            "Step 25804   [1.702 sec/step, loss=0.74045, avg_loss=0.77322]\n",
            "Step 25805   [1.715 sec/step, loss=0.77548, avg_loss=0.77369]\n",
            "Step 25806   [1.707 sec/step, loss=0.81100, avg_loss=0.77506]\n",
            "Generated 32 batches of size 8 in 7.373 sec\n",
            "Step 25807   [1.707 sec/step, loss=0.77953, avg_loss=0.77512]\n",
            "Step 25808   [1.703 sec/step, loss=0.78212, avg_loss=0.77511]\n",
            "Step 25809   [1.702 sec/step, loss=0.78839, avg_loss=0.77525]\n",
            "Step 25810   [1.695 sec/step, loss=0.80205, avg_loss=0.77576]\n",
            "Step 25811   [1.702 sec/step, loss=0.69686, avg_loss=0.77457]\n",
            "Step 25812   [1.704 sec/step, loss=0.78589, avg_loss=0.77470]\n",
            "Step 25813   [1.700 sec/step, loss=0.75589, avg_loss=0.77427]\n",
            "Step 25814   [1.698 sec/step, loss=0.81699, avg_loss=0.77472]\n",
            "Step 25815   [1.676 sec/step, loss=0.80935, avg_loss=0.77526]\n",
            "Step 25816   [1.680 sec/step, loss=0.72437, avg_loss=0.77469]\n",
            "Step 25817   [1.681 sec/step, loss=0.77815, avg_loss=0.77450]\n",
            "Step 25818   [1.670 sec/step, loss=0.76658, avg_loss=0.77432]\n",
            "Step 25819   [1.667 sec/step, loss=0.78578, avg_loss=0.77458]\n",
            "Step 25820   [1.674 sec/step, loss=0.80997, avg_loss=0.77459]\n",
            "Step 25821   [1.671 sec/step, loss=0.79338, avg_loss=0.77465]\n",
            "Step 25822   [1.660 sec/step, loss=0.79056, avg_loss=0.77535]\n",
            "Step 25823   [1.662 sec/step, loss=0.79283, avg_loss=0.77585]\n",
            "Step 25824   [1.663 sec/step, loss=0.73296, avg_loss=0.77518]\n",
            "Step 25825   [1.665 sec/step, loss=0.81983, avg_loss=0.77578]\n",
            "Step 25826   [1.656 sec/step, loss=0.78421, avg_loss=0.77578]\n",
            "Step 25827   [1.662 sec/step, loss=0.77987, avg_loss=0.77530]\n",
            "Step 25828   [1.634 sec/step, loss=0.79860, avg_loss=0.77512]\n",
            "Step 25829   [1.635 sec/step, loss=0.73103, avg_loss=0.77444]\n",
            "Step 25830   [1.668 sec/step, loss=0.68143, avg_loss=0.77332]\n",
            "Step 25831   [1.664 sec/step, loss=0.74944, avg_loss=0.77397]\n",
            "Step 25832   [1.656 sec/step, loss=0.79297, avg_loss=0.77404]\n",
            "Step 25833   [1.661 sec/step, loss=0.78682, avg_loss=0.77405]\n",
            "Step 25834   [1.663 sec/step, loss=0.73310, avg_loss=0.77307]\n",
            "Step 25835   [1.683 sec/step, loss=0.78054, avg_loss=0.77365]\n",
            "Step 25836   [1.677 sec/step, loss=0.75881, avg_loss=0.77419]\n",
            "Step 25837   [1.671 sec/step, loss=0.77713, avg_loss=0.77360]\n",
            "Step 25838   [1.690 sec/step, loss=0.79516, avg_loss=0.77396]\n",
            "Generated 32 batches of size 8 in 7.336 sec\n",
            "Step 25839   [1.689 sec/step, loss=0.77146, avg_loss=0.77410]\n",
            "Step 25840   [1.693 sec/step, loss=0.82278, avg_loss=0.77461]\n",
            "Step 25841   [1.683 sec/step, loss=0.77642, avg_loss=0.77465]\n",
            "Step 25842   [1.678 sec/step, loss=0.75635, avg_loss=0.77428]\n",
            "Step 25843   [1.676 sec/step, loss=0.76595, avg_loss=0.77407]\n",
            "Step 25844   [1.671 sec/step, loss=0.74929, avg_loss=0.77332]\n",
            "Step 25845   [1.661 sec/step, loss=0.73945, avg_loss=0.77272]\n",
            "Step 25846   [1.688 sec/step, loss=0.78312, avg_loss=0.77284]\n",
            "Step 25847   [1.691 sec/step, loss=0.76135, avg_loss=0.77284]\n",
            "Step 25848   [1.692 sec/step, loss=0.69410, avg_loss=0.77207]\n",
            "Step 25849   [1.694 sec/step, loss=0.71875, avg_loss=0.77194]\n",
            "Step 25850   [1.693 sec/step, loss=0.80684, avg_loss=0.77213]\n",
            "Step 25851   [1.693 sec/step, loss=0.80143, avg_loss=0.77197]\n",
            "Step 25852   [1.703 sec/step, loss=0.71791, avg_loss=0.77083]\n",
            "Step 25853   [1.704 sec/step, loss=0.77165, avg_loss=0.77045]\n",
            "Step 25854   [1.706 sec/step, loss=0.78619, avg_loss=0.77055]\n",
            "Step 25855   [1.704 sec/step, loss=0.78391, avg_loss=0.77073]\n",
            "Step 25856   [1.704 sec/step, loss=0.79994, avg_loss=0.77061]\n",
            "Step 25857   [1.711 sec/step, loss=0.78761, avg_loss=0.77124]\n",
            "Step 25858   [1.704 sec/step, loss=0.75910, avg_loss=0.77123]\n",
            "Step 25859   [1.702 sec/step, loss=0.77178, avg_loss=0.77118]\n",
            "Step 25860   [1.694 sec/step, loss=0.73947, avg_loss=0.77127]\n",
            "Step 25861   [1.692 sec/step, loss=0.76240, avg_loss=0.77113]\n",
            "Step 25862   [1.690 sec/step, loss=0.69630, avg_loss=0.77045]\n",
            "Step 25863   [1.694 sec/step, loss=0.75459, avg_loss=0.77043]\n",
            "Step 25864   [1.697 sec/step, loss=0.74201, avg_loss=0.77018]\n",
            "Step 25865   [1.696 sec/step, loss=0.78170, avg_loss=0.76971]\n",
            "Step 25866   [1.693 sec/step, loss=0.79184, avg_loss=0.76969]\n",
            "Step 25867   [1.692 sec/step, loss=0.74254, avg_loss=0.76942]\n",
            "Step 25868   [1.701 sec/step, loss=0.80334, avg_loss=0.76977]\n",
            "Step 25869   [1.699 sec/step, loss=0.76275, avg_loss=0.76960]\n",
            "Step 25870   [1.703 sec/step, loss=0.74297, avg_loss=0.76902]\n",
            "Step 25871   [1.710 sec/step, loss=0.80634, avg_loss=0.76962]\n",
            "Generated 32 batches of size 8 in 7.925 sec\n",
            "Step 25872   [1.710 sec/step, loss=0.74385, avg_loss=0.76946]\n",
            "Step 25873   [1.709 sec/step, loss=0.70584, avg_loss=0.76899]\n",
            "Step 25874   [1.713 sec/step, loss=0.76736, avg_loss=0.76920]\n",
            "Step 25875   [1.707 sec/step, loss=0.80340, avg_loss=0.77013]\n",
            "Step 25876   [1.710 sec/step, loss=0.82439, avg_loss=0.77085]\n",
            "Step 25877   [1.677 sec/step, loss=0.75948, avg_loss=0.77050]\n",
            "Step 25878   [1.677 sec/step, loss=0.75345, avg_loss=0.77028]\n",
            "Step 25879   [1.673 sec/step, loss=0.69946, avg_loss=0.76926]\n",
            "Step 25880   [1.681 sec/step, loss=0.82121, avg_loss=0.76988]\n",
            "Step 25881   [1.654 sec/step, loss=0.77809, avg_loss=0.77041]\n",
            "Step 25882   [1.647 sec/step, loss=0.78280, avg_loss=0.77029]\n",
            "Step 25883   [1.679 sec/step, loss=0.73476, avg_loss=0.76933]\n",
            "Step 25884   [1.684 sec/step, loss=0.74447, avg_loss=0.76924]\n",
            "Step 25885   [1.684 sec/step, loss=0.79590, avg_loss=0.76913]\n",
            "Step 25886   [1.642 sec/step, loss=0.75193, avg_loss=0.76968]\n",
            "Step 25887   [1.635 sec/step, loss=0.75828, avg_loss=0.76949]\n",
            "Step 25888   [1.648 sec/step, loss=0.81532, avg_loss=0.77025]\n",
            "Step 25889   [1.639 sec/step, loss=0.77485, avg_loss=0.77064]\n",
            "Step 25890   [1.640 sec/step, loss=0.76552, avg_loss=0.77080]\n",
            "Step 25891   [1.644 sec/step, loss=0.77493, avg_loss=0.77060]\n",
            "Step 25892   [1.645 sec/step, loss=0.73302, avg_loss=0.77011]\n",
            "Step 25893   [1.633 sec/step, loss=0.76433, avg_loss=0.77040]\n",
            "Step 25894   [1.635 sec/step, loss=0.75815, avg_loss=0.76992]\n",
            "Step 25895   [1.634 sec/step, loss=0.74199, avg_loss=0.76948]\n",
            "Step 25896   [1.632 sec/step, loss=0.76392, avg_loss=0.76916]\n",
            "Step 25897   [1.619 sec/step, loss=0.76239, avg_loss=0.76871]\n",
            "Step 25898   [1.623 sec/step, loss=0.72529, avg_loss=0.76800]\n",
            "Step 25899   [1.627 sec/step, loss=0.77842, avg_loss=0.76829]\n",
            "Step 25900   [1.629 sec/step, loss=0.74806, avg_loss=0.76858]\n",
            "Writing summary at step: 25900\n",
            "Step 25901   [1.626 sec/step, loss=0.79510, avg_loss=0.76865]\n",
            "Step 25902   [1.636 sec/step, loss=0.75986, avg_loss=0.76821]\n",
            "Generated 32 batches of size 8 in 8.136 sec\n",
            "Step 25903   [1.638 sec/step, loss=0.77636, avg_loss=0.76801]\n",
            "Step 25904   [1.649 sec/step, loss=0.73224, avg_loss=0.76793]\n",
            "Step 25905   [1.635 sec/step, loss=0.78185, avg_loss=0.76799]\n",
            "Step 25906   [1.633 sec/step, loss=0.74758, avg_loss=0.76736]\n",
            "Step 25907   [1.629 sec/step, loss=0.76402, avg_loss=0.76721]\n",
            "Step 25908   [1.633 sec/step, loss=0.72471, avg_loss=0.76663]\n",
            "Step 25909   [1.641 sec/step, loss=0.79338, avg_loss=0.76668]\n",
            "Step 25910   [1.637 sec/step, loss=0.79527, avg_loss=0.76661]\n",
            "Step 25911   [1.627 sec/step, loss=0.76153, avg_loss=0.76726]\n",
            "Step 25912   [1.627 sec/step, loss=0.74625, avg_loss=0.76686]\n",
            "Step 25913   [1.625 sec/step, loss=0.74619, avg_loss=0.76677]\n",
            "Step 25914   [1.664 sec/step, loss=0.72210, avg_loss=0.76582]\n",
            "Step 25915   [1.663 sec/step, loss=0.79606, avg_loss=0.76568]\n",
            "Step 25916   [1.665 sec/step, loss=0.77123, avg_loss=0.76615]\n",
            "Step 25917   [1.662 sec/step, loss=0.80525, avg_loss=0.76642]\n",
            "Step 25918   [1.661 sec/step, loss=0.79987, avg_loss=0.76676]\n",
            "Step 25919   [1.675 sec/step, loss=0.75558, avg_loss=0.76646]\n",
            "Step 25920   [1.670 sec/step, loss=0.80816, avg_loss=0.76644]\n",
            "Step 25921   [1.667 sec/step, loss=0.78769, avg_loss=0.76638]\n",
            "Step 25922   [1.673 sec/step, loss=0.77249, avg_loss=0.76620]\n",
            "Step 25923   [1.672 sec/step, loss=0.70940, avg_loss=0.76537]\n",
            "Step 25924   [1.670 sec/step, loss=0.78478, avg_loss=0.76588]\n",
            "Step 25925   [1.674 sec/step, loss=0.78221, avg_loss=0.76551]\n",
            "Step 25926   [1.674 sec/step, loss=0.75776, avg_loss=0.76524]\n",
            "Step 25927   [1.670 sec/step, loss=0.80627, avg_loss=0.76551]\n",
            "Step 25928   [1.657 sec/step, loss=0.74888, avg_loss=0.76501]\n",
            "Step 25929   [1.654 sec/step, loss=0.78900, avg_loss=0.76559]\n",
            "Step 25930   [1.622 sec/step, loss=0.80623, avg_loss=0.76684]\n",
            "Step 25931   [1.637 sec/step, loss=0.81214, avg_loss=0.76746]\n",
            "Step 25932   [1.648 sec/step, loss=0.77618, avg_loss=0.76730]\n",
            "Step 25933   [1.650 sec/step, loss=0.74725, avg_loss=0.76690]\n",
            "Step 25934   [1.661 sec/step, loss=0.84838, avg_loss=0.76805]\n",
            "Generated 32 batches of size 8 in 9.565 sec\n",
            "Step 25935   [1.646 sec/step, loss=0.78577, avg_loss=0.76811]\n",
            "Step 25936   [1.641 sec/step, loss=0.76309, avg_loss=0.76815]\n",
            "Step 25937   [1.644 sec/step, loss=0.72326, avg_loss=0.76761]\n",
            "Step 25938   [1.625 sec/step, loss=0.77431, avg_loss=0.76740]\n",
            "Step 25939   [1.630 sec/step, loss=0.79162, avg_loss=0.76760]\n",
            "Step 25940   [1.634 sec/step, loss=0.76935, avg_loss=0.76707]\n",
            "Step 25941   [1.633 sec/step, loss=0.76615, avg_loss=0.76697]\n",
            "Step 25942   [1.639 sec/step, loss=0.79257, avg_loss=0.76733]\n",
            "Step 25943   [1.645 sec/step, loss=0.77891, avg_loss=0.76746]\n",
            "Step 25944   [1.650 sec/step, loss=0.76891, avg_loss=0.76765]\n",
            "Step 25945   [1.646 sec/step, loss=0.79092, avg_loss=0.76817]\n",
            "Step 25946   [1.617 sec/step, loss=0.74815, avg_loss=0.76782]\n",
            "Step 25947   [1.617 sec/step, loss=0.78860, avg_loss=0.76809]\n",
            "Step 25948   [1.629 sec/step, loss=0.68305, avg_loss=0.76798]\n",
            "Step 25949   [1.627 sec/step, loss=0.79032, avg_loss=0.76870]\n",
            "Step 25950   [1.627 sec/step, loss=0.78892, avg_loss=0.76852]\n",
            "Step 25951   [1.614 sec/step, loss=0.76706, avg_loss=0.76817]\n",
            "Step 25952   [1.606 sec/step, loss=0.78875, avg_loss=0.76888]\n",
            "Step 25953   [1.607 sec/step, loss=0.83541, avg_loss=0.76952]\n",
            "Step 25954   [1.604 sec/step, loss=0.73245, avg_loss=0.76898]\n",
            "Step 25955   [1.598 sec/step, loss=0.78659, avg_loss=0.76901]\n",
            "Step 25956   [1.596 sec/step, loss=0.72994, avg_loss=0.76831]\n",
            "Step 25957   [1.590 sec/step, loss=0.75166, avg_loss=0.76795]\n",
            "Step 25958   [1.590 sec/step, loss=0.78181, avg_loss=0.76818]\n",
            "Step 25959   [1.593 sec/step, loss=0.81132, avg_loss=0.76857]\n",
            "Step 25960   [1.599 sec/step, loss=0.70799, avg_loss=0.76826]\n",
            "Step 25961   [1.597 sec/step, loss=0.71909, avg_loss=0.76782]\n",
            "Step 25962   [1.595 sec/step, loss=0.80755, avg_loss=0.76894]\n",
            "Step 25963   [1.593 sec/step, loss=0.77725, avg_loss=0.76916]\n",
            "Step 25964   [1.591 sec/step, loss=0.76139, avg_loss=0.76936]\n",
            "Step 25965   [1.596 sec/step, loss=0.77429, avg_loss=0.76928]\n",
            "Generated 32 batches of size 8 in 9.810 sec\n",
            "Step 25966   [1.628 sec/step, loss=0.78168, avg_loss=0.76918]\n",
            "Step 25967   [1.668 sec/step, loss=0.72724, avg_loss=0.76903]\n",
            "Step 25968   [1.664 sec/step, loss=0.78876, avg_loss=0.76888]\n",
            "Step 25969   [1.661 sec/step, loss=0.79772, avg_loss=0.76923]\n",
            "Step 25970   [1.660 sec/step, loss=0.77778, avg_loss=0.76958]\n",
            "Step 25971   [1.660 sec/step, loss=0.81364, avg_loss=0.76965]\n",
            "Step 25972   [1.663 sec/step, loss=0.77113, avg_loss=0.76993]\n",
            "Step 25973   [1.656 sec/step, loss=0.71388, avg_loss=0.77001]\n",
            "Step 25974   [1.646 sec/step, loss=0.79127, avg_loss=0.77025]\n",
            "Step 25975   [1.647 sec/step, loss=0.80896, avg_loss=0.77030]\n",
            "Step 25976   [1.639 sec/step, loss=0.77845, avg_loss=0.76984]\n",
            "Step 25977   [1.627 sec/step, loss=0.81684, avg_loss=0.77042]\n",
            "Step 25978   [1.625 sec/step, loss=0.75056, avg_loss=0.77039]\n",
            "Step 25979   [1.620 sec/step, loss=0.75573, avg_loss=0.77095]\n",
            "Step 25980   [1.618 sec/step, loss=0.76898, avg_loss=0.77043]\n",
            "Step 25981   [1.626 sec/step, loss=0.80304, avg_loss=0.77068]\n",
            "Step 25982   [1.631 sec/step, loss=0.68571, avg_loss=0.76971]\n",
            "Step 25983   [1.602 sec/step, loss=0.74695, avg_loss=0.76983]\n",
            "Step 25984   [1.590 sec/step, loss=0.71147, avg_loss=0.76950]\n",
            "Step 25985   [1.600 sec/step, loss=0.77396, avg_loss=0.76928]\n",
            "Step 25986   [1.607 sec/step, loss=0.74642, avg_loss=0.76922]\n",
            "Step 25987   [1.604 sec/step, loss=0.77590, avg_loss=0.76940]\n",
            "Step 25988   [1.597 sec/step, loss=0.77506, avg_loss=0.76900]\n",
            "Step 25989   [1.612 sec/step, loss=0.79491, avg_loss=0.76920]\n",
            "Step 25990   [1.608 sec/step, loss=0.77895, avg_loss=0.76933]\n",
            "Step 25991   [1.602 sec/step, loss=0.76513, avg_loss=0.76923]\n",
            "Step 25992   [1.643 sec/step, loss=0.55537, avg_loss=0.76746]\n",
            "Step 25993   [1.637 sec/step, loss=0.80966, avg_loss=0.76791]\n",
            "Step 25994   [1.635 sec/step, loss=0.82574, avg_loss=0.76859]\n",
            "Step 25995   [1.637 sec/step, loss=0.73553, avg_loss=0.76852]\n",
            "Step 25996   [1.647 sec/step, loss=0.75383, avg_loss=0.76842]\n",
            "Step 25997   [1.649 sec/step, loss=0.74800, avg_loss=0.76828]\n",
            "Step 25998   [1.660 sec/step, loss=0.75425, avg_loss=0.76857]\n",
            "Step 25999   [1.663 sec/step, loss=0.78390, avg_loss=0.76862]\n",
            "Generated 32 batches of size 8 in 10.107 sec\n",
            "Step 26000   [1.665 sec/step, loss=0.83300, avg_loss=0.76947]\n",
            "Writing summary at step: 26000\n",
            "Saving checkpoint to: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/model.ckpt-26000\n",
            "Step 26001   [1.661 sec/step, loss=0.74289, avg_loss=0.76895]\n",
            "Step 26002   [1.659 sec/step, loss=0.77395, avg_loss=0.76909]\n",
            "Step 26003   [1.659 sec/step, loss=0.76500, avg_loss=0.76898]\n",
            "Step 26004   [1.641 sec/step, loss=0.78535, avg_loss=0.76951]\n",
            "Step 26005   [1.632 sec/step, loss=0.78264, avg_loss=0.76951]\n",
            "Step 26006   [1.637 sec/step, loss=0.76813, avg_loss=0.76972]\n",
            "Step 26007   [1.643 sec/step, loss=0.74591, avg_loss=0.76954]\n",
            "Step 26008   [1.649 sec/step, loss=0.80427, avg_loss=0.77033]\n",
            "Step 26009   [1.642 sec/step, loss=0.74159, avg_loss=0.76982]\n",
            "Step 26010   [1.647 sec/step, loss=0.75638, avg_loss=0.76943]\n",
            "Step 26011   [1.654 sec/step, loss=0.76971, avg_loss=0.76951]\n",
            "Step 26012   [1.657 sec/step, loss=0.74535, avg_loss=0.76950]\n",
            "Step 26013   [1.670 sec/step, loss=0.73781, avg_loss=0.76942]\n",
            "Step 26014   [1.628 sec/step, loss=0.79735, avg_loss=0.77017]\n",
            "Step 26015   [1.627 sec/step, loss=0.71162, avg_loss=0.76932]\n",
            "Step 26016   [1.630 sec/step, loss=0.71200, avg_loss=0.76873]\n",
            "Step 26017   [1.633 sec/step, loss=0.74037, avg_loss=0.76808]\n",
            "Step 26018   [1.634 sec/step, loss=0.77469, avg_loss=0.76783]\n",
            "Step 26019   [1.634 sec/step, loss=0.74901, avg_loss=0.76777]\n",
            "Step 26020   [1.639 sec/step, loss=0.79117, avg_loss=0.76760]\n",
            "Step 26021   [1.639 sec/step, loss=0.78947, avg_loss=0.76761]\n",
            "Step 26022   [1.629 sec/step, loss=0.77598, avg_loss=0.76765]\n",
            "Step 26023   [1.632 sec/step, loss=0.81713, avg_loss=0.76873]\n",
            "Step 26024   [1.644 sec/step, loss=0.82268, avg_loss=0.76911]\n",
            "Step 26025   [1.639 sec/step, loss=0.73253, avg_loss=0.76861]\n",
            "Step 26026   [1.643 sec/step, loss=0.75625, avg_loss=0.76859]\n",
            "Step 26027   [1.645 sec/step, loss=0.75058, avg_loss=0.76804]\n",
            "Step 26028   [1.651 sec/step, loss=0.73217, avg_loss=0.76787]\n",
            "Step 26029   [1.667 sec/step, loss=0.70537, avg_loss=0.76703]\n",
            "Step 26030   [1.669 sec/step, loss=0.79184, avg_loss=0.76689]\n",
            "Generated 32 batches of size 8 in 10.824 sec\n",
            "Step 26031   [1.693 sec/step, loss=0.67886, avg_loss=0.76556]\n",
            "Step 26032   [1.683 sec/step, loss=0.75141, avg_loss=0.76531]\n",
            "Step 26033   [1.676 sec/step, loss=0.76705, avg_loss=0.76551]\n",
            "Step 26034   [1.665 sec/step, loss=0.79724, avg_loss=0.76500]\n",
            "Step 26035   [1.662 sec/step, loss=0.73317, avg_loss=0.76447]\n",
            "Step 26036   [1.655 sec/step, loss=0.75718, avg_loss=0.76441]\n",
            "Step 26037   [1.654 sec/step, loss=0.69376, avg_loss=0.76412]\n",
            "Step 26038   [1.654 sec/step, loss=0.72895, avg_loss=0.76366]\n",
            "Step 26039   [1.651 sec/step, loss=0.75521, avg_loss=0.76330]\n",
            "Step 26040   [1.652 sec/step, loss=0.79239, avg_loss=0.76353]\n",
            "Step 26041   [1.647 sec/step, loss=0.79706, avg_loss=0.76384]\n",
            "Step 26042   [1.647 sec/step, loss=0.78413, avg_loss=0.76375]\n",
            "Step 26043   [1.646 sec/step, loss=0.77006, avg_loss=0.76366]\n",
            "Step 26044   [1.642 sec/step, loss=0.77755, avg_loss=0.76375]\n",
            "Step 26045   [1.686 sec/step, loss=0.84459, avg_loss=0.76429]\n",
            "Step 26046   [1.685 sec/step, loss=0.74768, avg_loss=0.76428]\n",
            "Step 26047   [1.679 sec/step, loss=0.78455, avg_loss=0.76424]\n",
            "Step 26048   [1.663 sec/step, loss=0.73495, avg_loss=0.76476]\n",
            "Step 26049   [1.686 sec/step, loss=0.75236, avg_loss=0.76438]\n",
            "Step 26050   [1.689 sec/step, loss=0.79035, avg_loss=0.76440]\n",
            "Step 26051   [1.702 sec/step, loss=0.79138, avg_loss=0.76464]\n",
            "Step 26052   [1.705 sec/step, loss=0.75854, avg_loss=0.76434]\n",
            "Step 26053   [1.702 sec/step, loss=0.72816, avg_loss=0.76326]\n",
            "Step 26054   [1.708 sec/step, loss=0.75003, avg_loss=0.76344]\n",
            "Step 26055   [1.708 sec/step, loss=0.76312, avg_loss=0.76321]\n",
            "Step 26056   [1.710 sec/step, loss=0.79797, avg_loss=0.76389]\n",
            "Step 26057   [1.713 sec/step, loss=0.74589, avg_loss=0.76383]\n",
            "Step 26058   [1.728 sec/step, loss=0.82782, avg_loss=0.76429]\n",
            "Step 26059   [1.731 sec/step, loss=0.75163, avg_loss=0.76369]\n",
            "Step 26060   [1.734 sec/step, loss=0.75796, avg_loss=0.76419]\n",
            "Step 26061   [1.758 sec/step, loss=0.76075, avg_loss=0.76461]\n",
            "Step 26062   [1.757 sec/step, loss=0.75102, avg_loss=0.76404]\n",
            "Generated 32 batches of size 8 in 10.632 sec\n",
            "Step 26063   [1.756 sec/step, loss=0.73115, avg_loss=0.76358]\n",
            "Step 26064   [1.755 sec/step, loss=0.76378, avg_loss=0.76361]\n",
            "Step 26065   [1.753 sec/step, loss=0.76091, avg_loss=0.76347]\n",
            "Step 26066   [1.732 sec/step, loss=0.74322, avg_loss=0.76309]\n",
            "Step 26067   [1.692 sec/step, loss=0.75071, avg_loss=0.76332]\n",
            "Step 26068   [1.724 sec/step, loss=0.75031, avg_loss=0.76294]\n",
            "Step 26069   [1.726 sec/step, loss=0.79723, avg_loss=0.76293]\n",
            "Step 26070   [1.731 sec/step, loss=0.78452, avg_loss=0.76300]\n",
            "Step 26071   [1.724 sec/step, loss=0.80397, avg_loss=0.76290]\n",
            "Step 26072   [1.716 sec/step, loss=0.80477, avg_loss=0.76324]\n",
            "Step 26073   [1.718 sec/step, loss=0.78203, avg_loss=0.76392]\n",
            "Step 26074   [1.723 sec/step, loss=0.78691, avg_loss=0.76388]\n",
            "Step 26075   [1.721 sec/step, loss=0.79615, avg_loss=0.76375]\n",
            "Step 26076   [1.744 sec/step, loss=0.77795, avg_loss=0.76374]\n",
            "Step 26077   [1.741 sec/step, loss=0.74221, avg_loss=0.76300]\n",
            "Step 26078   [1.739 sec/step, loss=0.72543, avg_loss=0.76275]\n",
            "Step 26079   [1.738 sec/step, loss=0.77596, avg_loss=0.76295]\n",
            "Step 26080   [1.744 sec/step, loss=0.78253, avg_loss=0.76308]\n",
            "Step 26081   [1.736 sec/step, loss=0.77529, avg_loss=0.76281]\n",
            "Step 26082   [1.731 sec/step, loss=0.80791, avg_loss=0.76403]\n",
            "Step 26083   [1.721 sec/step, loss=0.76303, avg_loss=0.76419]\n",
            "Step 26084   [1.730 sec/step, loss=0.71065, avg_loss=0.76418]\n",
            "Step 26085   [1.717 sec/step, loss=0.74148, avg_loss=0.76386]\n",
            "Step 26086   [1.737 sec/step, loss=0.83044, avg_loss=0.76470]\n",
            "Step 26087   [1.739 sec/step, loss=0.83752, avg_loss=0.76531]\n",
            "Step 26088   [1.739 sec/step, loss=0.72446, avg_loss=0.76481]\n",
            "Step 26089   [1.739 sec/step, loss=0.71342, avg_loss=0.76399]\n",
            "Step 26090   [1.745 sec/step, loss=0.69745, avg_loss=0.76318]\n",
            "Step 26091   [1.753 sec/step, loss=0.76827, avg_loss=0.76321]\n",
            "Step 26092   [1.715 sec/step, loss=0.74644, avg_loss=0.76512]\n",
            "Step 26093   [1.715 sec/step, loss=0.71350, avg_loss=0.76416]\n",
            "Step 26094   [1.720 sec/step, loss=0.81473, avg_loss=0.76405]\n",
            "Generated 32 batches of size 8 in 10.233 sec\n",
            "Step 26095   [1.735 sec/step, loss=0.79084, avg_loss=0.76460]\n",
            "Step 26096   [1.724 sec/step, loss=0.74768, avg_loss=0.76454]\n",
            "Step 26097   [1.720 sec/step, loss=0.75193, avg_loss=0.76458]\n",
            "Step 26098   [1.711 sec/step, loss=0.79902, avg_loss=0.76503]\n",
            "Step 26099   [1.707 sec/step, loss=0.75272, avg_loss=0.76471]\n",
            "Step 26100   [1.714 sec/step, loss=0.76674, avg_loss=0.76405]\n",
            "Writing summary at step: 26100\n",
            "Step 26101   [1.726 sec/step, loss=0.80023, avg_loss=0.76463]\n",
            "Step 26102   [1.719 sec/step, loss=0.79536, avg_loss=0.76484]\n",
            "Step 26103   [1.746 sec/step, loss=0.70751, avg_loss=0.76426]\n",
            "Step 26104   [1.749 sec/step, loss=0.78118, avg_loss=0.76422]\n",
            "Step 26105   [1.755 sec/step, loss=0.78758, avg_loss=0.76427]\n",
            "Step 26106   [1.748 sec/step, loss=0.84696, avg_loss=0.76506]\n",
            "Step 26107   [1.741 sec/step, loss=0.78483, avg_loss=0.76545]\n",
            "Step 26108   [1.733 sec/step, loss=0.71048, avg_loss=0.76451]\n",
            "Step 26109   [1.733 sec/step, loss=0.80101, avg_loss=0.76511]\n",
            "Step 26110   [1.731 sec/step, loss=0.81617, avg_loss=0.76570]\n",
            "Step 26111   [1.730 sec/step, loss=0.77563, avg_loss=0.76576]\n",
            "Step 26112   [1.732 sec/step, loss=0.72359, avg_loss=0.76555]\n",
            "Step 26113   [1.717 sec/step, loss=0.80846, avg_loss=0.76625]\n",
            "Step 26114   [1.721 sec/step, loss=0.75820, avg_loss=0.76586]\n",
            "Step 26115   [1.727 sec/step, loss=0.73831, avg_loss=0.76613]\n",
            "Step 26116   [1.727 sec/step, loss=0.76448, avg_loss=0.76665]\n",
            "Step 26117   [1.731 sec/step, loss=0.80996, avg_loss=0.76735]\n",
            "Step 26118   [1.731 sec/step, loss=0.80214, avg_loss=0.76762]\n",
            "Step 26119   [1.719 sec/step, loss=0.75643, avg_loss=0.76770]\n",
            "Step 26120   [1.715 sec/step, loss=0.75706, avg_loss=0.76736]\n",
            "Step 26121   [1.720 sec/step, loss=0.77211, avg_loss=0.76718]\n",
            "Step 26122   [1.721 sec/step, loss=0.77897, avg_loss=0.76721]\n",
            "Step 26123   [1.719 sec/step, loss=0.78286, avg_loss=0.76687]\n",
            "Step 26124   [1.706 sec/step, loss=0.78624, avg_loss=0.76651]\n",
            "Step 26125   [1.721 sec/step, loss=0.76564, avg_loss=0.76684]\n",
            "Generated 32 batches of size 8 in 10.139 sec\n",
            "Step 26126   [1.740 sec/step, loss=0.74895, avg_loss=0.76676]\n",
            "Step 26127   [1.736 sec/step, loss=0.82818, avg_loss=0.76754]\n",
            "Step 26128   [1.726 sec/step, loss=0.75712, avg_loss=0.76779]\n",
            "Step 26129   [1.709 sec/step, loss=0.76089, avg_loss=0.76834]\n",
            "Step 26130   [1.711 sec/step, loss=0.73827, avg_loss=0.76781]\n",
            "Step 26131   [1.676 sec/step, loss=0.78902, avg_loss=0.76891]\n",
            "Step 26132   [1.685 sec/step, loss=0.80448, avg_loss=0.76944]\n",
            "Step 26133   [1.689 sec/step, loss=0.78810, avg_loss=0.76965]\n",
            "Step 26134   [1.688 sec/step, loss=0.74936, avg_loss=0.76917]\n",
            "Step 26135   [1.688 sec/step, loss=0.75614, avg_loss=0.76940]\n",
            "Step 26136   [1.690 sec/step, loss=0.80011, avg_loss=0.76983]\n",
            "Step 26137   [1.691 sec/step, loss=0.76456, avg_loss=0.77054]\n",
            "Step 26138   [1.698 sec/step, loss=0.76778, avg_loss=0.77093]\n",
            "Step 26139   [1.698 sec/step, loss=0.72420, avg_loss=0.77062]\n",
            "Step 26140   [1.684 sec/step, loss=0.77215, avg_loss=0.77041]\n",
            "Step 26141   [1.682 sec/step, loss=0.78899, avg_loss=0.77033]\n",
            "Step 26142   [1.677 sec/step, loss=0.80578, avg_loss=0.77055]\n",
            "Step 26143   [1.685 sec/step, loss=0.77479, avg_loss=0.77060]\n",
            "Step 26144   [1.684 sec/step, loss=0.67267, avg_loss=0.76955]\n",
            "Step 26145   [1.648 sec/step, loss=0.78822, avg_loss=0.76899]\n",
            "Step 26146   [1.647 sec/step, loss=0.73564, avg_loss=0.76887]\n",
            "Step 26147   [1.649 sec/step, loss=0.76969, avg_loss=0.76872]\n",
            "Step 26148   [1.652 sec/step, loss=0.75590, avg_loss=0.76893]\n",
            "Step 26149   [1.626 sec/step, loss=0.77718, avg_loss=0.76917]\n",
            "Step 26150   [1.625 sec/step, loss=0.72243, avg_loss=0.76850]\n",
            "Step 26151   [1.629 sec/step, loss=0.78957, avg_loss=0.76848]\n",
            "Step 26152   [1.623 sec/step, loss=0.71929, avg_loss=0.76808]\n",
            "Step 26153   [1.634 sec/step, loss=0.77557, avg_loss=0.76856]\n",
            "Step 26154   [1.632 sec/step, loss=0.80693, avg_loss=0.76913]\n",
            "Generated 32 batches of size 8 in 8.041 sec\n",
            "Step 26155   [1.679 sec/step, loss=0.73126, avg_loss=0.76881]\n",
            "Step 26156   [1.676 sec/step, loss=0.77955, avg_loss=0.76862]\n",
            "Step 26157   [1.676 sec/step, loss=0.76655, avg_loss=0.76883]\n",
            "Step 26158   [1.679 sec/step, loss=0.82333, avg_loss=0.76879]\n",
            "Step 26159   [1.673 sec/step, loss=0.74601, avg_loss=0.76873]\n",
            "Step 26160   [1.668 sec/step, loss=0.71233, avg_loss=0.76827]\n",
            "Step 26161   [1.649 sec/step, loss=0.80425, avg_loss=0.76871]\n",
            "Step 26162   [1.655 sec/step, loss=0.78153, avg_loss=0.76901]\n",
            "Step 26163   [1.643 sec/step, loss=0.72726, avg_loss=0.76898]\n",
            "Step 26164   [1.678 sec/step, loss=0.67960, avg_loss=0.76813]\n",
            "Step 26165   [1.681 sec/step, loss=0.68665, avg_loss=0.76739]\n",
            "Step 26166   [1.671 sec/step, loss=0.78714, avg_loss=0.76783]\n",
            "Step 26167   [1.674 sec/step, loss=0.66486, avg_loss=0.76697]\n",
            "Step 26168   [1.638 sec/step, loss=0.77732, avg_loss=0.76724]\n",
            "Step 26169   [1.632 sec/step, loss=0.83434, avg_loss=0.76761]\n",
            "Step 26170   [1.631 sec/step, loss=0.72584, avg_loss=0.76703]\n",
            "Step 26171   [1.641 sec/step, loss=0.76505, avg_loss=0.76664]\n",
            "Step 26172   [1.640 sec/step, loss=0.75138, avg_loss=0.76610]\n",
            "Step 26173   [1.636 sec/step, loss=0.81087, avg_loss=0.76639]\n",
            "Step 26174   [1.634 sec/step, loss=0.78257, avg_loss=0.76635]\n",
            "Step 26175   [1.653 sec/step, loss=0.76642, avg_loss=0.76605]\n",
            "Step 26176   [1.631 sec/step, loss=0.78289, avg_loss=0.76610]\n",
            "Step 26177   [1.632 sec/step, loss=0.77853, avg_loss=0.76646]\n",
            "Step 26178   [1.630 sec/step, loss=0.74604, avg_loss=0.76667]\n",
            "Step 26179   [1.632 sec/step, loss=0.74874, avg_loss=0.76640]\n",
            "Step 26180   [1.629 sec/step, loss=0.89357, avg_loss=0.76751]\n",
            "Step 26181   [1.630 sec/step, loss=0.84611, avg_loss=0.76822]\n",
            "Step 26182   [1.641 sec/step, loss=0.77473, avg_loss=0.76788]\n",
            "Step 26183   [1.642 sec/step, loss=0.82759, avg_loss=0.76853]\n",
            "Step 26184   [1.642 sec/step, loss=0.81895, avg_loss=0.76961]\n",
            "Step 26185   [1.648 sec/step, loss=0.80835, avg_loss=0.77028]\n",
            "Step 26186   [1.626 sec/step, loss=0.82187, avg_loss=0.77020]\n",
            "Step 26187   [1.631 sec/step, loss=0.77018, avg_loss=0.76952]\n",
            "Step 26188   [1.634 sec/step, loss=0.78868, avg_loss=0.77016]\n",
            "Generated 32 batches of size 8 in 7.025 sec\n",
            "Step 26189   [1.624 sec/step, loss=0.75588, avg_loss=0.77059]\n",
            "Step 26190   [1.624 sec/step, loss=0.71941, avg_loss=0.77081]\n",
            "Step 26191   [1.627 sec/step, loss=0.77090, avg_loss=0.77083]\n",
            "Step 26192   [1.625 sec/step, loss=0.79642, avg_loss=0.77133]\n",
            "Step 26193   [1.631 sec/step, loss=0.80134, avg_loss=0.77221]\n",
            "Step 26194   [1.658 sec/step, loss=0.75119, avg_loss=0.77158]\n",
            "Step 26195   [1.666 sec/step, loss=0.83324, avg_loss=0.77200]\n",
            "Step 26196   [1.668 sec/step, loss=0.76948, avg_loss=0.77222]\n",
            "Step 26197   [1.666 sec/step, loss=0.79291, avg_loss=0.77263]\n",
            "Step 26198   [1.659 sec/step, loss=0.79046, avg_loss=0.77254]\n",
            "Step 26199   [1.665 sec/step, loss=0.73064, avg_loss=0.77232]\n",
            "Step 26200   [1.650 sec/step, loss=0.77790, avg_loss=0.77243]\n",
            "Writing summary at step: 26200\n",
            "Step 26201   [1.640 sec/step, loss=0.76414, avg_loss=0.77207]\n",
            "Step 26202   [1.640 sec/step, loss=0.75452, avg_loss=0.77167]\n",
            "Step 26203   [1.602 sec/step, loss=0.78523, avg_loss=0.77244]\n",
            "Step 26204   [1.602 sec/step, loss=0.78252, avg_loss=0.77246]\n",
            "Step 26205   [1.604 sec/step, loss=0.78172, avg_loss=0.77240]\n",
            "Step 26206   [1.603 sec/step, loss=0.76773, avg_loss=0.77160]\n",
            "Step 26207   [1.606 sec/step, loss=0.79150, avg_loss=0.77167]\n",
            "Step 26208   [1.609 sec/step, loss=0.71775, avg_loss=0.77174]\n",
            "Step 26209   [1.612 sec/step, loss=0.80557, avg_loss=0.77179]\n",
            "Step 26210   [1.610 sec/step, loss=0.76938, avg_loss=0.77132]\n",
            "Step 26211   [1.602 sec/step, loss=0.75733, avg_loss=0.77114]\n",
            "Step 26212   [1.605 sec/step, loss=0.78865, avg_loss=0.77179]\n",
            "Step 26213   [1.605 sec/step, loss=0.78401, avg_loss=0.77155]\n",
            "Step 26214   [1.612 sec/step, loss=0.81124, avg_loss=0.77208]\n",
            "Step 26215   [1.613 sec/step, loss=0.78373, avg_loss=0.77253]\n",
            "Step 26216   [1.609 sec/step, loss=0.78878, avg_loss=0.77277]\n",
            "Step 26217   [1.606 sec/step, loss=0.78826, avg_loss=0.77256]\n",
            "Step 26218   [1.614 sec/step, loss=0.80287, avg_loss=0.77256]\n",
            "Step 26219   [1.621 sec/step, loss=0.72696, avg_loss=0.77227]\n",
            "Generated 32 batches of size 8 in 7.328 sec\n",
            "Step 26220   [1.627 sec/step, loss=0.81341, avg_loss=0.77283]\n",
            "Step 26221   [1.622 sec/step, loss=0.79707, avg_loss=0.77308]\n",
            "Step 26222   [1.622 sec/step, loss=0.81686, avg_loss=0.77346]\n",
            "Step 26223   [1.630 sec/step, loss=0.76088, avg_loss=0.77324]\n",
            "Step 26224   [1.644 sec/step, loss=0.77755, avg_loss=0.77315]\n",
            "Step 26225   [1.672 sec/step, loss=0.70110, avg_loss=0.77251]\n",
            "Step 26226   [1.651 sec/step, loss=0.77939, avg_loss=0.77281]\n",
            "Step 26227   [1.650 sec/step, loss=0.80953, avg_loss=0.77263]\n",
            "Step 26228   [1.653 sec/step, loss=0.78898, avg_loss=0.77294]\n",
            "Step 26229   [1.653 sec/step, loss=0.80133, avg_loss=0.77335]\n",
            "Step 26230   [1.649 sec/step, loss=0.76766, avg_loss=0.77364]\n",
            "Step 26231   [1.648 sec/step, loss=0.78938, avg_loss=0.77365]\n",
            "Step 26232   [1.647 sec/step, loss=0.77123, avg_loss=0.77331]\n",
            "Step 26233   [1.643 sec/step, loss=0.81380, avg_loss=0.77357]\n",
            "Step 26234   [1.647 sec/step, loss=0.74623, avg_loss=0.77354]\n",
            "Step 26235   [1.669 sec/step, loss=0.78341, avg_loss=0.77381]\n",
            "Step 26236   [1.662 sec/step, loss=0.73989, avg_loss=0.77321]\n",
            "Step 26237   [1.670 sec/step, loss=0.73816, avg_loss=0.77295]\n",
            "Step 26238   [1.663 sec/step, loss=0.78876, avg_loss=0.77316]\n",
            "Step 26239   [1.673 sec/step, loss=0.80084, avg_loss=0.77392]\n",
            "Step 26240   [1.672 sec/step, loss=0.76043, avg_loss=0.77381]\n",
            "Step 26241   [1.678 sec/step, loss=0.78297, avg_loss=0.77375]\n",
            "Step 26242   [1.678 sec/step, loss=0.77340, avg_loss=0.77342]\n",
            "Step 26243   [1.675 sec/step, loss=0.82831, avg_loss=0.77396]\n",
            "Step 26244   [1.678 sec/step, loss=0.72685, avg_loss=0.77450]\n",
            "Step 26245   [1.670 sec/step, loss=0.76052, avg_loss=0.77422]\n",
            "Step 26246   [1.679 sec/step, loss=0.71917, avg_loss=0.77406]\n",
            "Step 26247   [1.683 sec/step, loss=0.77175, avg_loss=0.77408]\n",
            "Step 26248   [1.684 sec/step, loss=0.74647, avg_loss=0.77398]\n",
            "Step 26249   [1.685 sec/step, loss=0.79133, avg_loss=0.77412]\n",
            "Step 26250   [1.684 sec/step, loss=0.78913, avg_loss=0.77479]\n",
            "Step 26251   [1.674 sec/step, loss=0.74719, avg_loss=0.77437]\n",
            "Generated 32 batches of size 8 in 7.843 sec\n",
            "Step 26252   [1.680 sec/step, loss=0.75438, avg_loss=0.77472]\n",
            "Step 26253   [1.688 sec/step, loss=0.75076, avg_loss=0.77447]\n",
            "Step 26254   [1.686 sec/step, loss=0.79992, avg_loss=0.77440]\n",
            "Step 26255   [1.640 sec/step, loss=0.72828, avg_loss=0.77437]\n",
            "Step 26256   [1.646 sec/step, loss=0.77032, avg_loss=0.77428]\n",
            "Step 26257   [1.644 sec/step, loss=0.78598, avg_loss=0.77447]\n",
            "Step 26258   [1.624 sec/step, loss=0.75070, avg_loss=0.77375]\n",
            "Step 26259   [1.626 sec/step, loss=0.75685, avg_loss=0.77385]\n",
            "Step 26260   [1.653 sec/step, loss=0.70122, avg_loss=0.77374]\n",
            "Step 26261   [1.666 sec/step, loss=0.73685, avg_loss=0.77307]\n",
            "Step 26262   [1.658 sec/step, loss=0.77451, avg_loss=0.77300]\n",
            "Step 26263   [1.660 sec/step, loss=0.69413, avg_loss=0.77267]\n",
            "Step 26264   [1.625 sec/step, loss=0.79716, avg_loss=0.77384]\n",
            "Step 26265   [1.621 sec/step, loss=0.77493, avg_loss=0.77473]\n",
            "Step 26266   [1.622 sec/step, loss=0.78065, avg_loss=0.77466]\n",
            "Step 26267   [1.625 sec/step, loss=0.76385, avg_loss=0.77565]\n",
            "Step 26268   [1.624 sec/step, loss=0.72678, avg_loss=0.77515]\n",
            "Step 26269   [1.625 sec/step, loss=0.75573, avg_loss=0.77436]\n",
            "Step 26270   [1.620 sec/step, loss=0.79776, avg_loss=0.77508]\n",
            "Step 26271   [1.617 sec/step, loss=0.77388, avg_loss=0.77517]\n",
            "Step 26272   [1.627 sec/step, loss=0.80998, avg_loss=0.77575]\n",
            "Step 26273   [1.632 sec/step, loss=0.78529, avg_loss=0.77550]\n",
            "Step 26274   [1.631 sec/step, loss=0.78473, avg_loss=0.77552]\n",
            "Step 26275   [1.609 sec/step, loss=0.75563, avg_loss=0.77541]\n",
            "Step 26276   [1.611 sec/step, loss=0.81119, avg_loss=0.77569]\n",
            "Step 26277   [1.626 sec/step, loss=0.72168, avg_loss=0.77513]\n",
            "Step 26278   [1.630 sec/step, loss=0.74698, avg_loss=0.77513]\n",
            "Step 26279   [1.629 sec/step, loss=0.77027, avg_loss=0.77535]\n",
            "Step 26280   [1.636 sec/step, loss=0.81096, avg_loss=0.77452]\n",
            "Step 26281   [1.642 sec/step, loss=0.77874, avg_loss=0.77385]\n",
            "Step 26282   [1.634 sec/step, loss=0.76985, avg_loss=0.77380]\n",
            "Step 26283   [1.640 sec/step, loss=0.72389, avg_loss=0.77276]\n",
            "Generated 32 batches of size 8 in 8.313 sec\n",
            "Step 26284   [1.649 sec/step, loss=0.76727, avg_loss=0.77225]\n",
            "Step 26285   [1.649 sec/step, loss=0.76930, avg_loss=0.77186]\n",
            "Step 26286   [1.653 sec/step, loss=0.82047, avg_loss=0.77184]\n",
            "Step 26287   [1.646 sec/step, loss=0.77693, avg_loss=0.77191]\n",
            "Step 26288   [1.640 sec/step, loss=0.74527, avg_loss=0.77148]\n",
            "Step 26289   [1.638 sec/step, loss=0.69297, avg_loss=0.77085]\n",
            "Step 26290   [1.636 sec/step, loss=0.81091, avg_loss=0.77176]\n",
            "Step 26291   [1.626 sec/step, loss=0.73989, avg_loss=0.77145]\n",
            "Step 26292   [1.627 sec/step, loss=0.76524, avg_loss=0.77114]\n",
            "Step 26293   [1.625 sec/step, loss=0.73923, avg_loss=0.77052]\n",
            "Step 26294   [1.591 sec/step, loss=0.80542, avg_loss=0.77106]\n",
            "Step 26295   [1.568 sec/step, loss=0.80371, avg_loss=0.77077]\n",
            "Step 26296   [1.576 sec/step, loss=0.69571, avg_loss=0.77003]\n",
            "Step 26297   [1.578 sec/step, loss=0.66422, avg_loss=0.76874]\n",
            "Step 26298   [1.576 sec/step, loss=0.74955, avg_loss=0.76833]\n",
            "Step 26299   [1.575 sec/step, loss=0.76859, avg_loss=0.76871]\n",
            "Step 26300   [1.576 sec/step, loss=0.77536, avg_loss=0.76869]\n",
            "Writing summary at step: 26300\n",
            "Step 26301   [1.572 sec/step, loss=0.75577, avg_loss=0.76860]\n",
            "Step 26302   [1.571 sec/step, loss=0.78643, avg_loss=0.76892]\n",
            "Step 26303   [1.573 sec/step, loss=0.79591, avg_loss=0.76903]\n",
            "Step 26304   [1.609 sec/step, loss=0.67554, avg_loss=0.76796]\n",
            "Step 26305   [1.601 sec/step, loss=0.73322, avg_loss=0.76747]\n",
            "Step 26306   [1.600 sec/step, loss=0.75071, avg_loss=0.76730]\n",
            "Step 26307   [1.601 sec/step, loss=0.79196, avg_loss=0.76731]\n",
            "Step 26308   [1.599 sec/step, loss=0.77083, avg_loss=0.76784]\n",
            "Step 26309   [1.600 sec/step, loss=0.77220, avg_loss=0.76751]\n",
            "Step 26310   [1.607 sec/step, loss=0.66814, avg_loss=0.76649]\n",
            "Step 26311   [1.634 sec/step, loss=0.82482, avg_loss=0.76717]\n",
            "Step 26312   [1.631 sec/step, loss=0.77181, avg_loss=0.76700]\n",
            "Step 26313   [1.640 sec/step, loss=0.80751, avg_loss=0.76723]\n",
            "Step 26314   [1.631 sec/step, loss=0.82762, avg_loss=0.76740]\n",
            "Generated 32 batches of size 8 in 8.924 sec\n",
            "Step 26315   [1.633 sec/step, loss=0.77069, avg_loss=0.76727]\n",
            "Step 26316   [1.629 sec/step, loss=0.75491, avg_loss=0.76693]\n",
            "Step 26317   [1.625 sec/step, loss=0.77360, avg_loss=0.76678]\n",
            "Step 26318   [1.618 sec/step, loss=0.78260, avg_loss=0.76658]\n",
            "Step 26319   [1.617 sec/step, loss=0.80967, avg_loss=0.76741]\n",
            "Step 26320   [1.609 sec/step, loss=0.78573, avg_loss=0.76713]\n",
            "Step 26321   [1.609 sec/step, loss=0.76735, avg_loss=0.76683]\n",
            "Step 26322   [1.607 sec/step, loss=0.80731, avg_loss=0.76674]\n",
            "Step 26323   [1.597 sec/step, loss=0.77987, avg_loss=0.76693]\n",
            "Step 26324   [1.584 sec/step, loss=0.73237, avg_loss=0.76648]\n",
            "Step 26325   [1.542 sec/step, loss=0.80697, avg_loss=0.76753]\n",
            "Step 26326   [1.542 sec/step, loss=0.74952, avg_loss=0.76724]\n",
            "Step 26327   [1.551 sec/step, loss=0.73579, avg_loss=0.76650]\n",
            "Step 26328   [1.550 sec/step, loss=0.75176, avg_loss=0.76613]\n",
            "Step 26329   [1.548 sec/step, loss=0.80833, avg_loss=0.76620]\n",
            "Step 26330   [1.558 sec/step, loss=0.78126, avg_loss=0.76633]\n",
            "Step 26331   [1.561 sec/step, loss=0.77353, avg_loss=0.76617]\n",
            "Step 26332   [1.555 sec/step, loss=0.80040, avg_loss=0.76647]\n",
            "Step 26333   [1.560 sec/step, loss=0.75743, avg_loss=0.76590]\n",
            "Step 26334   [1.556 sec/step, loss=0.76328, avg_loss=0.76607]\n",
            "Step 26335   [1.537 sec/step, loss=0.76908, avg_loss=0.76593]\n",
            "Step 26336   [1.543 sec/step, loss=0.77028, avg_loss=0.76623]\n",
            "Step 26337   [1.536 sec/step, loss=0.79310, avg_loss=0.76678]\n",
            "Step 26338   [1.534 sec/step, loss=0.73449, avg_loss=0.76624]\n",
            "Step 26339   [1.523 sec/step, loss=0.81493, avg_loss=0.76638]\n",
            "Step 26340   [1.525 sec/step, loss=0.80594, avg_loss=0.76684]\n",
            "Step 26341   [1.519 sec/step, loss=0.75739, avg_loss=0.76658]\n",
            "Step 26342   [1.517 sec/step, loss=0.79069, avg_loss=0.76675]\n",
            "Step 26343   [1.541 sec/step, loss=0.71508, avg_loss=0.76562]\n",
            "Step 26344   [1.540 sec/step, loss=0.78907, avg_loss=0.76624]\n",
            "Step 26345   [1.564 sec/step, loss=0.76863, avg_loss=0.76632]\n",
            "Generated 32 batches of size 8 in 9.836 sec\n",
            "Step 26346   [1.598 sec/step, loss=0.70336, avg_loss=0.76617]\n",
            "Step 26347   [1.596 sec/step, loss=0.81432, avg_loss=0.76659]\n",
            "Step 26348   [1.598 sec/step, loss=0.75893, avg_loss=0.76672]\n",
            "Step 26349   [1.612 sec/step, loss=0.68206, avg_loss=0.76562]\n",
            "Step 26350   [1.611 sec/step, loss=0.77511, avg_loss=0.76548]\n",
            "Step 26351   [1.602 sec/step, loss=0.78934, avg_loss=0.76590]\n",
            "Step 26352   [1.604 sec/step, loss=0.77795, avg_loss=0.76614]\n",
            "Step 26353   [1.589 sec/step, loss=0.77099, avg_loss=0.76634]\n",
            "Step 26354   [1.588 sec/step, loss=0.81955, avg_loss=0.76654]\n",
            "Step 26355   [1.587 sec/step, loss=0.80614, avg_loss=0.76732]\n",
            "Step 26356   [1.583 sec/step, loss=0.76346, avg_loss=0.76725]\n",
            "Step 26357   [1.588 sec/step, loss=0.71095, avg_loss=0.76650]\n",
            "Step 26358   [1.594 sec/step, loss=0.75595, avg_loss=0.76655]\n",
            "Step 26359   [1.597 sec/step, loss=0.79890, avg_loss=0.76697]\n",
            "Step 26360   [1.569 sec/step, loss=0.78806, avg_loss=0.76784]\n",
            "Step 26361   [1.553 sec/step, loss=0.75844, avg_loss=0.76806]\n",
            "Step 26362   [1.554 sec/step, loss=0.79215, avg_loss=0.76823]\n",
            "Step 26363   [1.545 sec/step, loss=0.80067, avg_loss=0.76930]\n",
            "Step 26364   [1.539 sec/step, loss=0.77754, avg_loss=0.76910]\n",
            "Step 26365   [1.539 sec/step, loss=0.78662, avg_loss=0.76922]\n",
            "Step 26366   [1.548 sec/step, loss=0.74270, avg_loss=0.76884]\n",
            "Step 26367   [1.550 sec/step, loss=0.75932, avg_loss=0.76879]\n",
            "Step 26368   [1.544 sec/step, loss=0.75731, avg_loss=0.76910]\n",
            "Step 26369   [1.548 sec/step, loss=0.79089, avg_loss=0.76945]\n",
            "Step 26370   [1.562 sec/step, loss=0.82045, avg_loss=0.76968]\n",
            "Step 26371   [1.562 sec/step, loss=0.73635, avg_loss=0.76930]\n",
            "Step 26372   [1.558 sec/step, loss=0.79640, avg_loss=0.76917]\n",
            "Step 26373   [1.555 sec/step, loss=0.78607, avg_loss=0.76917]\n",
            "Step 26374   [1.552 sec/step, loss=0.75347, avg_loss=0.76886]\n",
            "Step 26375   [1.557 sec/step, loss=0.73236, avg_loss=0.76863]\n",
            "Step 26376   [1.567 sec/step, loss=0.80150, avg_loss=0.76853]\n",
            "Step 26377   [1.572 sec/step, loss=0.76938, avg_loss=0.76901]\n",
            "Step 26378   [1.573 sec/step, loss=0.82756, avg_loss=0.76982]\n",
            "Step 26379   [1.572 sec/step, loss=0.75728, avg_loss=0.76969]\n",
            "Generated 32 batches of size 8 in 10.211 sec\n",
            "Step 26380   [1.563 sec/step, loss=0.76971, avg_loss=0.76927]\n",
            "Step 26381   [1.599 sec/step, loss=0.74822, avg_loss=0.76897]\n",
            "Step 26382   [1.621 sec/step, loss=0.74780, avg_loss=0.76875]\n",
            "Step 26383   [1.617 sec/step, loss=0.71701, avg_loss=0.76868]\n",
            "Step 26384   [1.602 sec/step, loss=0.74428, avg_loss=0.76845]\n",
            "Step 26385   [1.596 sec/step, loss=0.75889, avg_loss=0.76834]\n",
            "Step 26386   [1.608 sec/step, loss=0.72149, avg_loss=0.76735]\n",
            "Step 26387   [1.611 sec/step, loss=0.72893, avg_loss=0.76687]\n",
            "Step 26388   [1.609 sec/step, loss=0.78098, avg_loss=0.76723]\n",
            "Step 26389   [1.610 sec/step, loss=0.79590, avg_loss=0.76826]\n",
            "Step 26390   [1.616 sec/step, loss=0.70273, avg_loss=0.76718]\n",
            "Step 26391   [1.619 sec/step, loss=0.74155, avg_loss=0.76720]\n",
            "Step 26392   [1.616 sec/step, loss=0.79144, avg_loss=0.76746]\n",
            "Step 26393   [1.610 sec/step, loss=0.76752, avg_loss=0.76774]\n",
            "Step 26394   [1.609 sec/step, loss=0.78627, avg_loss=0.76755]\n",
            "Step 26395   [1.611 sec/step, loss=0.78055, avg_loss=0.76732]\n",
            "Step 26396   [1.602 sec/step, loss=0.75941, avg_loss=0.76795]\n",
            "Step 26397   [1.607 sec/step, loss=0.80568, avg_loss=0.76937]\n",
            "Step 26398   [1.614 sec/step, loss=0.77073, avg_loss=0.76958]\n",
            "Step 26399   [1.611 sec/step, loss=0.75775, avg_loss=0.76947]\n",
            "Step 26400   [1.613 sec/step, loss=0.80197, avg_loss=0.76974]\n",
            "Writing summary at step: 26400\n",
            "Step 26401   [1.617 sec/step, loss=0.77545, avg_loss=0.76994]\n",
            "Step 26402   [1.628 sec/step, loss=0.76356, avg_loss=0.76971]\n",
            "Step 26403   [1.642 sec/step, loss=0.74759, avg_loss=0.76922]\n",
            "Step 26404   [1.604 sec/step, loss=0.79105, avg_loss=0.77038]\n",
            "Step 26405   [1.611 sec/step, loss=0.74690, avg_loss=0.77052]\n",
            "Step 26406   [1.614 sec/step, loss=0.77819, avg_loss=0.77079]\n",
            "Step 26407   [1.613 sec/step, loss=0.75931, avg_loss=0.77046]\n",
            "Step 26408   [1.612 sec/step, loss=0.73802, avg_loss=0.77014]\n",
            "Step 26409   [1.622 sec/step, loss=0.75543, avg_loss=0.76997]\n",
            "Step 26410   [1.622 sec/step, loss=0.76635, avg_loss=0.77095]\n",
            "Generated 32 batches of size 8 in 10.125 sec\n",
            "Step 26411   [1.635 sec/step, loss=0.72223, avg_loss=0.76992]\n",
            "Step 26412   [1.628 sec/step, loss=0.77078, avg_loss=0.76991]\n",
            "Step 26413   [1.626 sec/step, loss=0.77395, avg_loss=0.76958]\n",
            "Step 26414   [1.625 sec/step, loss=0.76971, avg_loss=0.76900]\n",
            "Step 26415   [1.617 sec/step, loss=0.71518, avg_loss=0.76844]\n",
            "Step 26416   [1.616 sec/step, loss=0.76976, avg_loss=0.76859]\n",
            "Step 26417   [1.622 sec/step, loss=0.76605, avg_loss=0.76852]\n",
            "Step 26418   [1.627 sec/step, loss=0.77869, avg_loss=0.76848]\n",
            "Step 26419   [1.620 sec/step, loss=0.72960, avg_loss=0.76768]\n",
            "Step 26420   [1.626 sec/step, loss=0.84914, avg_loss=0.76831]\n",
            "Step 26421   [1.629 sec/step, loss=0.74258, avg_loss=0.76806]\n",
            "Step 26422   [1.628 sec/step, loss=0.79642, avg_loss=0.76795]\n",
            "Step 26423   [1.640 sec/step, loss=0.73417, avg_loss=0.76750]\n",
            "Step 26424   [1.645 sec/step, loss=0.81844, avg_loss=0.76836]\n",
            "Step 26425   [1.643 sec/step, loss=0.79094, avg_loss=0.76820]\n",
            "Step 26426   [1.646 sec/step, loss=0.76366, avg_loss=0.76834]\n",
            "Step 26427   [1.637 sec/step, loss=0.77689, avg_loss=0.76875]\n",
            "Step 26428   [1.638 sec/step, loss=0.67949, avg_loss=0.76803]\n",
            "Step 26429   [1.637 sec/step, loss=0.78718, avg_loss=0.76782]\n",
            "Step 26430   [1.629 sec/step, loss=0.80559, avg_loss=0.76806]\n",
            "Step 26431   [1.629 sec/step, loss=0.77960, avg_loss=0.76812]\n",
            "Step 26432   [1.626 sec/step, loss=0.77968, avg_loss=0.76791]\n",
            "Step 26433   [1.622 sec/step, loss=0.79056, avg_loss=0.76824]\n",
            "Step 26434   [1.622 sec/step, loss=0.71820, avg_loss=0.76779]\n",
            "Step 26435   [1.619 sec/step, loss=0.81394, avg_loss=0.76824]\n",
            "Step 26436   [1.610 sec/step, loss=0.74667, avg_loss=0.76801]\n",
            "Step 26437   [1.608 sec/step, loss=0.75448, avg_loss=0.76762]\n",
            "Step 26438   [1.640 sec/step, loss=0.75019, avg_loss=0.76778]\n",
            "Step 26439   [1.643 sec/step, loss=0.78769, avg_loss=0.76750]\n",
            "Step 26440   [1.656 sec/step, loss=0.78100, avg_loss=0.76726]\n",
            "Step 26441   [1.658 sec/step, loss=0.77018, avg_loss=0.76738]\n",
            "Generated 32 batches of size 8 in 9.946 sec\n",
            "Step 26442   [1.673 sec/step, loss=0.78353, avg_loss=0.76731]\n",
            "Step 26443   [1.642 sec/step, loss=0.74707, avg_loss=0.76763]\n",
            "Step 26444   [1.640 sec/step, loss=0.74339, avg_loss=0.76717]\n",
            "Step 26445   [1.655 sec/step, loss=0.67903, avg_loss=0.76628]\n",
            "Step 26446   [1.625 sec/step, loss=0.82034, avg_loss=0.76745]\n",
            "Step 26447   [1.628 sec/step, loss=0.73983, avg_loss=0.76670]\n",
            "Step 26448   [1.625 sec/step, loss=0.77177, avg_loss=0.76683]\n",
            "Step 26449   [1.614 sec/step, loss=0.75917, avg_loss=0.76760]\n",
            "Step 26450   [1.612 sec/step, loss=0.78275, avg_loss=0.76768]\n",
            "Step 26451   [1.616 sec/step, loss=0.78294, avg_loss=0.76762]\n",
            "Step 26452   [1.603 sec/step, loss=0.75381, avg_loss=0.76737]\n",
            "Step 26453   [1.598 sec/step, loss=0.74731, avg_loss=0.76714]\n",
            "Step 26454   [1.600 sec/step, loss=0.74838, avg_loss=0.76643]\n",
            "Step 26455   [1.602 sec/step, loss=0.78943, avg_loss=0.76626]\n",
            "Step 26456   [1.609 sec/step, loss=0.80474, avg_loss=0.76667]\n",
            "Step 26457   [1.613 sec/step, loss=0.77542, avg_loss=0.76732]\n",
            "Step 26458   [1.607 sec/step, loss=0.74556, avg_loss=0.76721]\n",
            "Step 26459   [1.608 sec/step, loss=0.75384, avg_loss=0.76676]\n",
            "Step 26460   [1.614 sec/step, loss=0.75637, avg_loss=0.76644]\n",
            "Step 26461   [1.616 sec/step, loss=0.73054, avg_loss=0.76617]\n",
            "Step 26462   [1.623 sec/step, loss=0.71269, avg_loss=0.76537]\n",
            "Step 26463   [1.631 sec/step, loss=0.76872, avg_loss=0.76505]\n",
            "Step 26464   [1.640 sec/step, loss=0.79541, avg_loss=0.76523]\n",
            "Step 26465   [1.638 sec/step, loss=0.81896, avg_loss=0.76555]\n",
            "Step 26466   [1.669 sec/step, loss=0.68191, avg_loss=0.76495]\n",
            "Step 26467   [1.659 sec/step, loss=0.74240, avg_loss=0.76478]\n",
            "Step 26468   [1.664 sec/step, loss=0.76368, avg_loss=0.76484]\n",
            "Step 26469   [1.677 sec/step, loss=0.73062, avg_loss=0.76424]\n",
            "Step 26470   [1.663 sec/step, loss=0.73903, avg_loss=0.76342]\n",
            "Step 26471   [1.661 sec/step, loss=0.72236, avg_loss=0.76328]\n",
            "Step 26472   [1.665 sec/step, loss=0.82699, avg_loss=0.76359]\n",
            "Step 26473   [1.669 sec/step, loss=0.71676, avg_loss=0.76290]\n",
            "Step 26474   [1.674 sec/step, loss=0.79227, avg_loss=0.76328]\n",
            "Step 26475   [1.673 sec/step, loss=0.81125, avg_loss=0.76407]\n",
            "Generated 32 batches of size 8 in 10.431 sec\n",
            "Step 26476   [1.663 sec/step, loss=0.70029, avg_loss=0.76306]\n",
            "Step 26477   [1.666 sec/step, loss=0.71733, avg_loss=0.76254]\n",
            "Step 26478   [1.668 sec/step, loss=0.77178, avg_loss=0.76198]\n",
            "Step 26479   [1.675 sec/step, loss=0.79307, avg_loss=0.76234]\n",
            "Step 26480   [1.672 sec/step, loss=0.75303, avg_loss=0.76217]\n",
            "Step 26481   [1.629 sec/step, loss=0.70397, avg_loss=0.76173]\n",
            "Step 26482   [1.607 sec/step, loss=0.82927, avg_loss=0.76255]\n",
            "Step 26483   [1.611 sec/step, loss=0.73566, avg_loss=0.76273]\n",
            "Step 26484   [1.608 sec/step, loss=0.74300, avg_loss=0.76272]\n",
            "Step 26485   [1.607 sec/step, loss=0.79088, avg_loss=0.76304]\n",
            "Step 26486   [1.590 sec/step, loss=0.75686, avg_loss=0.76339]\n",
            "Step 26487   [1.596 sec/step, loss=0.83868, avg_loss=0.76449]\n",
            "Step 26488   [1.600 sec/step, loss=0.70682, avg_loss=0.76375]\n",
            "Step 26489   [1.598 sec/step, loss=0.77906, avg_loss=0.76358]\n",
            "Step 26490   [1.591 sec/step, loss=0.78972, avg_loss=0.76445]\n",
            "Step 26491   [1.595 sec/step, loss=0.71070, avg_loss=0.76414]\n",
            "Step 26492   [1.595 sec/step, loss=0.77652, avg_loss=0.76399]\n",
            "Step 26493   [1.595 sec/step, loss=0.78320, avg_loss=0.76415]\n",
            "Step 26494   [1.598 sec/step, loss=0.79745, avg_loss=0.76426]\n",
            "Step 26495   [1.610 sec/step, loss=0.71884, avg_loss=0.76364]\n",
            "Step 26496   [1.619 sec/step, loss=0.82239, avg_loss=0.76427]\n",
            "Step 26497   [1.614 sec/step, loss=0.77480, avg_loss=0.76397]\n",
            "Step 26498   [1.607 sec/step, loss=0.77358, avg_loss=0.76399]\n",
            "Step 26499   [1.603 sec/step, loss=0.77844, avg_loss=0.76420]\n",
            "Step 26500   [1.600 sec/step, loss=0.76123, avg_loss=0.76379]\n",
            "Writing summary at step: 26500\n",
            "Step 26501   [1.624 sec/step, loss=0.75385, avg_loss=0.76358]\n",
            "Step 26502   [1.619 sec/step, loss=0.81548, avg_loss=0.76410]\n",
            "Step 26503   [1.613 sec/step, loss=0.74838, avg_loss=0.76410]\n",
            "Step 26504   [1.628 sec/step, loss=0.83579, avg_loss=0.76455]\n",
            "Generated 32 batches of size 8 in 10.538 sec\n",
            "Step 26505   [1.631 sec/step, loss=0.77571, avg_loss=0.76484]\n",
            "Step 26506   [1.639 sec/step, loss=0.79684, avg_loss=0.76503]\n",
            "Step 26507   [1.638 sec/step, loss=0.74729, avg_loss=0.76491]\n",
            "Step 26508   [1.642 sec/step, loss=0.79774, avg_loss=0.76550]\n",
            "Step 26509   [1.666 sec/step, loss=0.75153, avg_loss=0.76546]\n",
            "Step 26510   [1.656 sec/step, loss=0.80745, avg_loss=0.76588]\n",
            "Step 26511   [1.623 sec/step, loss=0.75296, avg_loss=0.76618]\n",
            "Step 26512   [1.625 sec/step, loss=0.75169, avg_loss=0.76599]\n",
            "Step 26513   [1.630 sec/step, loss=0.77705, avg_loss=0.76602]\n",
            "Step 26514   [1.632 sec/step, loss=0.77224, avg_loss=0.76605]\n",
            "Step 26515   [1.638 sec/step, loss=0.77257, avg_loss=0.76662]\n",
            "Step 26516   [1.638 sec/step, loss=0.79261, avg_loss=0.76685]\n",
            "Step 26517   [1.635 sec/step, loss=0.72694, avg_loss=0.76646]\n",
            "Step 26518   [1.631 sec/step, loss=0.76522, avg_loss=0.76632]\n",
            "Step 26519   [1.646 sec/step, loss=0.85333, avg_loss=0.76756]\n",
            "Step 26520   [1.653 sec/step, loss=0.81255, avg_loss=0.76720]\n",
            "Step 26521   [1.650 sec/step, loss=0.75083, avg_loss=0.76728]\n",
            "Step 26522   [1.651 sec/step, loss=0.78155, avg_loss=0.76713]\n",
            "Step 26523   [1.640 sec/step, loss=0.76279, avg_loss=0.76742]\n",
            "Step 26524   [1.634 sec/step, loss=0.74465, avg_loss=0.76668]\n",
            "Step 26525   [1.640 sec/step, loss=0.77558, avg_loss=0.76652]\n",
            "Step 26526   [1.678 sec/step, loss=0.72944, avg_loss=0.76618]\n",
            "Step 26527   [1.680 sec/step, loss=0.75510, avg_loss=0.76596]\n",
            "Step 26528   [1.682 sec/step, loss=0.82624, avg_loss=0.76743]\n",
            "Step 26529   [1.691 sec/step, loss=0.77873, avg_loss=0.76735]\n",
            "Step 26530   [1.700 sec/step, loss=0.69307, avg_loss=0.76622]\n",
            "Step 26531   [1.711 sec/step, loss=0.80268, avg_loss=0.76645]\n",
            "Step 26532   [1.712 sec/step, loss=0.76427, avg_loss=0.76630]\n",
            "Step 26533   [1.721 sec/step, loss=0.77127, avg_loss=0.76611]\n",
            "Step 26534   [1.722 sec/step, loss=0.76397, avg_loss=0.76656]\n",
            "Step 26535   [1.724 sec/step, loss=0.78443, avg_loss=0.76627]\n",
            "Step 26536   [1.731 sec/step, loss=0.79350, avg_loss=0.76674]\n",
            "Step 26537   [1.729 sec/step, loss=0.77731, avg_loss=0.76697]\n",
            "Step 26538   [1.701 sec/step, loss=0.78648, avg_loss=0.76733]\n",
            "Generated 32 batches of size 8 in 9.251 sec\n",
            "Step 26539   [1.699 sec/step, loss=0.77079, avg_loss=0.76716]\n",
            "Step 26540   [1.689 sec/step, loss=0.67806, avg_loss=0.76613]\n",
            "Step 26541   [1.691 sec/step, loss=0.81003, avg_loss=0.76653]\n",
            "Step 26542   [1.683 sec/step, loss=0.72075, avg_loss=0.76590]\n",
            "Step 26543   [1.682 sec/step, loss=0.73770, avg_loss=0.76581]\n",
            "Step 26544   [1.681 sec/step, loss=0.78610, avg_loss=0.76623]\n",
            "Step 26545   [1.643 sec/step, loss=0.79126, avg_loss=0.76736]\n",
            "Step 26546   [1.631 sec/step, loss=0.78162, avg_loss=0.76697]\n",
            "Step 26547   [1.630 sec/step, loss=0.73512, avg_loss=0.76692]\n",
            "Step 26548   [1.630 sec/step, loss=0.74656, avg_loss=0.76667]\n",
            "Step 26549   [1.628 sec/step, loss=0.77928, avg_loss=0.76687]\n",
            "Step 26550   [1.632 sec/step, loss=0.81065, avg_loss=0.76715]\n",
            "Step 26551   [1.630 sec/step, loss=0.79434, avg_loss=0.76726]\n",
            "Step 26552   [1.639 sec/step, loss=0.75858, avg_loss=0.76731]\n",
            "Step 26553   [1.653 sec/step, loss=0.71353, avg_loss=0.76697]\n",
            "Step 26554   [1.650 sec/step, loss=0.78105, avg_loss=0.76730]\n",
            "Step 26555   [1.647 sec/step, loss=0.78200, avg_loss=0.76723]\n",
            "Step 26556   [1.665 sec/step, loss=0.71973, avg_loss=0.76638]\n",
            "Step 26557   [1.659 sec/step, loss=0.73685, avg_loss=0.76599]\n",
            "Step 26558   [1.661 sec/step, loss=0.79434, avg_loss=0.76648]\n",
            "Step 26559   [1.663 sec/step, loss=0.77584, avg_loss=0.76670]\n",
            "Step 26560   [1.653 sec/step, loss=0.81038, avg_loss=0.76724]\n",
            "Step 26561   [1.651 sec/step, loss=0.75672, avg_loss=0.76750]\n",
            "Step 26562   [1.644 sec/step, loss=0.77429, avg_loss=0.76812]\n",
            "Step 26563   [1.642 sec/step, loss=0.72659, avg_loss=0.76769]\n",
            "Step 26564   [1.648 sec/step, loss=0.79625, avg_loss=0.76770]\n",
            "Step 26565   [1.659 sec/step, loss=0.71342, avg_loss=0.76665]\n",
            "Step 26566   [1.627 sec/step, loss=0.75636, avg_loss=0.76739]\n",
            "Step 26567   [1.642 sec/step, loss=0.73149, avg_loss=0.76728]\n",
            "Generated 32 batches of size 8 in 6.711 sec\n",
            "Step 26568   [1.639 sec/step, loss=0.71602, avg_loss=0.76681]\n",
            "Step 26569   [1.624 sec/step, loss=0.80811, avg_loss=0.76758]\n",
            "Step 26570   [1.632 sec/step, loss=0.77140, avg_loss=0.76791]\n",
            "Step 26571   [1.629 sec/step, loss=0.76177, avg_loss=0.76830]\n",
            "Step 26572   [1.620 sec/step, loss=0.74862, avg_loss=0.76752]\n",
            "Step 26573   [1.616 sec/step, loss=0.79910, avg_loss=0.76834]\n",
            "Step 26574   [1.611 sec/step, loss=0.79136, avg_loss=0.76833]\n",
            "Step 26575   [1.624 sec/step, loss=0.76590, avg_loss=0.76788]\n",
            "Step 26576   [1.623 sec/step, loss=0.69374, avg_loss=0.76781]\n",
            "Step 26577   [1.601 sec/step, loss=0.80915, avg_loss=0.76873]\n",
            "Step 26578   [1.598 sec/step, loss=0.80465, avg_loss=0.76906]\n",
            "Step 26579   [1.590 sec/step, loss=0.75506, avg_loss=0.76868]\n",
            "Step 26580   [1.589 sec/step, loss=0.71459, avg_loss=0.76829]\n",
            "Step 26581   [1.594 sec/step, loss=0.73959, avg_loss=0.76865]\n",
            "Step 26582   [1.594 sec/step, loss=0.78446, avg_loss=0.76820]\n",
            "Step 26583   [1.590 sec/step, loss=0.70604, avg_loss=0.76791]\n",
            "Step 26584   [1.600 sec/step, loss=0.75040, avg_loss=0.76798]\n",
            "Step 26585   [1.605 sec/step, loss=0.74543, avg_loss=0.76752]\n",
            "Step 26586   [1.606 sec/step, loss=0.74318, avg_loss=0.76739]\n",
            "Step 26587   [1.597 sec/step, loss=0.74359, avg_loss=0.76644]\n",
            "Step 26588   [1.595 sec/step, loss=0.74262, avg_loss=0.76679]\n",
            "Step 26589   [1.604 sec/step, loss=0.77818, avg_loss=0.76679]\n",
            "Step 26590   [1.607 sec/step, loss=0.79874, avg_loss=0.76688]\n",
            "Step 26591   [1.616 sec/step, loss=0.72349, avg_loss=0.76700]\n",
            "Step 26592   [1.613 sec/step, loss=0.75499, avg_loss=0.76679]\n",
            "Step 26593   [1.620 sec/step, loss=0.81068, avg_loss=0.76706]\n",
            "Step 26594   [1.618 sec/step, loss=0.75641, avg_loss=0.76665]\n",
            "Step 26595   [1.611 sec/step, loss=0.77504, avg_loss=0.76722]\n",
            "Step 26596   [1.603 sec/step, loss=0.74100, avg_loss=0.76640]\n",
            "Step 26597   [1.605 sec/step, loss=0.78701, avg_loss=0.76652]\n",
            "Step 26598   [1.615 sec/step, loss=0.72238, avg_loss=0.76601]\n",
            "Generated 32 batches of size 8 in 7.331 sec\n",
            "Step 26599   [1.657 sec/step, loss=0.61901, avg_loss=0.76442]\n",
            "Step 26600   [1.657 sec/step, loss=0.76740, avg_loss=0.76448]\n",
            "Writing summary at step: 26600\n",
            "Step 26601   [1.636 sec/step, loss=0.76662, avg_loss=0.76461]\n",
            "Step 26602   [1.632 sec/step, loss=0.77935, avg_loss=0.76425]\n",
            "Step 26603   [1.626 sec/step, loss=0.79815, avg_loss=0.76474]\n",
            "Step 26604   [1.618 sec/step, loss=0.71103, avg_loss=0.76350]\n",
            "Step 26605   [1.612 sec/step, loss=0.77288, avg_loss=0.76347]\n",
            "Step 26606   [1.613 sec/step, loss=0.79210, avg_loss=0.76342]\n",
            "Step 26607   [1.618 sec/step, loss=0.76986, avg_loss=0.76365]\n",
            "Step 26608   [1.614 sec/step, loss=0.76664, avg_loss=0.76333]\n",
            "Step 26609   [1.579 sec/step, loss=0.72905, avg_loss=0.76311]\n",
            "Step 26610   [1.582 sec/step, loss=0.80921, avg_loss=0.76313]\n",
            "Step 26611   [1.577 sec/step, loss=0.80791, avg_loss=0.76368]\n",
            "Step 26612   [1.577 sec/step, loss=0.77249, avg_loss=0.76388]\n",
            "Step 26613   [1.565 sec/step, loss=0.79297, avg_loss=0.76404]\n",
            "Step 26614   [1.563 sec/step, loss=0.77148, avg_loss=0.76404]\n",
            "Step 26615   [1.560 sec/step, loss=0.70941, avg_loss=0.76340]\n",
            "Step 26616   [1.566 sec/step, loss=0.75094, avg_loss=0.76299]\n",
            "Step 26617   [1.560 sec/step, loss=0.76006, avg_loss=0.76332]\n",
            "Step 26618   [1.559 sec/step, loss=0.71781, avg_loss=0.76285]\n",
            "Step 26619   [1.546 sec/step, loss=0.71750, avg_loss=0.76149]\n",
            "Step 26620   [1.536 sec/step, loss=0.71247, avg_loss=0.76049]\n",
            "Step 26621   [1.541 sec/step, loss=0.72672, avg_loss=0.76025]\n",
            "Step 26622   [1.541 sec/step, loss=0.74848, avg_loss=0.75991]\n",
            "Step 26623   [1.559 sec/step, loss=0.80408, avg_loss=0.76033]\n",
            "Step 26624   [1.599 sec/step, loss=0.71074, avg_loss=0.75999]\n",
            "Step 26625   [1.596 sec/step, loss=0.71790, avg_loss=0.75941]\n",
            "Step 26626   [1.562 sec/step, loss=0.81931, avg_loss=0.76031]\n",
            "Step 26627   [1.557 sec/step, loss=0.76532, avg_loss=0.76041]\n",
            "Step 26628   [1.564 sec/step, loss=0.81302, avg_loss=0.76028]\n",
            "Step 26629   [1.557 sec/step, loss=0.72140, avg_loss=0.75971]\n",
            "Step 26630   [1.549 sec/step, loss=0.79375, avg_loss=0.76071]\n",
            "Step 26631   [1.542 sec/step, loss=0.81310, avg_loss=0.76082]\n",
            "Generated 32 batches of size 8 in 7.622 sec\n",
            "Step 26632   [1.544 sec/step, loss=0.76604, avg_loss=0.76084]\n",
            "Step 26633   [1.543 sec/step, loss=0.75690, avg_loss=0.76069]\n",
            "Step 26634   [1.543 sec/step, loss=0.71220, avg_loss=0.76017]\n",
            "Step 26635   [1.555 sec/step, loss=0.66661, avg_loss=0.75900]\n",
            "Step 26636   [1.552 sec/step, loss=0.78618, avg_loss=0.75892]\n",
            "Step 26637   [1.558 sec/step, loss=0.73160, avg_loss=0.75847]\n",
            "Step 26638   [1.557 sec/step, loss=0.70034, avg_loss=0.75760]\n",
            "Step 26639   [1.555 sec/step, loss=0.77194, avg_loss=0.75762]\n",
            "Step 26640   [1.575 sec/step, loss=0.77316, avg_loss=0.75857]\n",
            "Step 26641   [1.574 sec/step, loss=0.75712, avg_loss=0.75804]\n",
            "Step 26642   [1.568 sec/step, loss=0.70919, avg_loss=0.75792]\n",
            "Step 26643   [1.576 sec/step, loss=0.77912, avg_loss=0.75834]\n",
            "Step 26644   [1.582 sec/step, loss=0.74326, avg_loss=0.75791]\n",
            "Step 26645   [1.587 sec/step, loss=0.76798, avg_loss=0.75768]\n",
            "Step 26646   [1.590 sec/step, loss=0.78368, avg_loss=0.75770]\n",
            "Step 26647   [1.593 sec/step, loss=0.75738, avg_loss=0.75792]\n",
            "Step 26648   [1.603 sec/step, loss=0.74285, avg_loss=0.75788]\n",
            "Step 26649   [1.601 sec/step, loss=0.80822, avg_loss=0.75817]\n",
            "Step 26650   [1.597 sec/step, loss=0.74754, avg_loss=0.75754]\n",
            "Step 26651   [1.595 sec/step, loss=0.75953, avg_loss=0.75719]\n",
            "Step 26652   [1.589 sec/step, loss=0.79350, avg_loss=0.75754]\n",
            "Step 26653   [1.579 sec/step, loss=0.80680, avg_loss=0.75847]\n",
            "Step 26654   [1.596 sec/step, loss=0.78159, avg_loss=0.75848]\n",
            "Step 26655   [1.603 sec/step, loss=0.69233, avg_loss=0.75758]\n",
            "Step 26656   [1.579 sec/step, loss=0.83005, avg_loss=0.75868]\n",
            "Step 26657   [1.616 sec/step, loss=0.69761, avg_loss=0.75829]\n",
            "Step 26658   [1.613 sec/step, loss=0.81111, avg_loss=0.75846]\n",
            "Step 26659   [1.618 sec/step, loss=0.77604, avg_loss=0.75846]\n",
            "Step 26660   [1.623 sec/step, loss=0.76029, avg_loss=0.75796]\n",
            "Step 26661   [1.626 sec/step, loss=0.71131, avg_loss=0.75751]\n",
            "Step 26662   [1.635 sec/step, loss=0.74607, avg_loss=0.75723]\n",
            "Step 26663   [1.639 sec/step, loss=0.76144, avg_loss=0.75757]\n",
            "Step 26664   [1.625 sec/step, loss=0.74169, avg_loss=0.75703]\n",
            "Generated 32 batches of size 8 in 7.806 sec\n",
            "Step 26665   [1.617 sec/step, loss=0.79970, avg_loss=0.75789]\n",
            "Step 26666   [1.607 sec/step, loss=0.77609, avg_loss=0.75809]\n",
            "Step 26667   [1.596 sec/step, loss=0.80227, avg_loss=0.75880]\n",
            "Step 26668   [1.599 sec/step, loss=0.76053, avg_loss=0.75924]\n",
            "Step 26669   [1.598 sec/step, loss=0.80786, avg_loss=0.75924]\n",
            "Step 26670   [1.586 sec/step, loss=0.79063, avg_loss=0.75943]\n",
            "Step 26671   [1.589 sec/step, loss=0.78037, avg_loss=0.75962]\n",
            "Step 26672   [1.594 sec/step, loss=0.76171, avg_loss=0.75975]\n",
            "Step 26673   [1.593 sec/step, loss=0.76678, avg_loss=0.75942]\n",
            "Step 26674   [1.594 sec/step, loss=0.71696, avg_loss=0.75868]\n",
            "Step 26675   [1.581 sec/step, loss=0.73969, avg_loss=0.75842]\n",
            "Step 26676   [1.582 sec/step, loss=0.76714, avg_loss=0.75915]\n",
            "Step 26677   [1.583 sec/step, loss=0.79587, avg_loss=0.75902]\n",
            "Step 26678   [1.585 sec/step, loss=0.84147, avg_loss=0.75939]\n",
            "Step 26679   [1.591 sec/step, loss=0.73551, avg_loss=0.75919]\n",
            "Step 26680   [1.609 sec/step, loss=0.74263, avg_loss=0.75947]\n",
            "Step 26681   [1.607 sec/step, loss=0.77911, avg_loss=0.75987]\n",
            "Step 26682   [1.613 sec/step, loss=0.73393, avg_loss=0.75936]\n",
            "Step 26683   [1.611 sec/step, loss=0.74752, avg_loss=0.75978]\n",
            "Step 26684   [1.603 sec/step, loss=0.73023, avg_loss=0.75958]\n",
            "Step 26685   [1.605 sec/step, loss=0.75484, avg_loss=0.75967]\n",
            "Step 26686   [1.600 sec/step, loss=0.73837, avg_loss=0.75962]\n",
            "Step 26687   [1.607 sec/step, loss=0.76529, avg_loss=0.75984]\n",
            "Step 26688   [1.605 sec/step, loss=0.79816, avg_loss=0.76039]\n",
            "Step 26689   [1.594 sec/step, loss=0.78551, avg_loss=0.76047]\n",
            "Step 26690   [1.590 sec/step, loss=0.77472, avg_loss=0.76023]\n",
            "Step 26691   [1.579 sec/step, loss=0.73832, avg_loss=0.76038]\n",
            "Step 26692   [1.582 sec/step, loss=0.72954, avg_loss=0.76012]\n",
            "Step 26693   [1.582 sec/step, loss=0.76157, avg_loss=0.75963]\n",
            "Step 26694   [1.588 sec/step, loss=0.78148, avg_loss=0.75988]\n",
            "Step 26695   [1.598 sec/step, loss=0.82411, avg_loss=0.76037]\n",
            "Generated 32 batches of size 8 in 8.642 sec\n",
            "Step 26696   [1.615 sec/step, loss=0.72314, avg_loss=0.76019]\n",
            "Step 26697   [1.647 sec/step, loss=0.61865, avg_loss=0.75851]\n",
            "Step 26698   [1.639 sec/step, loss=0.79018, avg_loss=0.75919]\n",
            "Step 26699   [1.601 sec/step, loss=0.78589, avg_loss=0.76086]\n",
            "Step 26700   [1.604 sec/step, loss=0.79263, avg_loss=0.76111]\n",
            "Writing summary at step: 26700\n",
            "Step 26701   [1.599 sec/step, loss=0.76324, avg_loss=0.76107]\n",
            "Step 26702   [1.615 sec/step, loss=0.70164, avg_loss=0.76030]\n",
            "Step 26703   [1.614 sec/step, loss=0.79533, avg_loss=0.76027]\n",
            "Step 26704   [1.612 sec/step, loss=0.76249, avg_loss=0.76078]\n",
            "Step 26705   [1.610 sec/step, loss=0.78865, avg_loss=0.76094]\n",
            "Step 26706   [1.606 sec/step, loss=0.76865, avg_loss=0.76071]\n",
            "Step 26707   [1.600 sec/step, loss=0.77282, avg_loss=0.76074]\n",
            "Step 26708   [1.607 sec/step, loss=0.76447, avg_loss=0.76072]\n",
            "Step 26709   [1.602 sec/step, loss=0.82730, avg_loss=0.76170]\n",
            "Step 26710   [1.602 sec/step, loss=0.73598, avg_loss=0.76097]\n",
            "Step 26711   [1.602 sec/step, loss=0.76478, avg_loss=0.76053]\n",
            "Step 26712   [1.601 sec/step, loss=0.78243, avg_loss=0.76063]\n",
            "Step 26713   [1.606 sec/step, loss=0.76973, avg_loss=0.76040]\n",
            "Step 26714   [1.646 sec/step, loss=0.58425, avg_loss=0.75853]\n",
            "Step 26715   [1.644 sec/step, loss=0.78161, avg_loss=0.75925]\n",
            "Step 26716   [1.642 sec/step, loss=0.80309, avg_loss=0.75977]\n",
            "Step 26717   [1.648 sec/step, loss=0.78432, avg_loss=0.76001]\n",
            "Step 26718   [1.661 sec/step, loss=0.73642, avg_loss=0.76020]\n",
            "Step 26719   [1.655 sec/step, loss=0.75918, avg_loss=0.76062]\n",
            "Step 26720   [1.651 sec/step, loss=0.73739, avg_loss=0.76087]\n",
            "Step 26721   [1.646 sec/step, loss=0.78285, avg_loss=0.76143]\n",
            "Step 26722   [1.645 sec/step, loss=0.78493, avg_loss=0.76179]\n",
            "Step 26723   [1.622 sec/step, loss=0.77567, avg_loss=0.76151]\n",
            "Step 26724   [1.587 sec/step, loss=0.69270, avg_loss=0.76133]\n",
            "Step 26725   [1.601 sec/step, loss=0.72139, avg_loss=0.76136]\n",
            "Step 26726   [1.606 sec/step, loss=0.73440, avg_loss=0.76051]\n",
            "Generated 32 batches of size 8 in 9.607 sec\n",
            "Step 26727   [1.615 sec/step, loss=0.77571, avg_loss=0.76062]\n",
            "Step 26728   [1.610 sec/step, loss=0.77242, avg_loss=0.76021]\n",
            "Step 26729   [1.609 sec/step, loss=0.76097, avg_loss=0.76061]\n",
            "Step 26730   [1.618 sec/step, loss=0.78102, avg_loss=0.76048]\n",
            "Step 26731   [1.615 sec/step, loss=0.75784, avg_loss=0.75993]\n",
            "Step 26732   [1.612 sec/step, loss=0.79358, avg_loss=0.76020]\n",
            "Step 26733   [1.608 sec/step, loss=0.73777, avg_loss=0.76001]\n",
            "Step 26734   [1.623 sec/step, loss=0.77877, avg_loss=0.76068]\n",
            "Step 26735   [1.613 sec/step, loss=0.73946, avg_loss=0.76141]\n",
            "Step 26736   [1.611 sec/step, loss=0.77334, avg_loss=0.76128]\n",
            "Step 26737   [1.608 sec/step, loss=0.79809, avg_loss=0.76194]\n",
            "Step 26738   [1.609 sec/step, loss=0.72503, avg_loss=0.76219]\n",
            "Step 26739   [1.612 sec/step, loss=0.79250, avg_loss=0.76239]\n",
            "Step 26740   [1.592 sec/step, loss=0.76665, avg_loss=0.76233]\n",
            "Step 26741   [1.589 sec/step, loss=0.78920, avg_loss=0.76265]\n",
            "Step 26742   [1.596 sec/step, loss=0.80976, avg_loss=0.76366]\n",
            "Step 26743   [1.589 sec/step, loss=0.80073, avg_loss=0.76387]\n",
            "Step 26744   [1.586 sec/step, loss=0.76441, avg_loss=0.76408]\n",
            "Step 26745   [1.578 sec/step, loss=0.75836, avg_loss=0.76399]\n",
            "Step 26746   [1.575 sec/step, loss=0.79858, avg_loss=0.76414]\n",
            "Step 26747   [1.569 sec/step, loss=0.74858, avg_loss=0.76405]\n",
            "Step 26748   [1.555 sec/step, loss=0.73823, avg_loss=0.76400]\n",
            "Step 26749   [1.565 sec/step, loss=0.74802, avg_loss=0.76340]\n",
            "Step 26750   [1.565 sec/step, loss=0.72110, avg_loss=0.76314]\n",
            "Step 26751   [1.579 sec/step, loss=0.78195, avg_loss=0.76336]\n",
            "Step 26752   [1.577 sec/step, loss=0.81997, avg_loss=0.76363]\n",
            "Step 26753   [1.581 sec/step, loss=0.74859, avg_loss=0.76304]\n",
            "Step 26754   [1.573 sec/step, loss=0.75886, avg_loss=0.76282]\n",
            "Step 26755   [1.573 sec/step, loss=0.78704, avg_loss=0.76376]\n",
            "Step 26756   [1.579 sec/step, loss=0.81154, avg_loss=0.76358]\n",
            "Step 26757   [1.569 sec/step, loss=0.76229, avg_loss=0.76422]\n",
            "Step 26758   [1.576 sec/step, loss=0.79783, avg_loss=0.76409]\n",
            "Generated 32 batches of size 8 in 10.314 sec\n",
            "Step 26759   [1.603 sec/step, loss=0.77497, avg_loss=0.76408]\n",
            "Step 26760   [1.600 sec/step, loss=0.81644, avg_loss=0.76464]\n",
            "Step 26761   [1.601 sec/step, loss=0.72274, avg_loss=0.76476]\n",
            "Step 26762   [1.589 sec/step, loss=0.71668, avg_loss=0.76446]\n",
            "Step 26763   [1.588 sec/step, loss=0.83171, avg_loss=0.76517]\n",
            "Step 26764   [1.587 sec/step, loss=0.80803, avg_loss=0.76583]\n",
            "Step 26765   [1.583 sec/step, loss=0.72979, avg_loss=0.76513]\n",
            "Step 26766   [1.601 sec/step, loss=0.78050, avg_loss=0.76517]\n",
            "Step 26767   [1.602 sec/step, loss=0.71365, avg_loss=0.76429]\n",
            "Step 26768   [1.599 sec/step, loss=0.83072, avg_loss=0.76499]\n",
            "Step 26769   [1.631 sec/step, loss=0.82262, avg_loss=0.76514]\n",
            "Step 26770   [1.642 sec/step, loss=0.83125, avg_loss=0.76554]\n",
            "Step 26771   [1.639 sec/step, loss=0.77292, avg_loss=0.76547]\n",
            "Step 26772   [1.635 sec/step, loss=0.79335, avg_loss=0.76579]\n",
            "Step 26773   [1.642 sec/step, loss=0.81729, avg_loss=0.76629]\n",
            "Step 26774   [1.657 sec/step, loss=0.75374, avg_loss=0.76666]\n",
            "Step 26775   [1.674 sec/step, loss=0.86941, avg_loss=0.76796]\n",
            "Step 26776   [1.680 sec/step, loss=0.73829, avg_loss=0.76767]\n",
            "Step 26777   [1.684 sec/step, loss=0.79963, avg_loss=0.76770]\n",
            "Step 26778   [1.681 sec/step, loss=0.70000, avg_loss=0.76629]\n",
            "Step 26779   [1.675 sec/step, loss=0.78037, avg_loss=0.76674]\n",
            "Step 26780   [1.653 sec/step, loss=0.77329, avg_loss=0.76705]\n",
            "Step 26781   [1.656 sec/step, loss=0.76929, avg_loss=0.76695]\n",
            "Step 26782   [1.647 sec/step, loss=0.77570, avg_loss=0.76736]\n",
            "Step 26783   [1.651 sec/step, loss=0.76230, avg_loss=0.76751]\n",
            "Step 26784   [1.657 sec/step, loss=0.77020, avg_loss=0.76791]\n",
            "Step 26785   [1.655 sec/step, loss=0.84274, avg_loss=0.76879]\n",
            "Step 26786   [1.662 sec/step, loss=0.75562, avg_loss=0.76896]\n",
            "Step 26787   [1.660 sec/step, loss=0.85839, avg_loss=0.76989]\n",
            "Step 26788   [1.663 sec/step, loss=0.75083, avg_loss=0.76942]\n",
            "Step 26789   [1.663 sec/step, loss=0.78853, avg_loss=0.76945]\n",
            "Step 26790   [1.669 sec/step, loss=0.86451, avg_loss=0.77035]\n",
            "Step 26791   [1.668 sec/step, loss=0.78548, avg_loss=0.77082]\n",
            "Step 26792   [1.671 sec/step, loss=0.76750, avg_loss=0.77120]\n",
            "Step 26793   [1.671 sec/step, loss=0.78017, avg_loss=0.77139]\n",
            "Generated 32 batches of size 8 in 10.513 sec\n",
            "Step 26794   [1.673 sec/step, loss=0.70133, avg_loss=0.77059]\n",
            "Step 26795   [1.656 sec/step, loss=0.79066, avg_loss=0.77025]\n",
            "Step 26796   [1.641 sec/step, loss=0.73304, avg_loss=0.77035]\n",
            "Step 26797   [1.602 sec/step, loss=0.75086, avg_loss=0.77167]\n",
            "Step 26798   [1.603 sec/step, loss=0.71287, avg_loss=0.77090]\n",
            "Step 26799   [1.618 sec/step, loss=0.72143, avg_loss=0.77025]\n",
            "Step 26800   [1.630 sec/step, loss=0.79134, avg_loss=0.77024]\n",
            "Writing summary at step: 26800\n",
            "Step 26801   [1.636 sec/step, loss=0.76944, avg_loss=0.77030]\n",
            "Step 26802   [1.622 sec/step, loss=0.80479, avg_loss=0.77133]\n",
            "Step 26803   [1.620 sec/step, loss=0.76678, avg_loss=0.77105]\n",
            "Step 26804   [1.613 sec/step, loss=0.77653, avg_loss=0.77119]\n",
            "Step 26805   [1.612 sec/step, loss=0.73306, avg_loss=0.77063]\n",
            "Step 26806   [1.614 sec/step, loss=0.77071, avg_loss=0.77065]\n",
            "Step 26807   [1.626 sec/step, loss=0.84717, avg_loss=0.77140]\n",
            "Step 26808   [1.620 sec/step, loss=0.76041, avg_loss=0.77136]\n",
            "Step 26809   [1.626 sec/step, loss=0.81704, avg_loss=0.77125]\n",
            "Step 26810   [1.626 sec/step, loss=0.77445, avg_loss=0.77164]\n",
            "Step 26811   [1.625 sec/step, loss=0.80108, avg_loss=0.77200]\n",
            "Step 26812   [1.628 sec/step, loss=0.76009, avg_loss=0.77178]\n",
            "Step 26813   [1.628 sec/step, loss=0.79942, avg_loss=0.77208]\n",
            "Step 26814   [1.591 sec/step, loss=0.79010, avg_loss=0.77413]\n",
            "Step 26815   [1.587 sec/step, loss=0.76908, avg_loss=0.77401]\n",
            "Step 26816   [1.587 sec/step, loss=0.77112, avg_loss=0.77369]\n",
            "Step 26817   [1.590 sec/step, loss=0.76155, avg_loss=0.77346]\n",
            "Step 26818   [1.594 sec/step, loss=0.79406, avg_loss=0.77404]\n",
            "Step 26819   [1.596 sec/step, loss=0.80638, avg_loss=0.77451]\n",
            "Step 26820   [1.631 sec/step, loss=0.85245, avg_loss=0.77566]\n",
            "Generated 32 batches of size 8 in 10.047 sec\n",
            "Step 26821   [1.642 sec/step, loss=0.76481, avg_loss=0.77548]\n",
            "Step 26822   [1.678 sec/step, loss=0.76080, avg_loss=0.77524]\n",
            "Step 26823   [1.682 sec/step, loss=0.77427, avg_loss=0.77522]\n",
            "Step 26824   [1.678 sec/step, loss=0.83583, avg_loss=0.77666]\n",
            "Step 26825   [1.662 sec/step, loss=0.74168, avg_loss=0.77686]\n",
            "Step 26826   [1.648 sec/step, loss=0.73216, avg_loss=0.77684]\n",
            "Step 26827   [1.645 sec/step, loss=0.81187, avg_loss=0.77720]\n",
            "Step 26828   [1.660 sec/step, loss=0.74585, avg_loss=0.77693]\n",
            "Step 26829   [1.706 sec/step, loss=0.78228, avg_loss=0.77715]\n",
            "Step 26830   [1.690 sec/step, loss=0.79780, avg_loss=0.77731]\n",
            "Step 26831   [1.690 sec/step, loss=0.77090, avg_loss=0.77744]\n",
            "Step 26832   [1.699 sec/step, loss=0.80252, avg_loss=0.77753]\n",
            "Step 26833   [1.695 sec/step, loss=0.79755, avg_loss=0.77813]\n",
            "Step 26834   [1.680 sec/step, loss=0.75458, avg_loss=0.77789]\n",
            "Step 26835   [1.676 sec/step, loss=0.80424, avg_loss=0.77854]\n",
            "Step 26836   [1.677 sec/step, loss=0.79914, avg_loss=0.77880]\n",
            "Step 26837   [1.676 sec/step, loss=0.81483, avg_loss=0.77896]\n",
            "Step 26838   [1.674 sec/step, loss=0.73406, avg_loss=0.77905]\n",
            "Step 26839   [1.683 sec/step, loss=0.79213, avg_loss=0.77905]\n",
            "Step 26840   [1.683 sec/step, loss=0.81786, avg_loss=0.77956]\n",
            "Step 26841   [1.686 sec/step, loss=0.78060, avg_loss=0.77948]\n",
            "Step 26842   [1.697 sec/step, loss=0.78204, avg_loss=0.77920]\n",
            "Step 26843   [1.703 sec/step, loss=0.81316, avg_loss=0.77932]\n",
            "Step 26844   [1.700 sec/step, loss=0.77598, avg_loss=0.77944]\n",
            "Step 26845   [1.701 sec/step, loss=0.79897, avg_loss=0.77984]\n",
            "Step 26846   [1.703 sec/step, loss=0.81462, avg_loss=0.78000]\n",
            "Step 26847   [1.702 sec/step, loss=0.76618, avg_loss=0.78018]\n",
            "Step 26848   [1.704 sec/step, loss=0.80827, avg_loss=0.78088]\n",
            "Step 26849   [1.692 sec/step, loss=0.79928, avg_loss=0.78139]\n",
            "Step 26850   [1.698 sec/step, loss=0.77607, avg_loss=0.78194]\n",
            "Step 26851   [1.686 sec/step, loss=0.80116, avg_loss=0.78214]\n",
            "Step 26852   [1.694 sec/step, loss=0.82727, avg_loss=0.78221]\n",
            "Step 26853   [1.693 sec/step, loss=0.74251, avg_loss=0.78215]\n",
            "Step 26854   [1.687 sec/step, loss=0.79991, avg_loss=0.78256]\n",
            "Generated 32 batches of size 8 in 10.134 sec\n",
            "Step 26855   [1.698 sec/step, loss=0.80484, avg_loss=0.78274]\n",
            "Step 26856   [1.693 sec/step, loss=0.73444, avg_loss=0.78196]\n",
            "Step 26857   [1.665 sec/step, loss=0.74273, avg_loss=0.78177]\n",
            "Step 26858   [1.662 sec/step, loss=0.80586, avg_loss=0.78185]\n",
            "Step 26859   [1.627 sec/step, loss=0.73184, avg_loss=0.78142]\n",
            "Step 26860   [1.627 sec/step, loss=0.81663, avg_loss=0.78142]\n",
            "Step 26861   [1.626 sec/step, loss=0.82210, avg_loss=0.78241]\n",
            "Step 26862   [1.645 sec/step, loss=0.77685, avg_loss=0.78302]\n",
            "Step 26863   [1.639 sec/step, loss=0.79203, avg_loss=0.78262]\n",
            "Step 26864   [1.650 sec/step, loss=0.78861, avg_loss=0.78242]\n",
            "Step 26865   [1.650 sec/step, loss=0.77581, avg_loss=0.78288]\n",
            "Step 26866   [1.638 sec/step, loss=0.78020, avg_loss=0.78288]\n",
            "Step 26867   [1.640 sec/step, loss=0.79451, avg_loss=0.78369]\n",
            "Step 26868   [1.647 sec/step, loss=0.83215, avg_loss=0.78370]\n",
            "Step 26869   [1.615 sec/step, loss=0.76087, avg_loss=0.78309]\n",
            "Step 26870   [1.606 sec/step, loss=0.78619, avg_loss=0.78264]\n",
            "Step 26871   [1.607 sec/step, loss=0.78688, avg_loss=0.78278]\n",
            "Step 26872   [1.604 sec/step, loss=0.72976, avg_loss=0.78214]\n",
            "Step 26873   [1.598 sec/step, loss=0.77193, avg_loss=0.78169]\n",
            "Step 26874   [1.591 sec/step, loss=0.79171, avg_loss=0.78207]\n",
            "Step 26875   [1.608 sec/step, loss=0.75682, avg_loss=0.78094]\n",
            "Step 26876   [1.598 sec/step, loss=0.74671, avg_loss=0.78102]\n",
            "Step 26877   [1.608 sec/step, loss=0.71570, avg_loss=0.78019]\n",
            "Step 26878   [1.622 sec/step, loss=0.75822, avg_loss=0.78077]\n",
            "Step 26879   [1.622 sec/step, loss=0.78048, avg_loss=0.78077]\n",
            "Step 26880   [1.626 sec/step, loss=0.75664, avg_loss=0.78060]\n",
            "Step 26881   [1.626 sec/step, loss=0.76051, avg_loss=0.78051]\n",
            "Step 26882   [1.632 sec/step, loss=0.75648, avg_loss=0.78032]\n",
            "Step 26883   [1.632 sec/step, loss=0.72055, avg_loss=0.77990]\n",
            "Step 26884   [1.628 sec/step, loss=0.81981, avg_loss=0.78040]\n",
            "Step 26885   [1.625 sec/step, loss=0.75242, avg_loss=0.77950]\n",
            "Step 26886   [1.622 sec/step, loss=0.81128, avg_loss=0.78005]\n",
            "Step 26887   [1.618 sec/step, loss=0.80316, avg_loss=0.77950]\n",
            "Step 26888   [1.624 sec/step, loss=0.80040, avg_loss=0.78000]\n",
            "Generated 32 batches of size 8 in 10.424 sec\n",
            "Step 26889   [1.625 sec/step, loss=0.71253, avg_loss=0.77924]\n",
            "Step 26890   [1.627 sec/step, loss=0.79868, avg_loss=0.77858]\n",
            "Step 26891   [1.630 sec/step, loss=0.72999, avg_loss=0.77802]\n",
            "Step 26892   [1.641 sec/step, loss=0.72934, avg_loss=0.77764]\n",
            "Step 26893   [1.636 sec/step, loss=0.78371, avg_loss=0.77768]\n",
            "Step 26894   [1.630 sec/step, loss=0.78429, avg_loss=0.77851]\n",
            "Step 26895   [1.636 sec/step, loss=0.76811, avg_loss=0.77828]\n",
            "Step 26896   [1.630 sec/step, loss=0.79599, avg_loss=0.77891]\n",
            "Step 26897   [1.635 sec/step, loss=0.79987, avg_loss=0.77940]\n",
            "Step 26898   [1.640 sec/step, loss=0.80305, avg_loss=0.78030]\n",
            "Step 26899   [1.620 sec/step, loss=0.72463, avg_loss=0.78034]\n",
            "Step 26900   [1.629 sec/step, loss=0.79733, avg_loss=0.78040]\n",
            "Writing summary at step: 26900\n",
            "Step 26901   [1.630 sec/step, loss=0.77417, avg_loss=0.78044]\n",
            "Step 26902   [1.631 sec/step, loss=0.75424, avg_loss=0.77994]\n",
            "Step 26903   [1.635 sec/step, loss=0.79042, avg_loss=0.78017]\n",
            "Step 26904   [1.634 sec/step, loss=0.73151, avg_loss=0.77972]\n",
            "Step 26905   [1.640 sec/step, loss=0.74250, avg_loss=0.77982]\n",
            "Step 26906   [1.643 sec/step, loss=0.82564, avg_loss=0.78037]\n",
            "Step 26907   [1.638 sec/step, loss=0.80566, avg_loss=0.77995]\n",
            "Step 26908   [1.636 sec/step, loss=0.79439, avg_loss=0.78029]\n",
            "Step 26909   [1.631 sec/step, loss=0.73700, avg_loss=0.77949]\n",
            "Step 26910   [1.631 sec/step, loss=0.80061, avg_loss=0.77975]\n",
            "Step 26911   [1.635 sec/step, loss=0.78702, avg_loss=0.77961]\n",
            "Step 26912   [1.634 sec/step, loss=0.75688, avg_loss=0.77958]\n",
            "Step 26913   [1.685 sec/step, loss=0.66946, avg_loss=0.77828]\n",
            "Step 26914   [1.685 sec/step, loss=0.72638, avg_loss=0.77764]\n",
            "Step 26915   [1.688 sec/step, loss=0.74045, avg_loss=0.77736]\n",
            "Generated 32 batches of size 8 in 9.918 sec\n",
            "Step 26916   [1.693 sec/step, loss=0.72590, avg_loss=0.77691]\n",
            "Step 26917   [1.687 sec/step, loss=0.80112, avg_loss=0.77730]\n",
            "Step 26918   [1.681 sec/step, loss=0.79911, avg_loss=0.77735]\n",
            "Step 26919   [1.676 sec/step, loss=0.78921, avg_loss=0.77718]\n",
            "Step 26920   [1.646 sec/step, loss=0.67994, avg_loss=0.77545]\n",
            "Step 26921   [1.643 sec/step, loss=0.78401, avg_loss=0.77565]\n",
            "Step 26922   [1.613 sec/step, loss=0.73870, avg_loss=0.77543]\n",
            "Step 26923   [1.614 sec/step, loss=0.79237, avg_loss=0.77561]\n",
            "Step 26924   [1.630 sec/step, loss=0.80829, avg_loss=0.77533]\n",
            "Step 26925   [1.627 sec/step, loss=0.76692, avg_loss=0.77558]\n",
            "Step 26926   [1.628 sec/step, loss=0.76198, avg_loss=0.77588]\n",
            "Step 26927   [1.623 sec/step, loss=0.77687, avg_loss=0.77553]\n",
            "Step 26928   [1.622 sec/step, loss=0.76857, avg_loss=0.77576]\n",
            "Step 26929   [1.586 sec/step, loss=0.79911, avg_loss=0.77593]\n",
            "Step 26930   [1.586 sec/step, loss=0.83771, avg_loss=0.77633]\n",
            "Step 26931   [1.618 sec/step, loss=0.68552, avg_loss=0.77547]\n",
            "Step 26932   [1.610 sec/step, loss=0.76533, avg_loss=0.77510]\n",
            "Step 26933   [1.608 sec/step, loss=0.76279, avg_loss=0.77475]\n",
            "Step 26934   [1.606 sec/step, loss=0.79421, avg_loss=0.77515]\n",
            "Step 26935   [1.608 sec/step, loss=0.78087, avg_loss=0.77492]\n",
            "Step 26936   [1.609 sec/step, loss=0.75065, avg_loss=0.77443]\n",
            "Step 26937   [1.607 sec/step, loss=0.76382, avg_loss=0.77392]\n",
            "Step 26938   [1.609 sec/step, loss=0.80058, avg_loss=0.77459]\n",
            "Step 26939   [1.604 sec/step, loss=0.75972, avg_loss=0.77426]\n",
            "Step 26940   [1.598 sec/step, loss=0.74441, avg_loss=0.77353]\n",
            "Step 26941   [1.599 sec/step, loss=0.79245, avg_loss=0.77365]\n",
            "Step 26942   [1.593 sec/step, loss=0.75944, avg_loss=0.77342]\n",
            "Step 26943   [1.589 sec/step, loss=0.75976, avg_loss=0.77289]\n",
            "Step 26944   [1.591 sec/step, loss=0.77047, avg_loss=0.77283]\n",
            "Step 26945   [1.594 sec/step, loss=0.83573, avg_loss=0.77320]\n",
            "Step 26946   [1.602 sec/step, loss=0.72139, avg_loss=0.77227]\n",
            "Step 26947   [1.605 sec/step, loss=0.79741, avg_loss=0.77258]\n",
            "Step 26948   [1.606 sec/step, loss=0.80427, avg_loss=0.77254]\n",
            "Step 26949   [1.619 sec/step, loss=0.80265, avg_loss=0.77257]\n",
            "Generated 32 batches of size 8 in 8.329 sec\n",
            "Step 26950   [1.613 sec/step, loss=0.78994, avg_loss=0.77271]\n",
            "Step 26951   [1.622 sec/step, loss=0.76876, avg_loss=0.77239]\n",
            "Step 26952   [1.612 sec/step, loss=0.77566, avg_loss=0.77187]\n",
            "Step 26953   [1.608 sec/step, loss=0.75297, avg_loss=0.77197]\n",
            "Step 26954   [1.606 sec/step, loss=0.77104, avg_loss=0.77169]\n",
            "Step 26955   [1.628 sec/step, loss=0.71820, avg_loss=0.77082]\n",
            "Step 26956   [1.624 sec/step, loss=0.74626, avg_loss=0.77094]\n",
            "Step 26957   [1.623 sec/step, loss=0.78125, avg_loss=0.77132]\n",
            "Step 26958   [1.621 sec/step, loss=0.75170, avg_loss=0.77078]\n",
            "Step 26959   [1.619 sec/step, loss=0.81259, avg_loss=0.77159]\n",
            "Step 26960   [1.617 sec/step, loss=0.79660, avg_loss=0.77139]\n",
            "Step 26961   [1.621 sec/step, loss=0.80656, avg_loss=0.77123]\n",
            "Step 26962   [1.608 sec/step, loss=0.72358, avg_loss=0.77070]\n",
            "Step 26963   [1.612 sec/step, loss=0.77599, avg_loss=0.77054]\n",
            "Step 26964   [1.606 sec/step, loss=0.77434, avg_loss=0.77040]\n",
            "Step 26965   [1.610 sec/step, loss=0.69140, avg_loss=0.76955]\n",
            "Step 26966   [1.613 sec/step, loss=0.85701, avg_loss=0.77032]\n",
            "Step 26967   [1.618 sec/step, loss=0.75311, avg_loss=0.76991]\n",
            "Step 26968   [1.621 sec/step, loss=0.76562, avg_loss=0.76924]\n",
            "Step 26969   [1.637 sec/step, loss=0.69969, avg_loss=0.76863]\n",
            "Step 26970   [1.655 sec/step, loss=0.84048, avg_loss=0.76917]\n",
            "Step 26971   [1.655 sec/step, loss=0.74074, avg_loss=0.76871]\n",
            "Step 26972   [1.666 sec/step, loss=0.73237, avg_loss=0.76874]\n",
            "Step 26973   [1.673 sec/step, loss=0.70707, avg_loss=0.76809]\n",
            "Step 26974   [1.670 sec/step, loss=0.74964, avg_loss=0.76767]\n",
            "Step 26975   [1.635 sec/step, loss=0.77805, avg_loss=0.76788]\n",
            "Step 26976   [1.635 sec/step, loss=0.72704, avg_loss=0.76768]\n",
            "Step 26977   [1.623 sec/step, loss=0.79059, avg_loss=0.76843]\n",
            "Step 26978   [1.611 sec/step, loss=0.75380, avg_loss=0.76839]\n",
            "Step 26979   [1.615 sec/step, loss=0.77299, avg_loss=0.76831]\n",
            "Step 26980   [1.613 sec/step, loss=0.74865, avg_loss=0.76823]\n",
            "Step 26981   [1.612 sec/step, loss=0.74929, avg_loss=0.76812]\n",
            "Generated 32 batches of size 8 in 7.651 sec\n",
            "Step 26982   [1.609 sec/step, loss=0.76991, avg_loss=0.76826]\n",
            "Step 26983   [1.603 sec/step, loss=0.78499, avg_loss=0.76890]\n",
            "Step 26984   [1.601 sec/step, loss=0.76174, avg_loss=0.76832]\n",
            "Step 26985   [1.605 sec/step, loss=0.77266, avg_loss=0.76852]\n",
            "Step 26986   [1.620 sec/step, loss=0.76199, avg_loss=0.76803]\n",
            "Step 26987   [1.626 sec/step, loss=0.72693, avg_loss=0.76727]\n",
            "Step 26988   [1.620 sec/step, loss=0.76380, avg_loss=0.76690]\n",
            "Step 26989   [1.621 sec/step, loss=0.75461, avg_loss=0.76732]\n",
            "Step 26990   [1.613 sec/step, loss=0.81697, avg_loss=0.76751]\n",
            "Step 26991   [1.608 sec/step, loss=0.77377, avg_loss=0.76794]\n",
            "Step 26992   [1.601 sec/step, loss=0.77203, avg_loss=0.76837]\n",
            "Step 26993   [1.600 sec/step, loss=0.74830, avg_loss=0.76802]\n",
            "Step 26994   [1.620 sec/step, loss=0.80372, avg_loss=0.76821]\n",
            "Step 26995   [1.657 sec/step, loss=0.68950, avg_loss=0.76742]\n",
            "Step 26996   [1.658 sec/step, loss=0.75101, avg_loss=0.76697]\n",
            "Step 26997   [1.659 sec/step, loss=0.77979, avg_loss=0.76677]\n",
            "Step 26998   [1.651 sec/step, loss=0.76753, avg_loss=0.76642]\n",
            "Step 26999   [1.656 sec/step, loss=0.78201, avg_loss=0.76699]\n",
            "Step 27000   [1.641 sec/step, loss=0.79772, avg_loss=0.76700]\n",
            "Writing summary at step: 27000\n",
            "Saving checkpoint to: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/model.ckpt-27000\n",
            "Step 27001   [1.639 sec/step, loss=0.82939, avg_loss=0.76755]\n",
            "Step 27002   [1.645 sec/step, loss=0.77344, avg_loss=0.76774]\n",
            "Step 27003   [1.653 sec/step, loss=0.76573, avg_loss=0.76749]\n",
            "Step 27004   [1.653 sec/step, loss=0.78677, avg_loss=0.76805]\n",
            "Step 27005   [1.650 sec/step, loss=0.74828, avg_loss=0.76810]\n",
            "Step 27006   [1.647 sec/step, loss=0.76327, avg_loss=0.76748]\n",
            "Step 27007   [1.649 sec/step, loss=0.71591, avg_loss=0.76658]\n",
            "Step 27008   [1.660 sec/step, loss=0.74907, avg_loss=0.76613]\n",
            "Step 27009   [1.662 sec/step, loss=0.78600, avg_loss=0.76662]\n",
            "Step 27010   [1.665 sec/step, loss=0.75366, avg_loss=0.76615]\n",
            "2020-10-21 15:34:44.586615: W tensorflow/core/kernels/queue_base.cc:277] _0_datafeeder/input_queue_1: Skipping cancelled enqueue attempt with queue not closed\n",
            "Traceback (most recent call last):\n",
            "  File \"./Tacotron2-Wavenet-Korean-TTS/train_tacotron2.py\", line 297, in <module>\n",
            "    main()\n",
            "  File \"./Tacotron2-Wavenet-Korean-TTS/train_tacotron2.py\", line 293, in main\n",
            "    train(config.model_dir, config)\n",
            "  File \"./Tacotron2-Wavenet-Korean-TTS/train_tacotron2.py\", line 191, in train\n",
            "    step, loss, opt = sess.run([global_step, model.loss_without_coeff, model.optimize])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85IQaeppJ7Jc",
        "outputId": "035bc92b-5dce-483a-c50f-0b18b9389c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# batch_size=16\n",
        "\n",
        "!python ./Tacotron2-Wavenet-Korean-TTS/train_tacotron2.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Step 82374   [1.715 sec/step, loss=0.56773, avg_loss=0.56684]\n",
            "Step 82375   [1.713 sec/step, loss=0.59219, avg_loss=0.56669]\n",
            "Step 82376   [1.716 sec/step, loss=0.46014, avg_loss=0.56635]\n",
            "Step 82377   [1.725 sec/step, loss=0.58249, avg_loss=0.56659]\n",
            "Step 82378   [1.734 sec/step, loss=0.57521, avg_loss=0.56679]\n",
            "Step 82379   [1.741 sec/step, loss=0.57527, avg_loss=0.56659]\n",
            "Step 82380   [1.742 sec/step, loss=0.56130, avg_loss=0.56649]\n",
            "Generated 16 batches of size 16 in 7.027 sec\n",
            "Step 82381   [1.762 sec/step, loss=0.53019, avg_loss=0.56584]\n",
            "Step 82382   [1.756 sec/step, loss=0.57048, avg_loss=0.56579]\n",
            "Step 82383   [1.751 sec/step, loss=0.58047, avg_loss=0.56550]\n",
            "Step 82384   [1.733 sec/step, loss=0.60498, avg_loss=0.56671]\n",
            "Step 82385   [1.733 sec/step, loss=0.60882, avg_loss=0.56697]\n",
            "Step 82386   [1.730 sec/step, loss=0.56342, avg_loss=0.56717]\n",
            "Step 82387   [1.733 sec/step, loss=0.56956, avg_loss=0.56702]\n",
            "Step 82388   [1.725 sec/step, loss=0.57392, avg_loss=0.56740]\n",
            "Step 82389   [1.726 sec/step, loss=0.60018, avg_loss=0.56741]\n",
            "Step 82390   [1.722 sec/step, loss=0.58158, avg_loss=0.56748]\n",
            "Step 82391   [1.727 sec/step, loss=0.56600, avg_loss=0.56711]\n",
            "Step 82392   [1.740 sec/step, loss=0.52536, avg_loss=0.56648]\n",
            "Step 82393   [1.744 sec/step, loss=0.55693, avg_loss=0.56586]\n",
            "Step 82394   [1.734 sec/step, loss=0.58118, avg_loss=0.56615]\n",
            "Step 82395   [1.741 sec/step, loss=0.55379, avg_loss=0.56594]\n",
            "Step 82396   [1.743 sec/step, loss=0.54148, avg_loss=0.56569]\n",
            "Step 82397   [1.743 sec/step, loss=0.59117, avg_loss=0.56567]\n",
            "Generated 16 batches of size 16 in 7.876 sec\n",
            "Step 82398   [1.748 sec/step, loss=0.41232, avg_loss=0.56536]\n",
            "Step 82399   [1.742 sec/step, loss=0.60029, avg_loss=0.56570]\n",
            "Step 82400   [1.738 sec/step, loss=0.52767, avg_loss=0.56535]\n",
            "Writing summary at step: 82400\n",
            "Step 82401   [1.736 sec/step, loss=0.58916, avg_loss=0.56529]\n",
            "Step 82402   [1.733 sec/step, loss=0.54445, avg_loss=0.56483]\n",
            "Step 82403   [1.726 sec/step, loss=0.56932, avg_loss=0.56492]\n",
            "Step 82404   [1.718 sec/step, loss=0.59074, avg_loss=0.56547]\n",
            "Step 82405   [1.721 sec/step, loss=0.59394, avg_loss=0.56552]\n",
            "Step 82406   [1.723 sec/step, loss=0.59817, avg_loss=0.56563]\n",
            "Step 82407   [1.723 sec/step, loss=0.56639, avg_loss=0.56559]\n",
            "Step 82408   [1.740 sec/step, loss=0.50266, avg_loss=0.56491]\n",
            "Step 82409   [1.712 sec/step, loss=0.53277, avg_loss=0.56554]\n",
            "Step 82410   [1.760 sec/step, loss=0.49328, avg_loss=0.56457]\n",
            "Generated 16 batches of size 16 in 8.314 sec\n",
            "Step 82411   [1.763 sec/step, loss=0.59530, avg_loss=0.56492]\n",
            "Step 82412   [1.755 sec/step, loss=0.57460, avg_loss=0.56509]\n",
            "Step 82413   [1.755 sec/step, loss=0.63081, avg_loss=0.56582]\n",
            "Step 82414   [1.743 sec/step, loss=0.59306, avg_loss=0.56580]\n",
            "Step 82415   [1.739 sec/step, loss=0.56884, avg_loss=0.56604]\n",
            "Step 82416   [1.742 sec/step, loss=0.51903, avg_loss=0.56553]\n",
            "Step 82417   [1.738 sec/step, loss=0.56966, avg_loss=0.56537]\n",
            "Step 82418   [1.732 sec/step, loss=0.54663, avg_loss=0.56527]\n",
            "Step 82419   [1.727 sec/step, loss=0.60926, avg_loss=0.56517]\n",
            "Step 82420   [1.724 sec/step, loss=0.59790, avg_loss=0.56563]\n",
            "Step 82421   [1.755 sec/step, loss=0.52381, avg_loss=0.56495]\n",
            "Step 82422   [1.762 sec/step, loss=0.56964, avg_loss=0.56481]\n",
            "Step 82423   [1.779 sec/step, loss=0.54350, avg_loss=0.56416]\n",
            "Step 82424   [1.743 sec/step, loss=0.59121, avg_loss=0.56570]\n",
            "Step 82425   [1.759 sec/step, loss=0.56324, avg_loss=0.56588]\n",
            "Step 82426   [1.757 sec/step, loss=0.59022, avg_loss=0.56658]\n",
            "Step 82427   [1.750 sec/step, loss=0.58833, avg_loss=0.56676]\n",
            "Step 82428   [1.753 sec/step, loss=0.63301, avg_loss=0.56726]\n",
            "Step 82429   [1.758 sec/step, loss=0.56863, avg_loss=0.56742]\n",
            "Generated 16 batches of size 16 in 8.951 sec\n",
            "Step 82430   [1.759 sec/step, loss=0.57605, avg_loss=0.56726]\n",
            "Step 82431   [1.757 sec/step, loss=0.52361, avg_loss=0.56674]\n",
            "Step 82432   [1.748 sec/step, loss=0.58798, avg_loss=0.56736]\n",
            "Step 82433   [1.749 sec/step, loss=0.56985, avg_loss=0.56733]\n",
            "Step 82434   [1.732 sec/step, loss=0.54546, avg_loss=0.56726]\n",
            "Step 82435   [1.731 sec/step, loss=0.60138, avg_loss=0.56724]\n",
            "Step 82436   [1.732 sec/step, loss=0.60247, avg_loss=0.56722]\n",
            "Step 82437   [1.741 sec/step, loss=0.59339, avg_loss=0.56713]\n",
            "Step 82438   [1.733 sec/step, loss=0.58539, avg_loss=0.56727]\n",
            "Step 82439   [1.735 sec/step, loss=0.59226, avg_loss=0.56745]\n",
            "Step 82440   [1.731 sec/step, loss=0.59115, avg_loss=0.56766]\n",
            "Step 82441   [1.726 sec/step, loss=0.55526, avg_loss=0.56766]\n",
            "Step 82442   [1.739 sec/step, loss=0.55137, avg_loss=0.56760]\n",
            "Step 82443   [1.739 sec/step, loss=0.60704, avg_loss=0.56768]\n",
            "Step 82444   [1.712 sec/step, loss=0.56934, avg_loss=0.56937]\n",
            "Generated 16 batches of size 16 in 9.421 sec\n",
            "Step 82445   [1.746 sec/step, loss=0.44464, avg_loss=0.56806]\n",
            "Step 82446   [1.742 sec/step, loss=0.61198, avg_loss=0.56817]\n",
            "Step 82447   [1.741 sec/step, loss=0.58026, avg_loss=0.56805]\n",
            "Step 82448   [1.747 sec/step, loss=0.57356, avg_loss=0.56781]\n",
            "Step 82449   [1.756 sec/step, loss=0.57122, avg_loss=0.56794]\n",
            "Step 82450   [1.751 sec/step, loss=0.58742, avg_loss=0.56774]\n",
            "Step 82451   [1.734 sec/step, loss=0.62252, avg_loss=0.56879]\n",
            "Step 82452   [1.726 sec/step, loss=0.52678, avg_loss=0.56833]\n",
            "Step 82453   [1.723 sec/step, loss=0.58463, avg_loss=0.56827]\n",
            "Step 82454   [1.727 sec/step, loss=0.56613, avg_loss=0.56837]\n",
            "Step 82455   [1.729 sec/step, loss=0.58222, avg_loss=0.56829]\n",
            "Step 82456   [1.744 sec/step, loss=0.46673, avg_loss=0.56689]\n",
            "Step 82457   [1.787 sec/step, loss=0.51955, avg_loss=0.56585]\n",
            "Step 82458   [1.748 sec/step, loss=0.57200, avg_loss=0.56774]\n",
            "Step 82459   [1.753 sec/step, loss=0.60574, avg_loss=0.56800]\n",
            "Generated 16 batches of size 16 in 9.418 sec\n",
            "Step 82460   [1.750 sec/step, loss=0.61326, avg_loss=0.56846]\n",
            "Step 82461   [1.757 sec/step, loss=0.57963, avg_loss=0.56833]\n",
            "Step 82462   [1.745 sec/step, loss=0.58383, avg_loss=0.56837]\n",
            "Step 82463   [1.749 sec/step, loss=0.59352, avg_loss=0.56869]\n",
            "Step 82464   [1.743 sec/step, loss=0.53403, avg_loss=0.56820]\n",
            "Step 82465   [1.736 sec/step, loss=0.57852, avg_loss=0.56880]\n",
            "Step 82466   [1.729 sec/step, loss=0.56307, avg_loss=0.56898]\n",
            "Step 82467   [1.726 sec/step, loss=0.59446, avg_loss=0.56910]\n",
            "Step 82468   [1.733 sec/step, loss=0.55056, avg_loss=0.56898]\n",
            "Step 82469   [1.728 sec/step, loss=0.57230, avg_loss=0.56887]\n",
            "Step 82470   [1.730 sec/step, loss=0.57516, avg_loss=0.56888]\n",
            "Step 82471   [1.731 sec/step, loss=0.59206, avg_loss=0.56912]\n",
            "Step 82472   [1.734 sec/step, loss=0.53259, avg_loss=0.56866]\n",
            "Step 82473   [1.744 sec/step, loss=0.51482, avg_loss=0.56757]\n",
            "Step 82474   [1.750 sec/step, loss=0.58246, avg_loss=0.56772]\n",
            "Generated 16 batches of size 16 in 10.511 sec\n",
            "Step 82475   [1.799 sec/step, loss=0.40261, avg_loss=0.56582]\n",
            "Step 82476   [1.759 sec/step, loss=0.57842, avg_loss=0.56700]\n",
            "Step 82477   [1.758 sec/step, loss=0.59969, avg_loss=0.56717]\n",
            "Step 82478   [1.744 sec/step, loss=0.55564, avg_loss=0.56698]\n",
            "Step 82479   [1.738 sec/step, loss=0.58177, avg_loss=0.56704]\n",
            "Step 82480   [1.734 sec/step, loss=0.59702, avg_loss=0.56740]\n",
            "Step 82481   [1.712 sec/step, loss=0.58576, avg_loss=0.56796]\n",
            "Step 82482   [1.717 sec/step, loss=0.57488, avg_loss=0.56800]\n",
            "Step 82483   [1.715 sec/step, loss=0.57042, avg_loss=0.56790]\n",
            "Step 82484   [1.721 sec/step, loss=0.55726, avg_loss=0.56742]\n",
            "Step 82485   [1.724 sec/step, loss=0.57771, avg_loss=0.56711]\n",
            "Step 82486   [1.723 sec/step, loss=0.60070, avg_loss=0.56749]\n",
            "Step 82487   [1.717 sec/step, loss=0.60904, avg_loss=0.56788]\n",
            "Step 82488   [1.716 sec/step, loss=0.55634, avg_loss=0.56770]\n",
            "Step 82489   [1.720 sec/step, loss=0.57300, avg_loss=0.56743]\n",
            "Step 82490   [1.738 sec/step, loss=0.54119, avg_loss=0.56703]\n",
            "Step 82491   [1.730 sec/step, loss=0.56666, avg_loss=0.56703]\n",
            "Step 82492   [1.718 sec/step, loss=0.59774, avg_loss=0.56776]\n",
            "Step 82493   [1.735 sec/step, loss=0.58750, avg_loss=0.56806]\n",
            "Generated 16 batches of size 16 in 10.126 sec\n",
            "Step 82494   [1.732 sec/step, loss=0.54323, avg_loss=0.56768]\n",
            "Step 82495   [1.727 sec/step, loss=0.59298, avg_loss=0.56808]\n",
            "Step 82496   [1.758 sec/step, loss=0.55897, avg_loss=0.56825]\n",
            "Step 82497   [1.760 sec/step, loss=0.51874, avg_loss=0.56753]\n",
            "Step 82498   [1.715 sec/step, loss=0.57641, avg_loss=0.56917]\n",
            "Step 82499   [1.720 sec/step, loss=0.58207, avg_loss=0.56899]\n",
            "Step 82500   [1.727 sec/step, loss=0.55285, avg_loss=0.56924]\n",
            "Writing summary at step: 82500\n",
            "Step 82501   [1.727 sec/step, loss=0.56508, avg_loss=0.56900]\n",
            "Step 82502   [1.728 sec/step, loss=0.59615, avg_loss=0.56951]\n",
            "Step 82503   [1.731 sec/step, loss=0.53475, avg_loss=0.56917]\n",
            "Step 82504   [1.730 sec/step, loss=0.55682, avg_loss=0.56883]\n",
            "Step 82505   [1.734 sec/step, loss=0.57600, avg_loss=0.56865]\n",
            "Step 82506   [1.736 sec/step, loss=0.58547, avg_loss=0.56852]\n",
            "Step 82507   [1.743 sec/step, loss=0.55283, avg_loss=0.56839]\n",
            "Step 82508   [1.729 sec/step, loss=0.60487, avg_loss=0.56941]\n",
            "Generated 16 batches of size 16 in 10.127 sec\n",
            "Step 82509   [1.767 sec/step, loss=0.39551, avg_loss=0.56804]\n",
            "Step 82510   [1.720 sec/step, loss=0.59914, avg_loss=0.56910]\n",
            "Step 82511   [1.725 sec/step, loss=0.51853, avg_loss=0.56833]\n",
            "Step 82512   [1.727 sec/step, loss=0.58982, avg_loss=0.56848]\n",
            "Step 82513   [1.760 sec/step, loss=0.42200, avg_loss=0.56639]\n",
            "Step 82514   [1.760 sec/step, loss=0.59610, avg_loss=0.56642]\n",
            "Step 82515   [1.761 sec/step, loss=0.57232, avg_loss=0.56646]\n",
            "Step 82516   [1.764 sec/step, loss=0.54805, avg_loss=0.56675]\n",
            "Step 82517   [1.771 sec/step, loss=0.54164, avg_loss=0.56647]\n",
            "Step 82518   [1.763 sec/step, loss=0.57694, avg_loss=0.56677]\n",
            "Step 82519   [1.767 sec/step, loss=0.58035, avg_loss=0.56648]\n",
            "Step 82520   [1.775 sec/step, loss=0.59581, avg_loss=0.56646]\n",
            "Step 82521   [1.744 sec/step, loss=0.55176, avg_loss=0.56674]\n",
            "Step 82522   [1.746 sec/step, loss=0.53826, avg_loss=0.56643]\n",
            "Step 82523   [1.733 sec/step, loss=0.59534, avg_loss=0.56694]\n",
            "Step 82524   [1.733 sec/step, loss=0.60658, avg_loss=0.56710]\n",
            "Step 82525   [1.724 sec/step, loss=0.62025, avg_loss=0.56767]\n",
            "Generated 16 batches of size 16 in 10.147 sec\n",
            "Step 82526   [1.725 sec/step, loss=0.60606, avg_loss=0.56783]\n",
            "Step 82527   [1.730 sec/step, loss=0.58157, avg_loss=0.56776]\n",
            "Step 82528   [1.732 sec/step, loss=0.56138, avg_loss=0.56704]\n",
            "Step 82529   [1.727 sec/step, loss=0.60634, avg_loss=0.56742]\n",
            "Step 82530   [1.724 sec/step, loss=0.56187, avg_loss=0.56728]\n",
            "Step 82531   [1.716 sec/step, loss=0.58953, avg_loss=0.56794]\n",
            "Step 82532   [1.717 sec/step, loss=0.60983, avg_loss=0.56816]\n",
            "Step 82533   [1.720 sec/step, loss=0.56446, avg_loss=0.56810]\n",
            "Step 82534   [1.716 sec/step, loss=0.56064, avg_loss=0.56825]\n",
            "Step 82535   [1.716 sec/step, loss=0.54374, avg_loss=0.56768]\n",
            "Step 82536   [1.726 sec/step, loss=0.53399, avg_loss=0.56699]\n",
            "Step 82537   [1.716 sec/step, loss=0.57645, avg_loss=0.56682]\n",
            "Step 82538   [1.717 sec/step, loss=0.54431, avg_loss=0.56641]\n",
            "Generated 16 batches of size 16 in 10.114 sec\n",
            "Step 82539   [1.767 sec/step, loss=0.40951, avg_loss=0.56458]\n",
            "Step 82540   [1.771 sec/step, loss=0.59712, avg_loss=0.56464]\n",
            "Step 82541   [1.776 sec/step, loss=0.58387, avg_loss=0.56493]\n",
            "Step 82542   [1.769 sec/step, loss=0.53729, avg_loss=0.56479]\n",
            "Step 82543   [1.771 sec/step, loss=0.53350, avg_loss=0.56405]\n",
            "Step 82544   [1.768 sec/step, loss=0.61161, avg_loss=0.56448]\n",
            "Step 82545   [1.742 sec/step, loss=0.52292, avg_loss=0.56526]\n",
            "Step 82546   [1.740 sec/step, loss=0.57594, avg_loss=0.56490]\n",
            "Step 82547   [1.739 sec/step, loss=0.57654, avg_loss=0.56486]\n",
            "Step 82548   [1.730 sec/step, loss=0.60269, avg_loss=0.56515]\n",
            "Step 82549   [1.723 sec/step, loss=0.59700, avg_loss=0.56541]\n",
            "Step 82550   [1.729 sec/step, loss=0.56633, avg_loss=0.56520]\n",
            "Step 82551   [1.731 sec/step, loss=0.59306, avg_loss=0.56491]\n",
            "Step 82552   [1.733 sec/step, loss=0.60101, avg_loss=0.56565]\n",
            "Step 82553   [1.744 sec/step, loss=0.58095, avg_loss=0.56561]\n",
            "Step 82554   [1.742 sec/step, loss=0.54747, avg_loss=0.56542]\n",
            "Step 82555   [1.754 sec/step, loss=0.57385, avg_loss=0.56534]\n",
            "Step 82556   [1.736 sec/step, loss=0.53641, avg_loss=0.56604]\n",
            "Generated 16 batches of size 16 in 10.114 sec\n",
            "Step 82557   [1.702 sec/step, loss=0.57051, avg_loss=0.56655]\n",
            "Step 82558   [1.700 sec/step, loss=0.58999, avg_loss=0.56673]\n",
            "Step 82559   [1.734 sec/step, loss=0.40740, avg_loss=0.56474]\n",
            "Step 82560   [1.736 sec/step, loss=0.56852, avg_loss=0.56430]\n",
            "Step 82561   [1.730 sec/step, loss=0.58494, avg_loss=0.56435]\n",
            "Step 82562   [1.734 sec/step, loss=0.56702, avg_loss=0.56418]\n",
            "Step 82563   [1.735 sec/step, loss=0.59299, avg_loss=0.56418]\n",
            "Step 82564   [1.734 sec/step, loss=0.61728, avg_loss=0.56501]\n",
            "Step 82565   [1.729 sec/step, loss=0.60447, avg_loss=0.56527]\n",
            "Step 82566   [1.740 sec/step, loss=0.51148, avg_loss=0.56475]\n",
            "Step 82567   [1.742 sec/step, loss=0.57596, avg_loss=0.56457]\n",
            "Step 82568   [1.793 sec/step, loss=0.40358, avg_loss=0.56310]\n",
            "Generated 16 batches of size 16 in 7.827 sec\n",
            "Step 82569   [1.792 sec/step, loss=0.56635, avg_loss=0.56304]\n",
            "Step 82570   [1.787 sec/step, loss=0.58926, avg_loss=0.56318]\n",
            "Step 82571   [1.792 sec/step, loss=0.56787, avg_loss=0.56294]\n",
            "Step 82572   [1.795 sec/step, loss=0.52068, avg_loss=0.56282]\n",
            "Step 82573   [1.777 sec/step, loss=0.60968, avg_loss=0.56377]\n",
            "Step 82574   [1.769 sec/step, loss=0.57374, avg_loss=0.56368]\n",
            "Step 82575   [1.724 sec/step, loss=0.54154, avg_loss=0.56507]\n",
            "Step 82576   [1.723 sec/step, loss=0.60361, avg_loss=0.56532]\n",
            "Step 82577   [1.727 sec/step, loss=0.53206, avg_loss=0.56464]\n",
            "Step 82578   [1.726 sec/step, loss=0.59935, avg_loss=0.56508]\n",
            "Step 82579   [1.726 sec/step, loss=0.58376, avg_loss=0.56510]\n",
            "Step 82580   [1.726 sec/step, loss=0.59347, avg_loss=0.56507]\n",
            "Step 82581   [1.733 sec/step, loss=0.56849, avg_loss=0.56489]\n",
            "Step 82582   [1.736 sec/step, loss=0.52143, avg_loss=0.56436]\n",
            "Step 82583   [1.739 sec/step, loss=0.61569, avg_loss=0.56481]\n",
            "Step 82584   [1.738 sec/step, loss=0.52944, avg_loss=0.56453]\n",
            "Step 82585   [1.737 sec/step, loss=0.59680, avg_loss=0.56472]\n",
            "Step 82586   [1.757 sec/step, loss=0.53952, avg_loss=0.56411]\n",
            "Generated 16 batches of size 16 in 7.049 sec\n",
            "Step 82587   [1.804 sec/step, loss=0.49031, avg_loss=0.56293]\n",
            "Step 82588   [1.804 sec/step, loss=0.58807, avg_loss=0.56324]\n",
            "Step 82589   [1.799 sec/step, loss=0.62931, avg_loss=0.56381]\n",
            "Step 82590   [1.788 sec/step, loss=0.56469, avg_loss=0.56404]\n",
            "Step 82591   [1.791 sec/step, loss=0.59631, avg_loss=0.56434]\n",
            "Step 82592   [1.789 sec/step, loss=0.60038, avg_loss=0.56436]\n",
            "Step 82593   [1.769 sec/step, loss=0.59462, avg_loss=0.56443]\n",
            "Step 82594   [1.774 sec/step, loss=0.52617, avg_loss=0.56426]\n",
            "Step 82595   [1.805 sec/step, loss=0.43980, avg_loss=0.56273]\n",
            "Step 82596   [1.761 sec/step, loss=0.54064, avg_loss=0.56255]\n",
            "Step 82597   [1.754 sec/step, loss=0.59751, avg_loss=0.56334]\n",
            "Step 82598   [1.762 sec/step, loss=0.58440, avg_loss=0.56342]\n",
            "Step 82599   [1.760 sec/step, loss=0.59771, avg_loss=0.56357]\n",
            "Step 82600   [1.758 sec/step, loss=0.54144, avg_loss=0.56346]\n",
            "Writing summary at step: 82600\n",
            "Step 82601   [1.774 sec/step, loss=0.58128, avg_loss=0.56362]\n",
            "Generated 16 batches of size 16 in 7.641 sec\n",
            "Step 82602   [1.778 sec/step, loss=0.57257, avg_loss=0.56338]\n",
            "Step 82603   [1.775 sec/step, loss=0.55424, avg_loss=0.56358]\n",
            "Step 82604   [1.773 sec/step, loss=0.57251, avg_loss=0.56374]\n",
            "Step 82605   [1.781 sec/step, loss=0.52965, avg_loss=0.56327]\n",
            "Step 82606   [1.775 sec/step, loss=0.58388, avg_loss=0.56326]\n",
            "Step 82607   [1.765 sec/step, loss=0.58998, avg_loss=0.56363]\n",
            "Step 82608   [1.766 sec/step, loss=0.60539, avg_loss=0.56363]\n",
            "Step 82609   [1.720 sec/step, loss=0.58948, avg_loss=0.56557]\n",
            "Step 82610   [1.722 sec/step, loss=0.64471, avg_loss=0.56603]\n",
            "Step 82611   [1.718 sec/step, loss=0.56045, avg_loss=0.56645]\n",
            "Step 82612   [1.711 sec/step, loss=0.58534, avg_loss=0.56640]\n",
            "Step 82613   [1.682 sec/step, loss=0.54674, avg_loss=0.56765]\n",
            "Step 82614   [1.682 sec/step, loss=0.56880, avg_loss=0.56738]\n",
            "Step 82615   [1.692 sec/step, loss=0.59791, avg_loss=0.56763]\n",
            "Step 82616   [1.700 sec/step, loss=0.50175, avg_loss=0.56717]\n",
            "Step 82617   [1.696 sec/step, loss=0.59881, avg_loss=0.56774]\n",
            "Step 82618   [1.697 sec/step, loss=0.59173, avg_loss=0.56789]\n",
            "Generated 16 batches of size 16 in 8.276 sec\n",
            "Step 82619   [1.698 sec/step, loss=0.59962, avg_loss=0.56808]\n",
            "Step 82620   [1.696 sec/step, loss=0.57443, avg_loss=0.56787]\n",
            "Step 82621   [1.698 sec/step, loss=0.59360, avg_loss=0.56829]\n",
            "Step 82622   [1.690 sec/step, loss=0.60560, avg_loss=0.56896]\n",
            "Step 82623   [1.718 sec/step, loss=0.56696, avg_loss=0.56868]\n",
            "Step 82624   [1.718 sec/step, loss=0.58123, avg_loss=0.56842]\n",
            "Step 82625   [1.713 sec/step, loss=0.55292, avg_loss=0.56775]\n",
            "Step 82626   [1.708 sec/step, loss=0.61743, avg_loss=0.56786]\n",
            "Step 82627   [1.706 sec/step, loss=0.53242, avg_loss=0.56737]\n",
            "Step 82628   [1.702 sec/step, loss=0.58382, avg_loss=0.56760]\n",
            "Step 82629   [1.717 sec/step, loss=0.54616, avg_loss=0.56700]\n",
            "Step 82630   [1.719 sec/step, loss=0.60572, avg_loss=0.56743]\n",
            "Step 82631   [1.723 sec/step, loss=0.59720, avg_loss=0.56751]\n",
            "Step 82632   [1.723 sec/step, loss=0.56732, avg_loss=0.56709]\n",
            "Step 82633   [1.728 sec/step, loss=0.52631, avg_loss=0.56670]\n",
            "Step 82634   [1.743 sec/step, loss=0.51969, avg_loss=0.56629]\n",
            "Generated 16 batches of size 16 in 9.306 sec\n",
            "Step 82635   [1.780 sec/step, loss=0.48743, avg_loss=0.56573]\n",
            "Step 82636   [1.765 sec/step, loss=0.61500, avg_loss=0.56654]\n",
            "Step 82637   [1.762 sec/step, loss=0.61617, avg_loss=0.56694]\n",
            "Step 82638   [1.767 sec/step, loss=0.58266, avg_loss=0.56732]\n",
            "Step 82639   [1.722 sec/step, loss=0.59022, avg_loss=0.56913]\n",
            "Step 82640   [1.718 sec/step, loss=0.58513, avg_loss=0.56901]\n",
            "Step 82641   [1.712 sec/step, loss=0.59074, avg_loss=0.56908]\n",
            "Step 82642   [1.704 sec/step, loss=0.54519, avg_loss=0.56916]\n",
            "Step 82643   [1.707 sec/step, loss=0.51675, avg_loss=0.56899]\n",
            "Step 82644   [1.703 sec/step, loss=0.60986, avg_loss=0.56897]\n",
            "Step 82645   [1.732 sec/step, loss=0.39159, avg_loss=0.56766]\n",
            "Step 82646   [1.737 sec/step, loss=0.53851, avg_loss=0.56728]\n",
            "Step 82647   [1.741 sec/step, loss=0.59039, avg_loss=0.56742]\n",
            "Step 82648   [1.744 sec/step, loss=0.60071, avg_loss=0.56740]\n",
            "Step 82649   [1.755 sec/step, loss=0.57961, avg_loss=0.56723]\n",
            "Step 82650   [1.753 sec/step, loss=0.55176, avg_loss=0.56708]\n",
            "Step 82651   [1.757 sec/step, loss=0.59011, avg_loss=0.56705]\n",
            "Step 82652   [1.758 sec/step, loss=0.59997, avg_loss=0.56704]\n",
            "Generated 16 batches of size 16 in 10.141 sec\n",
            "Step 82653   [1.748 sec/step, loss=0.59504, avg_loss=0.56718]\n",
            "Step 82654   [1.759 sec/step, loss=0.49741, avg_loss=0.56668]\n",
            "Step 82655   [1.749 sec/step, loss=0.57698, avg_loss=0.56672]\n",
            "Step 82656   [1.791 sec/step, loss=0.49047, avg_loss=0.56626]\n",
            "Step 82657   [1.782 sec/step, loss=0.56400, avg_loss=0.56619]\n",
            "Step 82658   [1.789 sec/step, loss=0.59005, avg_loss=0.56619]\n",
            "Step 82659   [1.754 sec/step, loss=0.58246, avg_loss=0.56794]\n",
            "Step 82660   [1.758 sec/step, loss=0.56270, avg_loss=0.56788]\n",
            "Step 82661   [1.758 sec/step, loss=0.62057, avg_loss=0.56824]\n",
            "Step 82662   [1.757 sec/step, loss=0.58101, avg_loss=0.56838]\n",
            "Step 82663   [1.758 sec/step, loss=0.57426, avg_loss=0.56819]\n",
            "Step 82664   [1.763 sec/step, loss=0.53827, avg_loss=0.56740]\n",
            "Step 82665   [1.760 sec/step, loss=0.59072, avg_loss=0.56726]\n",
            "Step 82666   [1.751 sec/step, loss=0.61335, avg_loss=0.56828]\n",
            "Step 82667   [1.764 sec/step, loss=0.59804, avg_loss=0.56850]\n",
            "Step 82668   [1.714 sec/step, loss=0.58576, avg_loss=0.57033]\n",
            "Generated 16 batches of size 16 in 10.321 sec\n",
            "Step 82669   [1.710 sec/step, loss=0.57995, avg_loss=0.57046]\n",
            "Step 82670   [1.733 sec/step, loss=0.51848, avg_loss=0.56975]\n",
            "Step 82671   [1.728 sec/step, loss=0.51104, avg_loss=0.56919]\n",
            "Step 82672   [1.721 sec/step, loss=0.58348, avg_loss=0.56981]\n",
            "Step 82673   [1.731 sec/step, loss=0.56259, avg_loss=0.56934]\n",
            "Step 82674   [1.726 sec/step, loss=0.56503, avg_loss=0.56926]\n",
            "Step 82675   [1.728 sec/step, loss=0.51129, avg_loss=0.56895]\n",
            "Step 82676   [1.730 sec/step, loss=0.58986, avg_loss=0.56882]\n",
            "Step 82677   [1.760 sec/step, loss=0.42543, avg_loss=0.56775]\n",
            "Step 82678   [1.761 sec/step, loss=0.57253, avg_loss=0.56748]\n",
            "Step 82679   [1.770 sec/step, loss=0.60902, avg_loss=0.56773]\n",
            "Step 82680   [1.776 sec/step, loss=0.56434, avg_loss=0.56744]\n",
            "Step 82681   [1.778 sec/step, loss=0.57668, avg_loss=0.56753]\n",
            "Step 82682   [1.776 sec/step, loss=0.58943, avg_loss=0.56821]\n",
            "Step 82683   [1.775 sec/step, loss=0.57899, avg_loss=0.56784]\n",
            "Generated 16 batches of size 16 in 10.016 sec\n",
            "Step 82684   [1.782 sec/step, loss=0.59212, avg_loss=0.56846]\n",
            "Step 82685   [1.779 sec/step, loss=0.61747, avg_loss=0.56867]\n",
            "Step 82686   [1.756 sec/step, loss=0.61006, avg_loss=0.56938]\n",
            "Step 82687   [1.726 sec/step, loss=0.56639, avg_loss=0.57014]\n",
            "Step 82688   [1.724 sec/step, loss=0.60806, avg_loss=0.57034]\n",
            "Step 82689   [1.724 sec/step, loss=0.61222, avg_loss=0.57017]\n",
            "Step 82690   [1.732 sec/step, loss=0.55171, avg_loss=0.57004]\n",
            "Step 82691   [1.729 sec/step, loss=0.58500, avg_loss=0.56992]\n",
            "Step 82692   [1.735 sec/step, loss=0.57298, avg_loss=0.56965]\n",
            "Step 82693   [1.733 sec/step, loss=0.56377, avg_loss=0.56934]\n",
            "Step 82694   [1.734 sec/step, loss=0.58566, avg_loss=0.56994]\n",
            "Step 82695   [1.701 sec/step, loss=0.57259, avg_loss=0.57126]\n",
            "Step 82696   [1.705 sec/step, loss=0.59914, avg_loss=0.57185]\n",
            "Step 82697   [1.714 sec/step, loss=0.52541, avg_loss=0.57113]\n",
            "Step 82698   [1.712 sec/step, loss=0.61822, avg_loss=0.57147]\n",
            "Step 82699   [1.719 sec/step, loss=0.57434, avg_loss=0.57123]\n",
            "Step 82700   [1.722 sec/step, loss=0.52478, avg_loss=0.57107]\n",
            "Writing summary at step: 82700\n",
            "Generated 16 batches of size 16 in 10.633 sec\n",
            "Step 82701   [1.706 sec/step, loss=0.56849, avg_loss=0.57094]\n",
            "Step 82702   [1.700 sec/step, loss=0.59844, avg_loss=0.57120]\n",
            "Step 82703   [1.700 sec/step, loss=0.56931, avg_loss=0.57135]\n",
            "Step 82704   [1.710 sec/step, loss=0.56742, avg_loss=0.57130]\n",
            "Step 82705   [1.703 sec/step, loss=0.59512, avg_loss=0.57195]\n",
            "Step 82706   [1.705 sec/step, loss=0.53236, avg_loss=0.57144]\n",
            "Step 82707   [1.704 sec/step, loss=0.59622, avg_loss=0.57150]\n",
            "Step 82708   [1.718 sec/step, loss=0.54401, avg_loss=0.57088]\n",
            "Step 82709   [1.719 sec/step, loss=0.60831, avg_loss=0.57107]\n",
            "Step 82710   [1.722 sec/step, loss=0.55214, avg_loss=0.57015]\n",
            "Step 82711   [1.770 sec/step, loss=0.45622, avg_loss=0.56911]\n",
            "Step 82712   [1.778 sec/step, loss=0.53400, avg_loss=0.56859]\n",
            "Generated 16 batches of size 16 in 10.245 sec\n",
            "Step 82713   [1.773 sec/step, loss=0.60202, avg_loss=0.56914]\n",
            "Step 82714   [1.778 sec/step, loss=0.56618, avg_loss=0.56912]\n",
            "Step 82715   [1.768 sec/step, loss=0.59406, avg_loss=0.56908]\n",
            "Step 82716   [1.746 sec/step, loss=0.55309, avg_loss=0.56959]\n",
            "Step 82717   [1.743 sec/step, loss=0.53100, avg_loss=0.56892]\n",
            "Step 82718   [1.747 sec/step, loss=0.57611, avg_loss=0.56876]\n",
            "Step 82719   [1.748 sec/step, loss=0.55209, avg_loss=0.56828]\n",
            "Step 82720   [1.743 sec/step, loss=0.58416, avg_loss=0.56838]\n",
            "Step 82721   [1.740 sec/step, loss=0.58083, avg_loss=0.56825]\n",
            "Step 82722   [1.742 sec/step, loss=0.59148, avg_loss=0.56811]\n",
            "Step 82723   [1.709 sec/step, loss=0.58565, avg_loss=0.56830]\n",
            "Step 82724   [1.708 sec/step, loss=0.60475, avg_loss=0.56853]\n",
            "Step 82725   [1.708 sec/step, loss=0.59828, avg_loss=0.56899]\n",
            "Step 82726   [1.716 sec/step, loss=0.55169, avg_loss=0.56833]\n",
            "Step 82727   [1.710 sec/step, loss=0.58476, avg_loss=0.56885]\n",
            "Step 82728   [1.710 sec/step, loss=0.54698, avg_loss=0.56849]\n",
            "Step 82729   [1.712 sec/step, loss=0.55425, avg_loss=0.56857]\n",
            "Step 82730   [1.723 sec/step, loss=0.54247, avg_loss=0.56793]\n",
            "Generated 16 batches of size 16 in 9.747 sec\n",
            "Step 82731   [1.729 sec/step, loss=0.54943, avg_loss=0.56746]\n",
            "Step 82732   [1.733 sec/step, loss=0.54706, avg_loss=0.56725]\n",
            "Step 82733   [1.731 sec/step, loss=0.53397, avg_loss=0.56733]\n",
            "Step 82734   [1.753 sec/step, loss=0.43649, avg_loss=0.56650]\n",
            "Step 82735   [1.717 sec/step, loss=0.55920, avg_loss=0.56722]\n",
            "Step 82736   [1.720 sec/step, loss=0.59216, avg_loss=0.56699]\n",
            "Step 82737   [1.762 sec/step, loss=0.34181, avg_loss=0.56424]\n",
            "Step 82738   [1.766 sec/step, loss=0.52676, avg_loss=0.56368]\n",
            "Step 82739   [1.771 sec/step, loss=0.53461, avg_loss=0.56313]\n",
            "Step 82740   [1.773 sec/step, loss=0.57723, avg_loss=0.56305]\n",
            "Step 82741   [1.779 sec/step, loss=0.54255, avg_loss=0.56257]\n",
            "Step 82742   [1.774 sec/step, loss=0.60633, avg_loss=0.56318]\n",
            "Step 82743   [1.774 sec/step, loss=0.55175, avg_loss=0.56353]\n",
            "Step 82744   [1.773 sec/step, loss=0.57971, avg_loss=0.56323]\n",
            "Step 82745   [1.733 sec/step, loss=0.60670, avg_loss=0.56538]\n",
            "Step 82746   [1.731 sec/step, loss=0.54736, avg_loss=0.56547]\n",
            "Step 82747   [1.734 sec/step, loss=0.51330, avg_loss=0.56470]\n",
            "Step 82748   [1.735 sec/step, loss=0.61813, avg_loss=0.56487]\n",
            "Generated 16 batches of size 16 in 9.927 sec\n",
            "Step 82749   [1.722 sec/step, loss=0.57605, avg_loss=0.56483]\n",
            "Step 82750   [1.726 sec/step, loss=0.54888, avg_loss=0.56481]\n",
            "Step 82751   [1.725 sec/step, loss=0.59255, avg_loss=0.56483]\n",
            "Step 82752   [1.731 sec/step, loss=0.55425, avg_loss=0.56437]\n",
            "Step 82753   [1.729 sec/step, loss=0.55588, avg_loss=0.56398]\n",
            "Step 82754   [1.713 sec/step, loss=0.53888, avg_loss=0.56440]\n",
            "Step 82755   [1.715 sec/step, loss=0.55231, avg_loss=0.56415]\n",
            "Step 82756   [1.702 sec/step, loss=0.42230, avg_loss=0.56347]\n",
            "Step 82757   [1.702 sec/step, loss=0.56368, avg_loss=0.56346]\n",
            "Step 82758   [1.697 sec/step, loss=0.60167, avg_loss=0.56358]\n",
            "Step 82759   [1.699 sec/step, loss=0.57933, avg_loss=0.56355]\n",
            "Step 82760   [1.698 sec/step, loss=0.56730, avg_loss=0.56360]\n",
            "Step 82761   [1.698 sec/step, loss=0.59253, avg_loss=0.56332]\n",
            "Step 82762   [1.700 sec/step, loss=0.62796, avg_loss=0.56378]\n",
            "Step 82763   [1.699 sec/step, loss=0.58342, avg_loss=0.56388]\n",
            "Generated 16 batches of size 16 in 9.459 sec\n",
            "Step 82764   [1.695 sec/step, loss=0.59075, avg_loss=0.56440]\n",
            "Step 82765   [1.699 sec/step, loss=0.57864, avg_loss=0.56428]\n",
            "Step 82766   [1.705 sec/step, loss=0.53889, avg_loss=0.56354]\n",
            "Step 82767   [1.690 sec/step, loss=0.59555, avg_loss=0.56351]\n",
            "Step 82768   [1.684 sec/step, loss=0.61352, avg_loss=0.56379]\n",
            "Step 82769   [1.688 sec/step, loss=0.53694, avg_loss=0.56336]\n",
            "Step 82770   [1.666 sec/step, loss=0.55332, avg_loss=0.56371]\n",
            "Step 82771   [1.683 sec/step, loss=0.55184, avg_loss=0.56411]\n",
            "Step 82772   [1.684 sec/step, loss=0.60481, avg_loss=0.56433]\n",
            "Step 82773   [1.680 sec/step, loss=0.56596, avg_loss=0.56436]\n",
            "Step 82774   [1.685 sec/step, loss=0.57736, avg_loss=0.56449]\n",
            "Step 82775   [1.680 sec/step, loss=0.57906, avg_loss=0.56516]\n",
            "Step 82776   [1.679 sec/step, loss=0.58695, avg_loss=0.56513]\n",
            "Generated 16 batches of size 16 in 7.323 sec\n",
            "Step 82777   [1.689 sec/step, loss=0.51626, avg_loss=0.56604]\n",
            "Step 82778   [1.694 sec/step, loss=0.53640, avg_loss=0.56568]\n",
            "Step 82779   [1.694 sec/step, loss=0.53271, avg_loss=0.56492]\n",
            "Step 82780   [1.697 sec/step, loss=0.57252, avg_loss=0.56500]\n",
            "Step 82781   [1.694 sec/step, loss=0.57968, avg_loss=0.56503]\n",
            "Step 82782   [1.691 sec/step, loss=0.57084, avg_loss=0.56484]\n",
            "Step 82783   [1.695 sec/step, loss=0.54571, avg_loss=0.56451]\n",
            "Step 82784   [1.682 sec/step, loss=0.59414, avg_loss=0.56453]\n",
            "Step 82785   [1.682 sec/step, loss=0.57626, avg_loss=0.56412]\n",
            "Step 82786   [1.681 sec/step, loss=0.60067, avg_loss=0.56402]\n",
            "Step 82787   [1.674 sec/step, loss=0.56659, avg_loss=0.56403]\n",
            "Step 82788   [1.673 sec/step, loss=0.60205, avg_loss=0.56397]\n",
            "Step 82789   [1.676 sec/step, loss=0.58730, avg_loss=0.56372]\n",
            "Step 82790   [1.663 sec/step, loss=0.59672, avg_loss=0.56417]\n",
            "Step 82791   [1.686 sec/step, loss=0.52510, avg_loss=0.56357]\n",
            "Step 82792   [1.699 sec/step, loss=0.58256, avg_loss=0.56366]\n",
            "Generated 16 batches of size 16 in 7.514 sec\n",
            "Step 82793   [1.706 sec/step, loss=0.59079, avg_loss=0.56393]\n",
            "Step 82794   [1.702 sec/step, loss=0.59216, avg_loss=0.56400]\n",
            "Step 82795   [1.705 sec/step, loss=0.53684, avg_loss=0.56364]\n",
            "Step 82796   [1.744 sec/step, loss=0.46663, avg_loss=0.56232]\n",
            "Step 82797   [1.738 sec/step, loss=0.57407, avg_loss=0.56280]\n",
            "Step 82798   [1.733 sec/step, loss=0.59324, avg_loss=0.56255]\n",
            "Step 82799   [1.723 sec/step, loss=0.59503, avg_loss=0.56276]\n",
            "Step 82800   [1.748 sec/step, loss=0.42450, avg_loss=0.56176]\n",
            "Writing summary at step: 82800\n",
            "Step 82801   [1.756 sec/step, loss=0.56763, avg_loss=0.56175]\n",
            "Step 82802   [1.763 sec/step, loss=0.59337, avg_loss=0.56170]\n",
            "Step 82803   [1.767 sec/step, loss=0.58360, avg_loss=0.56184]\n",
            "Step 82804   [1.760 sec/step, loss=0.55325, avg_loss=0.56170]\n",
            "Step 82805   [1.758 sec/step, loss=0.58791, avg_loss=0.56163]\n",
            "Step 82806   [1.757 sec/step, loss=0.56619, avg_loss=0.56197]\n",
            "Step 82807   [1.763 sec/step, loss=0.59601, avg_loss=0.56196]\n",
            "Step 82808   [1.746 sec/step, loss=0.58385, avg_loss=0.56236]\n",
            "Step 82809   [1.747 sec/step, loss=0.60025, avg_loss=0.56228]\n",
            "Step 82810   [1.744 sec/step, loss=0.57591, avg_loss=0.56252]\n",
            "Generated 16 batches of size 16 in 8.339 sec\n",
            "Step 82811   [1.699 sec/step, loss=0.54753, avg_loss=0.56343]\n",
            "Step 82812   [1.694 sec/step, loss=0.56921, avg_loss=0.56378]\n",
            "Step 82813   [1.691 sec/step, loss=0.60584, avg_loss=0.56382]\n",
            "Step 82814   [1.688 sec/step, loss=0.59452, avg_loss=0.56411]\n",
            "Step 82815   [1.725 sec/step, loss=0.39288, avg_loss=0.56209]\n",
            "Step 82816   [1.728 sec/step, loss=0.56829, avg_loss=0.56225]\n",
            "Step 82817   [1.731 sec/step, loss=0.57408, avg_loss=0.56268]\n",
            "Step 82818   [1.728 sec/step, loss=0.58964, avg_loss=0.56281]\n",
            "Step 82819   [1.726 sec/step, loss=0.58305, avg_loss=0.56312]\n",
            "Step 82820   [1.730 sec/step, loss=0.59343, avg_loss=0.56321]\n",
            "Step 82821   [1.736 sec/step, loss=0.58750, avg_loss=0.56328]\n",
            "Step 82822   [1.737 sec/step, loss=0.60390, avg_loss=0.56341]\n",
            "Step 82823   [1.739 sec/step, loss=0.57664, avg_loss=0.56332]\n",
            "Step 82824   [1.753 sec/step, loss=0.56058, avg_loss=0.56287]\n",
            "Step 82825   [1.752 sec/step, loss=0.59721, avg_loss=0.56286]\n",
            "Generated 16 batches of size 16 in 8.838 sec\n",
            "Step 82826   [1.754 sec/step, loss=0.58296, avg_loss=0.56318]\n",
            "Step 82827   [1.759 sec/step, loss=0.54333, avg_loss=0.56276]\n",
            "Step 82828   [1.774 sec/step, loss=0.51308, avg_loss=0.56242]\n",
            "Step 82829   [1.753 sec/step, loss=0.56360, avg_loss=0.56252]\n",
            "Step 82830   [1.752 sec/step, loss=0.56828, avg_loss=0.56277]\n",
            "Step 82831   [1.785 sec/step, loss=0.37141, avg_loss=0.56099]\n",
            "Step 82832   [1.782 sec/step, loss=0.59215, avg_loss=0.56144]\n",
            "Step 82833   [1.778 sec/step, loss=0.59021, avg_loss=0.56201]\n",
            "Step 82834   [1.742 sec/step, loss=0.57237, avg_loss=0.56337]\n",
            "Step 82835   [1.747 sec/step, loss=0.57201, avg_loss=0.56349]\n",
            "Step 82836   [1.746 sec/step, loss=0.60353, avg_loss=0.56361]\n",
            "Step 82837   [1.710 sec/step, loss=0.55848, avg_loss=0.56577]\n",
            "Step 82838   [1.703 sec/step, loss=0.57602, avg_loss=0.56627]\n",
            "Step 82839   [1.692 sec/step, loss=0.61250, avg_loss=0.56705]\n",
            "Step 82840   [1.697 sec/step, loss=0.56233, avg_loss=0.56690]\n",
            "Step 82841   [1.693 sec/step, loss=0.56258, avg_loss=0.56710]\n",
            "Step 82842   [1.701 sec/step, loss=0.58820, avg_loss=0.56692]\n",
            "Generated 16 batches of size 16 in 9.428 sec\n",
            "Step 82843   [1.697 sec/step, loss=0.60325, avg_loss=0.56743]\n",
            "Step 82844   [1.703 sec/step, loss=0.54905, avg_loss=0.56712]\n",
            "Step 82845   [1.700 sec/step, loss=0.56778, avg_loss=0.56673]\n",
            "Step 82846   [1.699 sec/step, loss=0.58393, avg_loss=0.56710]\n",
            "Step 82847   [1.691 sec/step, loss=0.56636, avg_loss=0.56763]\n",
            "Step 82848   [1.724 sec/step, loss=0.48629, avg_loss=0.56631]\n",
            "Step 82849   [1.741 sec/step, loss=0.56067, avg_loss=0.56616]\n",
            "Step 82850   [1.735 sec/step, loss=0.55784, avg_loss=0.56625]\n",
            "Step 82851   [1.730 sec/step, loss=0.58444, avg_loss=0.56617]\n",
            "Step 82852   [1.719 sec/step, loss=0.60285, avg_loss=0.56665]\n",
            "Step 82853   [1.727 sec/step, loss=0.59384, avg_loss=0.56703]\n",
            "Step 82854   [1.733 sec/step, loss=0.59388, avg_loss=0.56758]\n",
            "Step 82855   [1.733 sec/step, loss=0.57496, avg_loss=0.56781]\n",
            "Step 82856   [1.708 sec/step, loss=0.58613, avg_loss=0.56945]\n",
            "Step 82857   [1.716 sec/step, loss=0.57121, avg_loss=0.56952]\n",
            "Generated 16 batches of size 16 in 9.222 sec\n",
            "Step 82858   [1.723 sec/step, loss=0.56945, avg_loss=0.56920]\n",
            "Step 82859   [1.720 sec/step, loss=0.57961, avg_loss=0.56920]\n",
            "Step 82860   [1.722 sec/step, loss=0.54154, avg_loss=0.56895]\n",
            "Step 82861   [1.721 sec/step, loss=0.58392, avg_loss=0.56886]\n",
            "Step 82862   [1.716 sec/step, loss=0.57997, avg_loss=0.56838]\n",
            "Step 82863   [1.713 sec/step, loss=0.63408, avg_loss=0.56889]\n",
            "Step 82864   [1.715 sec/step, loss=0.59947, avg_loss=0.56897]\n",
            "Step 82865   [1.712 sec/step, loss=0.59319, avg_loss=0.56912]\n",
            "Step 82866   [1.744 sec/step, loss=0.37424, avg_loss=0.56747]\n",
            "Step 82867   [1.746 sec/step, loss=0.54962, avg_loss=0.56701]\n",
            "Step 82868   [1.745 sec/step, loss=0.56689, avg_loss=0.56655]\n",
            "Step 82869   [1.756 sec/step, loss=0.57839, avg_loss=0.56696]\n",
            "Step 82870   [1.760 sec/step, loss=0.60321, avg_loss=0.56746]\n",
            "Step 82871   [1.751 sec/step, loss=0.56824, avg_loss=0.56762]\n",
            "Step 82872   [1.759 sec/step, loss=0.59815, avg_loss=0.56756]\n",
            "Step 82873   [1.753 sec/step, loss=0.56373, avg_loss=0.56754]\n",
            "Generated 16 batches of size 16 in 10.026 sec\n",
            "Step 82874   [1.757 sec/step, loss=0.57511, avg_loss=0.56751]\n",
            "Step 82875   [1.756 sec/step, loss=0.58974, avg_loss=0.56762]\n",
            "Step 82876   [1.756 sec/step, loss=0.58659, avg_loss=0.56762]\n",
            "Step 82877   [1.716 sec/step, loss=0.55486, avg_loss=0.56800]\n",
            "Step 82878   [1.719 sec/step, loss=0.55544, avg_loss=0.56819]\n",
            "Step 82879   [1.709 sec/step, loss=0.60093, avg_loss=0.56888]\n",
            "Step 82880   [1.701 sec/step, loss=0.58724, avg_loss=0.56902]\n",
            "Step 82881   [1.701 sec/step, loss=0.57442, avg_loss=0.56897]\n",
            "Step 82882   [1.699 sec/step, loss=0.55848, avg_loss=0.56885]\n",
            "Step 82883   [1.700 sec/step, loss=0.50047, avg_loss=0.56839]\n",
            "Step 82884   [1.697 sec/step, loss=0.59598, avg_loss=0.56841]\n",
            "Step 82885   [1.705 sec/step, loss=0.59973, avg_loss=0.56865]\n",
            "Step 82886   [1.705 sec/step, loss=0.54729, avg_loss=0.56811]\n",
            "Step 82887   [1.705 sec/step, loss=0.55588, avg_loss=0.56801]\n",
            "Step 82888   [1.717 sec/step, loss=0.58034, avg_loss=0.56779]\n",
            "Generated 16 batches of size 16 in 10.015 sec\n",
            "Step 82889   [1.764 sec/step, loss=0.34369, avg_loss=0.56535]\n",
            "Step 82890   [1.764 sec/step, loss=0.55626, avg_loss=0.56495]\n",
            "Step 82891   [1.743 sec/step, loss=0.60207, avg_loss=0.56572]\n",
            "Step 82892   [1.737 sec/step, loss=0.53904, avg_loss=0.56528]\n",
            "Step 82893   [1.730 sec/step, loss=0.60831, avg_loss=0.56546]\n",
            "Step 82894   [1.728 sec/step, loss=0.59458, avg_loss=0.56548]\n",
            "Step 82895   [1.767 sec/step, loss=0.41499, avg_loss=0.56426]\n",
            "Step 82896   [1.728 sec/step, loss=0.55770, avg_loss=0.56517]\n",
            "Step 82897   [1.729 sec/step, loss=0.56436, avg_loss=0.56508]\n",
            "Step 82898   [1.729 sec/step, loss=0.59466, avg_loss=0.56509]\n",
            "Step 82899   [1.728 sec/step, loss=0.58373, avg_loss=0.56498]\n",
            "Step 82900   [1.697 sec/step, loss=0.57326, avg_loss=0.56647]\n",
            "Writing summary at step: 82900\n",
            "Step 82901   [1.690 sec/step, loss=0.56414, avg_loss=0.56643]\n",
            "Step 82902   [1.686 sec/step, loss=0.58714, avg_loss=0.56637]\n",
            "Step 82903   [1.699 sec/step, loss=0.55121, avg_loss=0.56604]\n",
            "Step 82904   [1.700 sec/step, loss=0.60173, avg_loss=0.56653]\n",
            "Step 82905   [1.702 sec/step, loss=0.58046, avg_loss=0.56646]\n",
            "Generated 16 batches of size 16 in 10.721 sec\n",
            "Step 82906   [1.707 sec/step, loss=0.55369, avg_loss=0.56633]\n",
            "Step 82907   [1.702 sec/step, loss=0.58908, avg_loss=0.56626]\n",
            "Step 82908   [1.710 sec/step, loss=0.52168, avg_loss=0.56564]\n",
            "Step 82909   [1.710 sec/step, loss=0.61196, avg_loss=0.56576]\n",
            "Step 82910   [1.705 sec/step, loss=0.56654, avg_loss=0.56566]\n",
            "Step 82911   [1.698 sec/step, loss=0.58641, avg_loss=0.56605]\n",
            "Step 82912   [1.698 sec/step, loss=0.58178, avg_loss=0.56618]\n",
            "Step 82913   [1.705 sec/step, loss=0.55928, avg_loss=0.56571]\n",
            "Step 82914   [1.707 sec/step, loss=0.56406, avg_loss=0.56541]\n",
            "Step 82915   [1.688 sec/step, loss=0.51136, avg_loss=0.56659]\n",
            "Step 82916   [1.690 sec/step, loss=0.59765, avg_loss=0.56689]\n",
            "Step 82917   [1.694 sec/step, loss=0.61382, avg_loss=0.56728]\n",
            "Step 82918   [1.707 sec/step, loss=0.56404, avg_loss=0.56703]\n",
            "Step 82919   [1.706 sec/step, loss=0.62172, avg_loss=0.56741]\n",
            "Step 82920   [1.703 sec/step, loss=0.59270, avg_loss=0.56741]\n",
            "Generated 16 batches of size 16 in 10.214 sec\n",
            "Step 82921   [1.742 sec/step, loss=0.50770, avg_loss=0.56661]\n",
            "Step 82922   [1.739 sec/step, loss=0.57662, avg_loss=0.56634]\n",
            "Step 82923   [1.740 sec/step, loss=0.58407, avg_loss=0.56641]\n",
            "Step 82924   [1.734 sec/step, loss=0.57116, avg_loss=0.56652]\n",
            "Step 82925   [1.733 sec/step, loss=0.57677, avg_loss=0.56631]\n",
            "Step 82926   [1.724 sec/step, loss=0.60423, avg_loss=0.56652]\n",
            "Step 82927   [1.730 sec/step, loss=0.55327, avg_loss=0.56662]\n",
            "Step 82928   [1.718 sec/step, loss=0.57597, avg_loss=0.56725]\n",
            "Step 82929   [1.718 sec/step, loss=0.55566, avg_loss=0.56717]\n",
            "Step 82930   [1.715 sec/step, loss=0.57655, avg_loss=0.56726]\n",
            "Step 82931   [1.673 sec/step, loss=0.60471, avg_loss=0.56959]\n",
            "Step 82932   [1.678 sec/step, loss=0.57917, avg_loss=0.56946]\n",
            "Step 82933   [1.682 sec/step, loss=0.56007, avg_loss=0.56916]\n",
            "Step 82934   [1.683 sec/step, loss=0.58012, avg_loss=0.56923]\n",
            "Generated 16 batches of size 16 in 10.222 sec\n",
            "Step 82935   [1.724 sec/step, loss=0.46545, avg_loss=0.56817]\n",
            "Step 82936   [1.722 sec/step, loss=0.57824, avg_loss=0.56792]\n",
            "Step 82937   [1.725 sec/step, loss=0.52532, avg_loss=0.56758]\n",
            "Step 82938   [1.723 sec/step, loss=0.61672, avg_loss=0.56799]\n",
            "Step 82939   [1.728 sec/step, loss=0.56471, avg_loss=0.56751]\n",
            "Step 82940   [1.726 sec/step, loss=0.52919, avg_loss=0.56718]\n",
            "Step 82941   [1.736 sec/step, loss=0.55616, avg_loss=0.56712]\n",
            "Step 82942   [1.741 sec/step, loss=0.55023, avg_loss=0.56674]\n",
            "Step 82943   [1.774 sec/step, loss=0.48121, avg_loss=0.56552]\n",
            "Step 82944   [1.769 sec/step, loss=0.56079, avg_loss=0.56564]\n",
            "Step 82945   [1.769 sec/step, loss=0.61680, avg_loss=0.56613]\n",
            "Step 82946   [1.771 sec/step, loss=0.56392, avg_loss=0.56593]\n",
            "Step 82947   [1.781 sec/step, loss=0.51753, avg_loss=0.56544]\n",
            "Step 82948   [1.747 sec/step, loss=0.60066, avg_loss=0.56658]\n",
            "Step 82949   [1.740 sec/step, loss=0.57042, avg_loss=0.56668]\n",
            "Step 82950   [1.743 sec/step, loss=0.60676, avg_loss=0.56717]\n",
            "Step 82951   [1.745 sec/step, loss=0.57371, avg_loss=0.56706]\n",
            "Step 82952   [1.748 sec/step, loss=0.54699, avg_loss=0.56650]\n",
            "Step 82953   [1.744 sec/step, loss=0.59512, avg_loss=0.56651]\n",
            "Step 82954   [1.740 sec/step, loss=0.57702, avg_loss=0.56635]\n",
            "Generated 16 batches of size 16 in 10.427 sec\n",
            "Step 82955   [1.739 sec/step, loss=0.60651, avg_loss=0.56666]\n",
            "Step 82956   [1.739 sec/step, loss=0.55829, avg_loss=0.56638]\n",
            "Step 82957   [1.749 sec/step, loss=0.50349, avg_loss=0.56571]\n",
            "Step 82958   [1.771 sec/step, loss=0.54817, avg_loss=0.56549]\n",
            "Step 82959   [1.768 sec/step, loss=0.60925, avg_loss=0.56579]\n",
            "Step 82960   [1.759 sec/step, loss=0.59059, avg_loss=0.56628]\n",
            "Step 82961   [1.765 sec/step, loss=0.57128, avg_loss=0.56615]\n",
            "Step 82962   [1.765 sec/step, loss=0.55738, avg_loss=0.56593]\n",
            "Step 82963   [1.763 sec/step, loss=0.59580, avg_loss=0.56554]\n",
            "Step 82964   [1.769 sec/step, loss=0.56637, avg_loss=0.56521]\n",
            "Step 82965   [1.772 sec/step, loss=0.60998, avg_loss=0.56538]\n",
            "Step 82966   [1.736 sec/step, loss=0.61798, avg_loss=0.56782]\n",
            "Generated 16 batches of size 16 in 7.733 sec\n",
            "Step 82967   [1.748 sec/step, loss=0.54995, avg_loss=0.56782]\n",
            "Step 82968   [1.746 sec/step, loss=0.57392, avg_loss=0.56789]\n",
            "Step 82969   [1.737 sec/step, loss=0.59838, avg_loss=0.56809]\n",
            "Step 82970   [1.737 sec/step, loss=0.50947, avg_loss=0.56715]\n",
            "Step 82971   [1.739 sec/step, loss=0.52201, avg_loss=0.56669]\n",
            "Step 82972   [1.730 sec/step, loss=0.58637, avg_loss=0.56657]\n",
            "Step 82973   [1.734 sec/step, loss=0.52511, avg_loss=0.56619]\n",
            "Step 82974   [1.729 sec/step, loss=0.56133, avg_loss=0.56605]\n",
            "Step 82975   [1.731 sec/step, loss=0.58736, avg_loss=0.56603]\n",
            "Step 82976   [1.732 sec/step, loss=0.59455, avg_loss=0.56611]\n",
            "Step 82977   [1.722 sec/step, loss=0.58617, avg_loss=0.56642]\n",
            "Step 82978   [1.723 sec/step, loss=0.52954, avg_loss=0.56616]\n",
            "Step 82979   [1.722 sec/step, loss=0.60683, avg_loss=0.56622]\n",
            "Step 82980   [1.729 sec/step, loss=0.58698, avg_loss=0.56622]\n",
            "Step 82981   [1.725 sec/step, loss=0.52766, avg_loss=0.56575]\n",
            "Step 82982   [1.737 sec/step, loss=0.54559, avg_loss=0.56562]\n",
            "Generated 16 batches of size 16 in 7.424 sec\n",
            "Step 82983   [1.775 sec/step, loss=0.46304, avg_loss=0.56525]\n",
            "Step 82984   [1.775 sec/step, loss=0.58847, avg_loss=0.56517]\n",
            "Step 82985   [1.769 sec/step, loss=0.60002, avg_loss=0.56517]\n",
            "Step 82986   [1.784 sec/step, loss=0.50149, avg_loss=0.56472]\n",
            "Step 82987   [1.782 sec/step, loss=0.55953, avg_loss=0.56475]\n",
            "Step 82988   [1.771 sec/step, loss=0.59695, avg_loss=0.56492]\n",
            "Step 82989   [1.724 sec/step, loss=0.53648, avg_loss=0.56685]\n",
            "Step 82990   [1.726 sec/step, loss=0.58562, avg_loss=0.56714]\n",
            "Step 82991   [1.724 sec/step, loss=0.55566, avg_loss=0.56668]\n",
            "Step 82992   [1.715 sec/step, loss=0.57446, avg_loss=0.56703]\n",
            "Step 82993   [1.711 sec/step, loss=0.56548, avg_loss=0.56660]\n",
            "Step 82994   [1.721 sec/step, loss=0.54063, avg_loss=0.56606]\n",
            "Step 82995   [1.680 sec/step, loss=0.60546, avg_loss=0.56797]\n",
            "Step 82996   [1.732 sec/step, loss=0.49217, avg_loss=0.56731]\n",
            "Generated 16 batches of size 16 in 7.706 sec\n",
            "Step 82997   [1.744 sec/step, loss=0.55865, avg_loss=0.56725]\n",
            "Step 82998   [1.761 sec/step, loss=0.54307, avg_loss=0.56674]\n",
            "Step 82999   [1.760 sec/step, loss=0.58565, avg_loss=0.56676]\n",
            "Step 83000   [1.752 sec/step, loss=0.60569, avg_loss=0.56708]\n",
            "Writing summary at step: 83000\n",
            "Saving checkpoint to: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/model.ckpt-83000\n",
            "Step 83001   [1.751 sec/step, loss=0.57837, avg_loss=0.56722]\n",
            "Step 83002   [1.748 sec/step, loss=0.61739, avg_loss=0.56753]\n",
            "Step 83003   [1.736 sec/step, loss=0.54215, avg_loss=0.56744]\n",
            "Step 83004   [1.779 sec/step, loss=0.42426, avg_loss=0.56566]\n",
            "Step 83005   [1.778 sec/step, loss=0.55658, avg_loss=0.56542]\n",
            "Step 83006   [1.775 sec/step, loss=0.59467, avg_loss=0.56583]\n",
            "Step 83007   [1.790 sec/step, loss=0.53951, avg_loss=0.56534]\n",
            "Step 83008   [1.790 sec/step, loss=0.55337, avg_loss=0.56565]\n",
            "Step 83009   [1.787 sec/step, loss=0.60981, avg_loss=0.56563]\n",
            "Step 83010   [1.789 sec/step, loss=0.58640, avg_loss=0.56583]\n",
            "Step 83011   [1.800 sec/step, loss=0.55666, avg_loss=0.56553]\n",
            "Step 83012   [1.804 sec/step, loss=0.59958, avg_loss=0.56571]\n",
            "Step 83013   [1.801 sec/step, loss=0.58796, avg_loss=0.56600]\n",
            "Step 83014   [1.815 sec/step, loss=0.54582, avg_loss=0.56582]\n",
            "Generated 16 batches of size 16 in 8.641 sec\n",
            "Step 83015   [1.797 sec/step, loss=0.57033, avg_loss=0.56641]\n",
            "Step 83016   [1.793 sec/step, loss=0.56275, avg_loss=0.56606]\n",
            "Step 83017   [1.785 sec/step, loss=0.58857, avg_loss=0.56580]\n",
            "Step 83018   [1.775 sec/step, loss=0.58832, avg_loss=0.56605]\n",
            "Step 83019   [1.773 sec/step, loss=0.61154, avg_loss=0.56595]\n",
            "Step 83020   [1.769 sec/step, loss=0.58198, avg_loss=0.56584]\n",
            "Step 83021   [1.723 sec/step, loss=0.55448, avg_loss=0.56631]\n",
            "Step 83022   [1.730 sec/step, loss=0.55290, avg_loss=0.56607]\n",
            "Step 83023   [1.730 sec/step, loss=0.61086, avg_loss=0.56634]\n",
            "Step 83024   [1.725 sec/step, loss=0.54019, avg_loss=0.56603]\n",
            "Step 83025   [1.725 sec/step, loss=0.58470, avg_loss=0.56611]\n",
            "Step 83026   [1.730 sec/step, loss=0.57115, avg_loss=0.56578]\n",
            "Step 83027   [1.724 sec/step, loss=0.59152, avg_loss=0.56616]\n",
            "Step 83028   [1.733 sec/step, loss=0.55433, avg_loss=0.56594]\n",
            "Step 83029   [1.735 sec/step, loss=0.58551, avg_loss=0.56624]\n",
            "Step 83030   [1.737 sec/step, loss=0.54548, avg_loss=0.56593]\n",
            "Step 83031   [1.741 sec/step, loss=0.61290, avg_loss=0.56601]\n",
            "Generated 16 batches of size 16 in 8.921 sec\n",
            "Step 83032   [1.737 sec/step, loss=0.60918, avg_loss=0.56631]\n",
            "Step 83033   [1.733 sec/step, loss=0.57611, avg_loss=0.56647]\n",
            "Step 83034   [1.741 sec/step, loss=0.54247, avg_loss=0.56610]\n",
            "Step 83035   [1.733 sec/step, loss=0.43752, avg_loss=0.56582]\n",
            "Step 83036   [1.737 sec/step, loss=0.51139, avg_loss=0.56515]\n",
            "Step 83037   [1.731 sec/step, loss=0.60814, avg_loss=0.56598]\n",
            "Step 83038   [1.736 sec/step, loss=0.56630, avg_loss=0.56547]\n",
            "Step 83039   [1.746 sec/step, loss=0.52680, avg_loss=0.56509]\n",
            "Step 83040   [1.781 sec/step, loss=0.41774, avg_loss=0.56398]\n",
            "Step 83041   [1.773 sec/step, loss=0.60054, avg_loss=0.56442]\n",
            "Step 83042   [1.761 sec/step, loss=0.60085, avg_loss=0.56493]\n",
            "Step 83043   [1.728 sec/step, loss=0.59382, avg_loss=0.56605]\n",
            "Step 83044   [1.735 sec/step, loss=0.54148, avg_loss=0.56586]\n",
            "Step 83045   [1.748 sec/step, loss=0.58184, avg_loss=0.56551]\n",
            "Step 83046   [1.751 sec/step, loss=0.57577, avg_loss=0.56563]\n",
            "Step 83047   [1.744 sec/step, loss=0.57375, avg_loss=0.56619]\n",
            "Generated 16 batches of size 16 in 9.642 sec\n",
            "Step 83048   [1.744 sec/step, loss=0.60515, avg_loss=0.56624]\n",
            "Step 83049   [1.733 sec/step, loss=0.57389, avg_loss=0.56627]\n",
            "Step 83050   [1.739 sec/step, loss=0.51715, avg_loss=0.56538]\n",
            "Step 83051   [1.738 sec/step, loss=0.57572, avg_loss=0.56540]\n",
            "Step 83052   [1.739 sec/step, loss=0.55844, avg_loss=0.56551]\n",
            "Step 83053   [1.739 sec/step, loss=0.58595, avg_loss=0.56542]\n",
            "Step 83054   [1.737 sec/step, loss=0.61091, avg_loss=0.56576]\n",
            "Step 83055   [1.736 sec/step, loss=0.58354, avg_loss=0.56553]\n",
            "Step 83056   [1.738 sec/step, loss=0.55968, avg_loss=0.56554]\n",
            "Step 83057   [1.722 sec/step, loss=0.54235, avg_loss=0.56593]\n",
            "Step 83058   [1.692 sec/step, loss=0.57895, avg_loss=0.56624]\n",
            "Step 83059   [1.704 sec/step, loss=0.59679, avg_loss=0.56611]\n",
            "Step 83060   [1.715 sec/step, loss=0.52629, avg_loss=0.56547]\n",
            "Generated 16 batches of size 16 in 10.037 sec\n",
            "Step 83061   [1.762 sec/step, loss=0.39309, avg_loss=0.56369]\n",
            "Step 83062   [1.776 sec/step, loss=0.55831, avg_loss=0.56370]\n",
            "Step 83063   [1.774 sec/step, loss=0.55658, avg_loss=0.56331]\n",
            "Step 83064   [1.762 sec/step, loss=0.59139, avg_loss=0.56356]\n",
            "Step 83065   [1.755 sec/step, loss=0.59954, avg_loss=0.56345]\n",
            "Step 83066   [1.750 sec/step, loss=0.60039, avg_loss=0.56328]\n",
            "Step 83067   [1.741 sec/step, loss=0.59086, avg_loss=0.56368]\n",
            "Step 83068   [1.752 sec/step, loss=0.55892, avg_loss=0.56353]\n",
            "Step 83069   [1.755 sec/step, loss=0.56118, avg_loss=0.56316]\n",
            "Step 83070   [1.749 sec/step, loss=0.59103, avg_loss=0.56398]\n",
            "Step 83071   [1.753 sec/step, loss=0.50122, avg_loss=0.56377]\n",
            "Step 83072   [1.751 sec/step, loss=0.60072, avg_loss=0.56391]\n",
            "Step 83073   [1.747 sec/step, loss=0.58553, avg_loss=0.56452]\n",
            "Step 83074   [1.787 sec/step, loss=0.40746, avg_loss=0.56298]\n",
            "Step 83075   [1.794 sec/step, loss=0.57172, avg_loss=0.56282]\n",
            "Step 83076   [1.797 sec/step, loss=0.53426, avg_loss=0.56222]\n",
            "Step 83077   [1.800 sec/step, loss=0.60647, avg_loss=0.56242]\n",
            "Step 83078   [1.794 sec/step, loss=0.61342, avg_loss=0.56326]\n",
            "Step 83079   [1.802 sec/step, loss=0.60539, avg_loss=0.56325]\n",
            "Step 83080   [1.798 sec/step, loss=0.59770, avg_loss=0.56335]\n",
            "Generated 16 batches of size 16 in 10.408 sec\n",
            "Step 83081   [1.800 sec/step, loss=0.55080, avg_loss=0.56359]\n",
            "Step 83082   [1.792 sec/step, loss=0.53562, avg_loss=0.56349]\n",
            "Step 83083   [1.749 sec/step, loss=0.57602, avg_loss=0.56462]\n",
            "Step 83084   [1.751 sec/step, loss=0.57661, avg_loss=0.56450]\n",
            "Step 83085   [1.762 sec/step, loss=0.51746, avg_loss=0.56367]\n",
            "Step 83086   [1.745 sec/step, loss=0.57783, avg_loss=0.56444]\n",
            "Step 83087   [1.742 sec/step, loss=0.59529, avg_loss=0.56479]\n",
            "Step 83088   [1.742 sec/step, loss=0.56932, avg_loss=0.56452]\n",
            "Step 83089   [1.743 sec/step, loss=0.57902, avg_loss=0.56494]\n",
            "Step 83090   [1.776 sec/step, loss=0.51154, avg_loss=0.56420]\n",
            "Step 83091   [1.788 sec/step, loss=0.57757, avg_loss=0.56442]\n",
            "Step 83092   [1.807 sec/step, loss=0.54273, avg_loss=0.56410]\n",
            "Step 83093   [1.810 sec/step, loss=0.57916, avg_loss=0.56424]\n",
            "Step 83094   [1.811 sec/step, loss=0.59499, avg_loss=0.56478]\n",
            "Step 83095   [1.810 sec/step, loss=0.61071, avg_loss=0.56484]\n",
            "Generated 16 batches of size 16 in 10.025 sec\n",
            "Step 83096   [1.759 sec/step, loss=0.54347, avg_loss=0.56535]\n",
            "Step 83097   [1.749 sec/step, loss=0.53358, avg_loss=0.56510]\n",
            "Step 83098   [1.733 sec/step, loss=0.59909, avg_loss=0.56566]\n",
            "Step 83099   [1.736 sec/step, loss=0.60453, avg_loss=0.56585]\n",
            "Step 83100   [1.779 sec/step, loss=0.41350, avg_loss=0.56393]\n",
            "Writing summary at step: 83100\n",
            "Step 83101   [1.782 sec/step, loss=0.58078, avg_loss=0.56395]\n",
            "Step 83102   [1.790 sec/step, loss=0.56464, avg_loss=0.56342]\n",
            "Step 83103   [1.783 sec/step, loss=0.58976, avg_loss=0.56390]\n",
            "Step 83104   [1.741 sec/step, loss=0.58447, avg_loss=0.56550]\n",
            "Step 83105   [1.741 sec/step, loss=0.57710, avg_loss=0.56571]\n",
            "Step 83106   [1.744 sec/step, loss=0.56940, avg_loss=0.56545]\n",
            "Step 83107   [1.734 sec/step, loss=0.59517, avg_loss=0.56601]\n",
            "Step 83108   [1.737 sec/step, loss=0.55741, avg_loss=0.56605]\n",
            "Step 83109   [1.738 sec/step, loss=0.61700, avg_loss=0.56612]\n",
            "Step 83110   [1.739 sec/step, loss=0.58040, avg_loss=0.56606]\n",
            "Step 83111   [1.736 sec/step, loss=0.60750, avg_loss=0.56657]\n",
            "Generated 16 batches of size 16 in 9.832 sec\n",
            "Step 83112   [1.729 sec/step, loss=0.57756, avg_loss=0.56635]\n",
            "Step 83113   [1.726 sec/step, loss=0.57735, avg_loss=0.56624]\n",
            "Step 83114   [1.707 sec/step, loss=0.57093, avg_loss=0.56649]\n",
            "Step 83115   [1.710 sec/step, loss=0.55536, avg_loss=0.56634]\n",
            "Step 83116   [1.716 sec/step, loss=0.58619, avg_loss=0.56658]\n",
            "Step 83117   [1.719 sec/step, loss=0.61865, avg_loss=0.56688]\n",
            "Step 83118   [1.738 sec/step, loss=0.51411, avg_loss=0.56614]\n",
            "Step 83119   [1.739 sec/step, loss=0.60499, avg_loss=0.56607]\n",
            "Step 83120   [1.739 sec/step, loss=0.61115, avg_loss=0.56636]\n",
            "Step 83121   [1.739 sec/step, loss=0.55369, avg_loss=0.56636]\n",
            "Step 83122   [1.736 sec/step, loss=0.61420, avg_loss=0.56697]\n",
            "Step 83123   [1.742 sec/step, loss=0.59422, avg_loss=0.56680]\n",
            "Step 83124   [1.740 sec/step, loss=0.57000, avg_loss=0.56710]\n",
            "Step 83125   [1.751 sec/step, loss=0.51580, avg_loss=0.56641]\n",
            "Step 83126   [1.752 sec/step, loss=0.60157, avg_loss=0.56672]\n",
            "Step 83127   [1.750 sec/step, loss=0.55598, avg_loss=0.56636]\n",
            "Generated 16 batches of size 16 in 10.530 sec\n",
            "Step 83128   [1.747 sec/step, loss=0.57594, avg_loss=0.56658]\n",
            "Step 83129   [1.747 sec/step, loss=0.58081, avg_loss=0.56653]\n",
            "Step 83130   [1.748 sec/step, loss=0.52269, avg_loss=0.56630]\n",
            "Step 83131   [1.743 sec/step, loss=0.58855, avg_loss=0.56606]\n",
            "Step 83132   [1.747 sec/step, loss=0.56667, avg_loss=0.56563]\n",
            "Step 83133   [1.758 sec/step, loss=0.55770, avg_loss=0.56545]\n",
            "Step 83134   [1.786 sec/step, loss=0.51285, avg_loss=0.56515]\n",
            "Step 83135   [1.746 sec/step, loss=0.58179, avg_loss=0.56660]\n",
            "Step 83136   [1.762 sec/step, loss=0.50374, avg_loss=0.56652]\n",
            "Step 83137   [1.763 sec/step, loss=0.60213, avg_loss=0.56646]\n",
            "Step 83138   [1.759 sec/step, loss=0.60752, avg_loss=0.56687]\n",
            "Step 83139   [1.749 sec/step, loss=0.58449, avg_loss=0.56745]\n",
            "Step 83140   [1.712 sec/step, loss=0.59829, avg_loss=0.56925]\n",
            "Step 83141   [1.718 sec/step, loss=0.49627, avg_loss=0.56821]\n",
            "Step 83142   [1.717 sec/step, loss=0.54172, avg_loss=0.56762]\n",
            "Step 83143   [1.722 sec/step, loss=0.54863, avg_loss=0.56717]\n",
            "Generated 16 batches of size 16 in 10.636 sec\n",
            "Step 83144   [1.715 sec/step, loss=0.57751, avg_loss=0.56753]\n",
            "Step 83145   [1.712 sec/step, loss=0.56839, avg_loss=0.56739]\n",
            "Step 83146   [1.705 sec/step, loss=0.58343, avg_loss=0.56747]\n",
            "Step 83147   [1.705 sec/step, loss=0.58551, avg_loss=0.56759]\n",
            "Step 83148   [1.704 sec/step, loss=0.58470, avg_loss=0.56738]\n",
            "Step 83149   [1.709 sec/step, loss=0.59451, avg_loss=0.56759]\n",
            "Step 83150   [1.698 sec/step, loss=0.59823, avg_loss=0.56840]\n",
            "Step 83151   [1.704 sec/step, loss=0.58124, avg_loss=0.56846]\n",
            "Step 83152   [1.716 sec/step, loss=0.47880, avg_loss=0.56766]\n",
            "Step 83153   [1.723 sec/step, loss=0.54227, avg_loss=0.56722]\n",
            "Step 83154   [1.772 sec/step, loss=0.49770, avg_loss=0.56609]\n",
            "Step 83155   [1.770 sec/step, loss=0.60902, avg_loss=0.56635]\n",
            "Step 83156   [1.770 sec/step, loss=0.57932, avg_loss=0.56654]\n",
            "Generated 16 batches of size 16 in 9.517 sec\n",
            "Step 83157   [1.775 sec/step, loss=0.54326, avg_loss=0.56655]\n",
            "Step 83158   [1.786 sec/step, loss=0.56106, avg_loss=0.56637]\n",
            "Step 83159   [1.773 sec/step, loss=0.56971, avg_loss=0.56610]\n",
            "Step 83160   [1.762 sec/step, loss=0.60310, avg_loss=0.56687]\n",
            "Step 83161   [1.710 sec/step, loss=0.56615, avg_loss=0.56860]\n",
            "Step 83162   [1.702 sec/step, loss=0.59184, avg_loss=0.56893]\n",
            "Step 83163   [1.705 sec/step, loss=0.59011, avg_loss=0.56927]\n",
            "Step 83164   [1.705 sec/step, loss=0.58964, avg_loss=0.56925]\n",
            "Step 83165   [1.711 sec/step, loss=0.61986, avg_loss=0.56946]\n",
            "Step 83166   [1.727 sec/step, loss=0.52390, avg_loss=0.56869]\n",
            "Step 83167   [1.756 sec/step, loss=0.49665, avg_loss=0.56775]\n",
            "Step 83168   [1.756 sec/step, loss=0.56195, avg_loss=0.56778]\n",
            "Step 83169   [1.749 sec/step, loss=0.59273, avg_loss=0.56809]\n",
            "Step 83170   [1.755 sec/step, loss=0.52594, avg_loss=0.56744]\n",
            "Step 83171   [1.742 sec/step, loss=0.53872, avg_loss=0.56782]\n",
            "Step 83172   [1.748 sec/step, loss=0.59559, avg_loss=0.56777]\n",
            "Step 83173   [1.753 sec/step, loss=0.57340, avg_loss=0.56765]\n",
            "Generated 16 batches of size 16 in 6.948 sec\n",
            "Step 83174   [1.718 sec/step, loss=0.60918, avg_loss=0.56966]\n",
            "Step 83175   [1.714 sec/step, loss=0.53801, avg_loss=0.56933]\n",
            "Step 83176   [1.716 sec/step, loss=0.56088, avg_loss=0.56959]\n",
            "Step 83177   [1.718 sec/step, loss=0.53048, avg_loss=0.56883]\n",
            "Step 83178   [1.717 sec/step, loss=0.61124, avg_loss=0.56881]\n",
            "Step 83179   [1.710 sec/step, loss=0.59834, avg_loss=0.56874]\n",
            "Step 83180   [1.706 sec/step, loss=0.57818, avg_loss=0.56855]\n",
            "Step 83181   [1.705 sec/step, loss=0.60349, avg_loss=0.56907]\n",
            "Step 83182   [1.736 sec/step, loss=0.45854, avg_loss=0.56830]\n",
            "Step 83183   [1.739 sec/step, loss=0.60689, avg_loss=0.56861]\n",
            "Step 83184   [1.737 sec/step, loss=0.58301, avg_loss=0.56867]\n",
            "Step 83185   [1.725 sec/step, loss=0.54132, avg_loss=0.56891]\n",
            "Step 83186   [1.747 sec/step, loss=0.48143, avg_loss=0.56795]\n",
            "Step 83187   [1.751 sec/step, loss=0.56042, avg_loss=0.56760]\n",
            "Generated 16 batches of size 16 in 7.326 sec\n",
            "Step 83188   [1.767 sec/step, loss=0.54891, avg_loss=0.56740]\n",
            "Step 83189   [1.763 sec/step, loss=0.57666, avg_loss=0.56737]\n",
            "Step 83190   [1.733 sec/step, loss=0.54621, avg_loss=0.56772]\n",
            "Step 83191   [1.724 sec/step, loss=0.58434, avg_loss=0.56779]\n",
            "Step 83192   [1.702 sec/step, loss=0.57916, avg_loss=0.56815]\n",
            "Step 83193   [1.704 sec/step, loss=0.59339, avg_loss=0.56829]\n",
            "Step 83194   [1.700 sec/step, loss=0.54612, avg_loss=0.56780]\n",
            "Step 83195   [1.700 sec/step, loss=0.60009, avg_loss=0.56770]\n",
            "Step 83196   [1.700 sec/step, loss=0.52856, avg_loss=0.56755]\n",
            "Step 83197   [1.694 sec/step, loss=0.58244, avg_loss=0.56804]\n",
            "Step 83198   [1.725 sec/step, loss=0.50814, avg_loss=0.56713]\n",
            "Step 83199   [1.729 sec/step, loss=0.57373, avg_loss=0.56682]\n",
            "Step 83200   [1.694 sec/step, loss=0.56369, avg_loss=0.56832]\n",
            "Writing summary at step: 83200\n",
            "Step 83201   [1.700 sec/step, loss=0.54717, avg_loss=0.56799]\n",
            "Step 83202   [1.693 sec/step, loss=0.59320, avg_loss=0.56827]\n",
            "Step 83203   [1.699 sec/step, loss=0.59796, avg_loss=0.56835]\n",
            "Step 83204   [1.719 sec/step, loss=0.49909, avg_loss=0.56750]\n",
            "Generated 16 batches of size 16 in 8.223 sec\n",
            "Step 83205   [1.715 sec/step, loss=0.52766, avg_loss=0.56701]\n",
            "Step 83206   [1.709 sec/step, loss=0.56668, avg_loss=0.56698]\n",
            "Step 83207   [1.706 sec/step, loss=0.58181, avg_loss=0.56684]\n",
            "Step 83208   [1.705 sec/step, loss=0.57388, avg_loss=0.56701]\n",
            "Step 83209   [1.706 sec/step, loss=0.57762, avg_loss=0.56662]\n",
            "Step 83210   [1.703 sec/step, loss=0.58765, avg_loss=0.56669]\n",
            "Step 83211   [1.700 sec/step, loss=0.57917, avg_loss=0.56640]\n",
            "Step 83212   [1.698 sec/step, loss=0.60521, avg_loss=0.56668]\n",
            "Step 83213   [1.696 sec/step, loss=0.56745, avg_loss=0.56658]\n",
            "Step 83214   [1.702 sec/step, loss=0.52338, avg_loss=0.56611]\n",
            "Step 83215   [1.702 sec/step, loss=0.55482, avg_loss=0.56610]\n",
            "Step 83216   [1.704 sec/step, loss=0.56048, avg_loss=0.56584]\n",
            "Step 83217   [1.709 sec/step, loss=0.60227, avg_loss=0.56568]\n",
            "Step 83218   [1.737 sec/step, loss=0.34158, avg_loss=0.56396]\n",
            "Generated 16 batches of size 16 in 8.520 sec\n",
            "Step 83219   [1.740 sec/step, loss=0.60671, avg_loss=0.56397]\n",
            "Step 83220   [1.743 sec/step, loss=0.57193, avg_loss=0.56358]\n",
            "Step 83221   [1.745 sec/step, loss=0.59370, avg_loss=0.56398]\n",
            "Step 83222   [1.748 sec/step, loss=0.59153, avg_loss=0.56375]\n",
            "Step 83223   [1.747 sec/step, loss=0.59450, avg_loss=0.56376]\n",
            "Step 83224   [1.746 sec/step, loss=0.58044, avg_loss=0.56386]\n",
            "Step 83225   [1.748 sec/step, loss=0.52397, avg_loss=0.56394]\n",
            "Step 83226   [1.745 sec/step, loss=0.59332, avg_loss=0.56386]\n",
            "Step 83227   [1.741 sec/step, loss=0.54010, avg_loss=0.56370]\n",
            "Step 83228   [1.777 sec/step, loss=0.38758, avg_loss=0.56182]\n",
            "Step 83229   [1.782 sec/step, loss=0.55293, avg_loss=0.56154]\n",
            "Step 83230   [1.768 sec/step, loss=0.59260, avg_loss=0.56224]\n",
            "Step 83231   [1.775 sec/step, loss=0.56146, avg_loss=0.56197]\n",
            "Step 83232   [1.769 sec/step, loss=0.57715, avg_loss=0.56207]\n",
            "Step 83233   [1.763 sec/step, loss=0.60560, avg_loss=0.56255]\n",
            "Step 83234   [1.735 sec/step, loss=0.53725, avg_loss=0.56280]\n",
            "Step 83235   [1.741 sec/step, loss=0.58424, avg_loss=0.56282]\n",
            "Step 83236   [1.723 sec/step, loss=0.57129, avg_loss=0.56350]\n",
            "Step 83237   [1.724 sec/step, loss=0.60142, avg_loss=0.56349]\n",
            "Generated 16 batches of size 16 in 8.460 sec\n",
            "Step 83238   [1.734 sec/step, loss=0.56588, avg_loss=0.56307]\n",
            "Step 83239   [1.727 sec/step, loss=0.56992, avg_loss=0.56293]\n",
            "Step 83240   [1.724 sec/step, loss=0.60985, avg_loss=0.56304]\n",
            "Step 83241   [1.728 sec/step, loss=0.54793, avg_loss=0.56356]\n",
            "Step 83242   [1.739 sec/step, loss=0.58621, avg_loss=0.56400]\n",
            "Step 83243   [1.735 sec/step, loss=0.59766, avg_loss=0.56449]\n",
            "Step 83244   [1.741 sec/step, loss=0.56183, avg_loss=0.56434]\n",
            "Step 83245   [1.736 sec/step, loss=0.56091, avg_loss=0.56426]\n",
            "Step 83246   [1.733 sec/step, loss=0.58187, avg_loss=0.56425]\n",
            "Step 83247   [1.739 sec/step, loss=0.58562, avg_loss=0.56425]\n",
            "Step 83248   [1.741 sec/step, loss=0.50059, avg_loss=0.56341]\n",
            "Step 83249   [1.749 sec/step, loss=0.57917, avg_loss=0.56325]\n",
            "Step 83250   [1.755 sec/step, loss=0.59678, avg_loss=0.56324]\n",
            "Step 83251   [1.750 sec/step, loss=0.60868, avg_loss=0.56351]\n",
            "Step 83252   [1.740 sec/step, loss=0.56919, avg_loss=0.56442]\n",
            "Step 83253   [1.733 sec/step, loss=0.56354, avg_loss=0.56463]\n",
            "Generated 16 batches of size 16 in 9.520 sec\n",
            "Step 83254   [1.690 sec/step, loss=0.58114, avg_loss=0.56546]\n",
            "Step 83255   [1.717 sec/step, loss=0.51812, avg_loss=0.56455]\n",
            "Step 83256   [1.710 sec/step, loss=0.56830, avg_loss=0.56444]\n",
            "Step 83257   [1.704 sec/step, loss=0.58871, avg_loss=0.56490]\n",
            "Step 83258   [1.700 sec/step, loss=0.54903, avg_loss=0.56478]\n",
            "Step 83259   [1.739 sec/step, loss=0.44709, avg_loss=0.56355]\n",
            "Step 83260   [1.738 sec/step, loss=0.60765, avg_loss=0.56360]\n",
            "Step 83261   [1.748 sec/step, loss=0.53750, avg_loss=0.56331]\n",
            "Step 83262   [1.745 sec/step, loss=0.59383, avg_loss=0.56333]\n",
            "Step 83263   [1.743 sec/step, loss=0.55663, avg_loss=0.56300]\n",
            "Step 83264   [1.745 sec/step, loss=0.59114, avg_loss=0.56301]\n",
            "Step 83265   [1.747 sec/step, loss=0.52931, avg_loss=0.56211]\n",
            "Step 83266   [1.738 sec/step, loss=0.55934, avg_loss=0.56246]\n",
            "Step 83267   [1.725 sec/step, loss=0.55128, avg_loss=0.56301]\n",
            "Step 83268   [1.715 sec/step, loss=0.56329, avg_loss=0.56302]\n",
            "Step 83269   [1.717 sec/step, loss=0.57595, avg_loss=0.56285]\n",
            "Generated 16 batches of size 16 in 9.713 sec\n",
            "Step 83270   [1.714 sec/step, loss=0.57351, avg_loss=0.56333]\n",
            "Step 83271   [1.716 sec/step, loss=0.55225, avg_loss=0.56346]\n",
            "Step 83272   [1.717 sec/step, loss=0.56255, avg_loss=0.56313]\n",
            "Step 83273   [1.716 sec/step, loss=0.60620, avg_loss=0.56346]\n",
            "Step 83274   [1.712 sec/step, loss=0.60479, avg_loss=0.56342]\n",
            "Step 83275   [1.706 sec/step, loss=0.59331, avg_loss=0.56397]\n",
            "Step 83276   [1.706 sec/step, loss=0.54746, avg_loss=0.56384]\n",
            "Step 83277   [1.698 sec/step, loss=0.58219, avg_loss=0.56435]\n",
            "Step 83278   [1.696 sec/step, loss=0.54838, avg_loss=0.56372]\n",
            "Step 83279   [1.696 sec/step, loss=0.57920, avg_loss=0.56353]\n",
            "Step 83280   [1.710 sec/step, loss=0.55845, avg_loss=0.56334]\n",
            "Step 83281   [1.721 sec/step, loss=0.55197, avg_loss=0.56282]\n",
            "Step 83282   [1.689 sec/step, loss=0.58274, avg_loss=0.56406]\n",
            "Step 83283   [1.707 sec/step, loss=0.53342, avg_loss=0.56333]\n",
            "Step 83284   [1.710 sec/step, loss=0.62264, avg_loss=0.56372]\n",
            "Step 83285   [1.711 sec/step, loss=0.59584, avg_loss=0.56427]\n",
            "Generated 16 batches of size 16 in 9.642 sec\n",
            "Step 83286   [1.731 sec/step, loss=0.46715, avg_loss=0.56413]\n",
            "Step 83287   [1.726 sec/step, loss=0.56366, avg_loss=0.56416]\n",
            "Step 83288   [1.715 sec/step, loss=0.56859, avg_loss=0.56436]\n",
            "Step 83289   [1.717 sec/step, loss=0.56977, avg_loss=0.56429]\n",
            "Step 83290   [1.711 sec/step, loss=0.57999, avg_loss=0.56462]\n",
            "Step 83291   [1.750 sec/step, loss=0.41248, avg_loss=0.56291]\n",
            "Step 83292   [1.751 sec/step, loss=0.62152, avg_loss=0.56333]\n",
            "Step 83293   [1.753 sec/step, loss=0.52018, avg_loss=0.56260]\n",
            "Step 83294   [1.747 sec/step, loss=0.55883, avg_loss=0.56272]\n",
            "Step 83295   [1.755 sec/step, loss=0.55563, avg_loss=0.56228]\n",
            "Step 83296   [1.756 sec/step, loss=0.61425, avg_loss=0.56314]\n",
            "Step 83297   [1.762 sec/step, loss=0.60096, avg_loss=0.56332]\n",
            "Step 83298   [1.733 sec/step, loss=0.61614, avg_loss=0.56440]\n",
            "Step 83299   [1.728 sec/step, loss=0.57524, avg_loss=0.56442]\n",
            "Step 83300   [1.721 sec/step, loss=0.54561, avg_loss=0.56424]\n",
            "Writing summary at step: 83300\n",
            "Step 83301   [1.720 sec/step, loss=0.57365, avg_loss=0.56450]\n",
            "Step 83302   [1.719 sec/step, loss=0.58594, avg_loss=0.56443]\n",
            "Generated 16 batches of size 16 in 9.935 sec\n",
            "Step 83303   [1.731 sec/step, loss=0.52700, avg_loss=0.56372]\n",
            "Step 83304   [1.709 sec/step, loss=0.59580, avg_loss=0.56469]\n",
            "Step 83305   [1.708 sec/step, loss=0.60381, avg_loss=0.56545]\n",
            "Step 83306   [1.708 sec/step, loss=0.61082, avg_loss=0.56589]\n",
            "Step 83307   [1.711 sec/step, loss=0.55367, avg_loss=0.56561]\n",
            "Step 83308   [1.743 sec/step, loss=0.42970, avg_loss=0.56417]\n",
            "Step 83309   [1.741 sec/step, loss=0.58936, avg_loss=0.56428]\n",
            "Step 83310   [1.746 sec/step, loss=0.56615, avg_loss=0.56407]\n",
            "Step 83311   [1.748 sec/step, loss=0.57146, avg_loss=0.56399]\n",
            "Step 83312   [1.749 sec/step, loss=0.58189, avg_loss=0.56376]\n",
            "Step 83313   [1.758 sec/step, loss=0.49636, avg_loss=0.56305]\n",
            "Step 83314   [1.757 sec/step, loss=0.56366, avg_loss=0.56345]\n",
            "Step 83315   [1.758 sec/step, loss=0.55979, avg_loss=0.56350]\n",
            "Step 83316   [1.769 sec/step, loss=0.46439, avg_loss=0.56254]\n",
            "Step 83317   [1.764 sec/step, loss=0.56266, avg_loss=0.56214]\n",
            "Generated 16 batches of size 16 in 10.110 sec\n",
            "Step 83318   [1.715 sec/step, loss=0.57999, avg_loss=0.56453]\n",
            "Step 83319   [1.715 sec/step, loss=0.59479, avg_loss=0.56441]\n",
            "Step 83320   [1.720 sec/step, loss=0.56129, avg_loss=0.56430]\n",
            "Step 83321   [1.720 sec/step, loss=0.57705, avg_loss=0.56413]\n",
            "Step 83322   [1.751 sec/step, loss=0.48224, avg_loss=0.56304]\n",
            "Step 83323   [1.758 sec/step, loss=0.54267, avg_loss=0.56252]\n",
            "Step 83324   [1.757 sec/step, loss=0.57190, avg_loss=0.56244]\n",
            "Step 83325   [1.751 sec/step, loss=0.53285, avg_loss=0.56253]\n",
            "Step 83326   [1.748 sec/step, loss=0.56177, avg_loss=0.56221]\n",
            "Step 83327   [1.748 sec/step, loss=0.57630, avg_loss=0.56257]\n",
            "Step 83328   [1.707 sec/step, loss=0.59933, avg_loss=0.56469]\n",
            "Step 83329   [1.711 sec/step, loss=0.53228, avg_loss=0.56448]\n",
            "Step 83330   [1.719 sec/step, loss=0.55914, avg_loss=0.56415]\n",
            "Step 83331   [1.718 sec/step, loss=0.58559, avg_loss=0.56439]\n",
            "Step 83332   [1.727 sec/step, loss=0.54974, avg_loss=0.56412]\n",
            "Step 83333   [1.728 sec/step, loss=0.58592, avg_loss=0.56392]\n",
            "Generated 16 batches of size 16 in 10.529 sec\n",
            "Step 83334   [1.716 sec/step, loss=0.57063, avg_loss=0.56425]\n",
            "Step 83335   [1.710 sec/step, loss=0.60391, avg_loss=0.56445]\n",
            "Step 83336   [1.712 sec/step, loss=0.60290, avg_loss=0.56477]\n",
            "Step 83337   [1.707 sec/step, loss=0.61923, avg_loss=0.56494]\n",
            "Step 83338   [1.698 sec/step, loss=0.61737, avg_loss=0.56546]\n",
            "Step 83339   [1.702 sec/step, loss=0.57403, avg_loss=0.56550]\n",
            "Step 83340   [1.710 sec/step, loss=0.50927, avg_loss=0.56450]\n",
            "Step 83341   [1.703 sec/step, loss=0.57331, avg_loss=0.56475]\n",
            "Step 83342   [1.706 sec/step, loss=0.53894, avg_loss=0.56428]\n",
            "Step 83343   [1.703 sec/step, loss=0.60503, avg_loss=0.56435]\n",
            "Step 83344   [1.708 sec/step, loss=0.54045, avg_loss=0.56414]\n",
            "Step 83345   [1.758 sec/step, loss=0.46550, avg_loss=0.56318]\n",
            "Step 83346   [1.760 sec/step, loss=0.55250, avg_loss=0.56289]\n",
            "Generated 16 batches of size 16 in 10.915 sec\n",
            "Step 83347   [1.762 sec/step, loss=0.57779, avg_loss=0.56281]\n",
            "Step 83348   [1.760 sec/step, loss=0.55794, avg_loss=0.56338]\n",
            "Step 83349   [1.751 sec/step, loss=0.60065, avg_loss=0.56360]\n",
            "Step 83350   [1.749 sec/step, loss=0.57485, avg_loss=0.56338]\n",
            "Step 83351   [1.749 sec/step, loss=0.63954, avg_loss=0.56369]\n",
            "Step 83352   [1.740 sec/step, loss=0.59835, avg_loss=0.56398]\n",
            "Step 83353   [1.780 sec/step, loss=0.52092, avg_loss=0.56355]\n",
            "Step 83354   [1.776 sec/step, loss=0.60901, avg_loss=0.56383]\n",
            "Step 83355   [1.751 sec/step, loss=0.57689, avg_loss=0.56442]\n",
            "Step 83356   [1.756 sec/step, loss=0.58571, avg_loss=0.56459]\n",
            "Step 83357   [1.759 sec/step, loss=0.55809, avg_loss=0.56429]\n",
            "Step 83358   [1.754 sec/step, loss=0.57589, avg_loss=0.56456]\n",
            "Step 83359   [1.713 sec/step, loss=0.58998, avg_loss=0.56598]\n",
            "Step 83360   [1.716 sec/step, loss=0.57884, avg_loss=0.56570]\n",
            "Step 83361   [1.716 sec/step, loss=0.58532, avg_loss=0.56617]\n",
            "Step 83362   [1.725 sec/step, loss=0.57966, avg_loss=0.56603]\n",
            "Step 83363   [1.728 sec/step, loss=0.58489, avg_loss=0.56632]\n",
            "Generated 16 batches of size 16 in 8.090 sec\n",
            "Step 83364   [1.731 sec/step, loss=0.58983, avg_loss=0.56630]\n",
            "Step 83365   [1.734 sec/step, loss=0.58811, avg_loss=0.56689]\n",
            "Step 83366   [1.737 sec/step, loss=0.60545, avg_loss=0.56735]\n",
            "Step 83367   [1.718 sec/step, loss=0.58337, avg_loss=0.56767]\n",
            "Step 83368   [1.737 sec/step, loss=0.55900, avg_loss=0.56763]\n",
            "Step 83369   [1.747 sec/step, loss=0.53930, avg_loss=0.56726]\n",
            "Step 83370   [1.745 sec/step, loss=0.56849, avg_loss=0.56721]\n",
            "Step 83371   [1.757 sec/step, loss=0.51864, avg_loss=0.56688]\n",
            "Step 83372   [1.793 sec/step, loss=0.41123, avg_loss=0.56536]\n",
            "Step 83373   [1.788 sec/step, loss=0.56991, avg_loss=0.56500]\n",
            "Step 83374   [1.788 sec/step, loss=0.55219, avg_loss=0.56447]\n",
            "Step 83375   [1.790 sec/step, loss=0.58467, avg_loss=0.56439]\n",
            "Step 83376   [1.795 sec/step, loss=0.54239, avg_loss=0.56434]\n",
            "Step 83377   [1.803 sec/step, loss=0.53522, avg_loss=0.56387]\n",
            "Step 83378   [1.808 sec/step, loss=0.57740, avg_loss=0.56416]\n",
            "Generated 16 batches of size 16 in 7.079 sec\n",
            "Step 83379   [1.818 sec/step, loss=0.58562, avg_loss=0.56422]\n",
            "Step 83380   [1.806 sec/step, loss=0.61147, avg_loss=0.56475]\n",
            "Step 83381   [1.796 sec/step, loss=0.62605, avg_loss=0.56549]\n",
            "Step 83382   [1.799 sec/step, loss=0.58448, avg_loss=0.56551]\n",
            "Step 83383   [1.777 sec/step, loss=0.60344, avg_loss=0.56621]\n",
            "Step 83384   [1.773 sec/step, loss=0.57419, avg_loss=0.56573]\n",
            "Step 83385   [1.773 sec/step, loss=0.60746, avg_loss=0.56584]\n",
            "Step 83386   [1.746 sec/step, loss=0.54547, avg_loss=0.56663]\n",
            "Step 83387   [1.786 sec/step, loss=0.40546, avg_loss=0.56504]\n",
            "Step 83388   [1.782 sec/step, loss=0.57309, avg_loss=0.56509]\n",
            "Step 83389   [1.782 sec/step, loss=0.54934, avg_loss=0.56488]\n",
            "Step 83390   [1.780 sec/step, loss=0.59418, avg_loss=0.56503]\n",
            "Step 83391   [1.743 sec/step, loss=0.58888, avg_loss=0.56679]\n",
            "Step 83392   [1.752 sec/step, loss=0.56034, avg_loss=0.56618]\n",
            "Step 83393   [1.758 sec/step, loss=0.52057, avg_loss=0.56618]\n",
            "Step 83394   [1.761 sec/step, loss=0.56465, avg_loss=0.56624]\n",
            "Generated 16 batches of size 16 in 7.618 sec\n",
            "Step 83395   [1.767 sec/step, loss=0.59962, avg_loss=0.56668]\n",
            "Step 83396   [1.766 sec/step, loss=0.61276, avg_loss=0.56667]\n",
            "Step 83397   [1.764 sec/step, loss=0.56734, avg_loss=0.56633]\n",
            "Step 83398   [1.758 sec/step, loss=0.57243, avg_loss=0.56589]\n",
            "Step 83399   [1.759 sec/step, loss=0.60465, avg_loss=0.56619]\n",
            "Step 83400   [1.759 sec/step, loss=0.58377, avg_loss=0.56657]\n",
            "Writing summary at step: 83400\n",
            "Step 83401   [1.765 sec/step, loss=0.52068, avg_loss=0.56604]\n",
            "Step 83402   [1.805 sec/step, loss=0.41826, avg_loss=0.56436]\n",
            "Step 83403   [1.788 sec/step, loss=0.58907, avg_loss=0.56498]\n",
            "Step 83404   [1.787 sec/step, loss=0.61745, avg_loss=0.56520]\n",
            "Step 83405   [1.786 sec/step, loss=0.59027, avg_loss=0.56506]\n",
            "Step 83406   [1.793 sec/step, loss=0.58434, avg_loss=0.56480]\n",
            "Step 83407   [1.789 sec/step, loss=0.59386, avg_loss=0.56520]\n",
            "Step 83408   [1.759 sec/step, loss=0.54391, avg_loss=0.56634]\n",
            "Step 83409   [1.763 sec/step, loss=0.62400, avg_loss=0.56669]\n",
            "Step 83410   [1.766 sec/step, loss=0.60945, avg_loss=0.56712]\n",
            "Step 83411   [1.765 sec/step, loss=0.57469, avg_loss=0.56715]\n",
            "Generated 16 batches of size 16 in 8.133 sec\n",
            "Step 83412   [1.765 sec/step, loss=0.58480, avg_loss=0.56718]\n",
            "Step 83413   [1.763 sec/step, loss=0.54337, avg_loss=0.56765]\n",
            "Step 83414   [1.759 sec/step, loss=0.60343, avg_loss=0.56805]\n",
            "Step 83415   [1.761 sec/step, loss=0.57425, avg_loss=0.56820]\n",
            "Step 83416   [1.740 sec/step, loss=0.59182, avg_loss=0.56947]\n",
            "Step 83417   [1.751 sec/step, loss=0.53674, avg_loss=0.56921]\n",
            "Step 83418   [1.749 sec/step, loss=0.58706, avg_loss=0.56928]\n",
            "Step 83419   [1.763 sec/step, loss=0.49506, avg_loss=0.56828]\n",
            "Step 83420   [1.757 sec/step, loss=0.57172, avg_loss=0.56839]\n",
            "Step 83421   [1.757 sec/step, loss=0.60977, avg_loss=0.56872]\n",
            "Step 83422   [1.718 sec/step, loss=0.60950, avg_loss=0.56999]\n",
            "Step 83423   [1.710 sec/step, loss=0.58865, avg_loss=0.57045]\n",
            "Step 83424   [1.720 sec/step, loss=0.53227, avg_loss=0.57005]\n",
            "Step 83425   [1.718 sec/step, loss=0.60325, avg_loss=0.57076]\n",
            "Step 83426   [1.720 sec/step, loss=0.55607, avg_loss=0.57070]\n",
            "Generated 16 batches of size 16 in 8.264 sec\n",
            "Step 83427   [1.728 sec/step, loss=0.58379, avg_loss=0.57077]\n",
            "Step 83428   [1.734 sec/step, loss=0.57587, avg_loss=0.57054]\n",
            "Step 83429   [1.768 sec/step, loss=0.45557, avg_loss=0.56977]\n",
            "Step 83430   [1.764 sec/step, loss=0.60338, avg_loss=0.57021]\n",
            "Step 83431   [1.760 sec/step, loss=0.61778, avg_loss=0.57054]\n",
            "Step 83432   [1.749 sec/step, loss=0.55415, avg_loss=0.57058]\n",
            "Step 83433   [1.746 sec/step, loss=0.60544, avg_loss=0.57078]\n",
            "Step 83434   [1.751 sec/step, loss=0.56262, avg_loss=0.57070]\n",
            "Step 83435   [1.792 sec/step, loss=0.41389, avg_loss=0.56880]\n",
            "Step 83436   [1.788 sec/step, loss=0.60053, avg_loss=0.56877]\n",
            "Step 83437   [1.790 sec/step, loss=0.62591, avg_loss=0.56884]\n",
            "Step 83438   [1.791 sec/step, loss=0.57534, avg_loss=0.56842]\n",
            "Step 83439   [1.801 sec/step, loss=0.57990, avg_loss=0.56848]\n",
            "Step 83440   [1.798 sec/step, loss=0.59035, avg_loss=0.56929]\n",
            "Step 83441   [1.799 sec/step, loss=0.56163, avg_loss=0.56917]\n",
            "Step 83442   [1.792 sec/step, loss=0.51398, avg_loss=0.56892]\n",
            "Step 83443   [1.794 sec/step, loss=0.53430, avg_loss=0.56821]\n",
            "Generated 16 batches of size 16 in 9.509 sec\n",
            "Step 83444   [1.779 sec/step, loss=0.56004, avg_loss=0.56841]\n",
            "Step 83445   [1.734 sec/step, loss=0.53942, avg_loss=0.56915]\n",
            "Step 83446   [1.734 sec/step, loss=0.59521, avg_loss=0.56958]\n",
            "Step 83447   [1.739 sec/step, loss=0.56239, avg_loss=0.56942]\n",
            "Step 83448   [1.741 sec/step, loss=0.58069, avg_loss=0.56965]\n",
            "Step 83449   [1.746 sec/step, loss=0.56663, avg_loss=0.56931]\n",
            "Step 83450   [1.749 sec/step, loss=0.60768, avg_loss=0.56964]\n",
            "Step 83451   [1.751 sec/step, loss=0.55074, avg_loss=0.56875]\n",
            "Step 83452   [1.754 sec/step, loss=0.57392, avg_loss=0.56851]\n",
            "Step 83453   [1.710 sec/step, loss=0.59864, avg_loss=0.56928]\n",
            "Step 83454   [1.709 sec/step, loss=0.60765, avg_loss=0.56927]\n",
            "Step 83455   [1.757 sec/step, loss=0.41824, avg_loss=0.56768]\n",
            "Step 83456   [1.755 sec/step, loss=0.58296, avg_loss=0.56766]\n",
            "Step 83457   [1.758 sec/step, loss=0.60073, avg_loss=0.56808]\n",
            "Generated 16 batches of size 16 in 10.105 sec\n",
            "Step 83458   [1.760 sec/step, loss=0.59863, avg_loss=0.56831]\n",
            "Step 83459   [1.763 sec/step, loss=0.57121, avg_loss=0.56812]\n",
            "Step 83460   [1.758 sec/step, loss=0.58876, avg_loss=0.56822]\n",
            "Step 83461   [1.763 sec/step, loss=0.51967, avg_loss=0.56756]\n",
            "Step 83462   [1.762 sec/step, loss=0.56536, avg_loss=0.56742]\n",
            "Step 83463   [1.758 sec/step, loss=0.57563, avg_loss=0.56733]\n",
            "Step 83464   [1.772 sec/step, loss=0.52324, avg_loss=0.56666]\n",
            "Step 83465   [1.762 sec/step, loss=0.60437, avg_loss=0.56683]\n",
            "Step 83466   [1.751 sec/step, loss=0.58355, avg_loss=0.56661]\n",
            "Step 83467   [1.757 sec/step, loss=0.55387, avg_loss=0.56631]\n",
            "Step 83468   [1.747 sec/step, loss=0.55856, avg_loss=0.56631]\n",
            "Step 83469   [1.741 sec/step, loss=0.55362, avg_loss=0.56645]\n",
            "Step 83470   [1.745 sec/step, loss=0.57278, avg_loss=0.56649]\n",
            "Step 83471   [1.781 sec/step, loss=0.47265, avg_loss=0.56603]\n",
            "Step 83472   [1.747 sec/step, loss=0.58831, avg_loss=0.56780]\n",
            "Step 83473   [1.753 sec/step, loss=0.60046, avg_loss=0.56811]\n",
            "Generated 16 batches of size 16 in 10.142 sec\n",
            "Step 83474   [1.752 sec/step, loss=0.56341, avg_loss=0.56822]\n",
            "Step 83475   [1.752 sec/step, loss=0.58248, avg_loss=0.56820]\n",
            "Step 83476   [1.742 sec/step, loss=0.60969, avg_loss=0.56887]\n",
            "Step 83477   [1.734 sec/step, loss=0.57199, avg_loss=0.56924]\n",
            "Step 83478   [1.726 sec/step, loss=0.57087, avg_loss=0.56917]\n",
            "Step 83479   [1.719 sec/step, loss=0.60121, avg_loss=0.56933]\n",
            "Step 83480   [1.761 sec/step, loss=0.38571, avg_loss=0.56707]\n",
            "Step 83481   [1.769 sec/step, loss=0.57980, avg_loss=0.56661]\n",
            "Step 83482   [1.767 sec/step, loss=0.61860, avg_loss=0.56695]\n",
            "Step 83483   [1.767 sec/step, loss=0.53981, avg_loss=0.56632]\n",
            "Step 83484   [1.768 sec/step, loss=0.56410, avg_loss=0.56621]\n",
            "Step 83485   [1.769 sec/step, loss=0.60208, avg_loss=0.56616]\n",
            "Step 83486   [1.754 sec/step, loss=0.59060, avg_loss=0.56661]\n",
            "Step 83487   [1.734 sec/step, loss=0.52097, avg_loss=0.56777]\n",
            "Step 83488   [1.737 sec/step, loss=0.57112, avg_loss=0.56775]\n",
            "Step 83489   [1.746 sec/step, loss=0.58594, avg_loss=0.56811]\n",
            "Step 83490   [1.747 sec/step, loss=0.58069, avg_loss=0.56798]\n",
            "Step 83491   [1.746 sec/step, loss=0.56271, avg_loss=0.56772]\n",
            "Generated 16 batches of size 16 in 10.438 sec\n",
            "Step 83492   [1.741 sec/step, loss=0.58109, avg_loss=0.56792]\n",
            "Step 83493   [1.731 sec/step, loss=0.58693, avg_loss=0.56859]\n",
            "Step 83494   [1.736 sec/step, loss=0.57197, avg_loss=0.56866]\n",
            "Step 83495   [1.727 sec/step, loss=0.55694, avg_loss=0.56823]\n",
            "Step 83496   [1.735 sec/step, loss=0.56012, avg_loss=0.56771]\n",
            "Step 83497   [1.742 sec/step, loss=0.57025, avg_loss=0.56774]\n",
            "Step 83498   [1.745 sec/step, loss=0.63837, avg_loss=0.56840]\n",
            "Step 83499   [1.747 sec/step, loss=0.54809, avg_loss=0.56783]\n",
            "Step 83500   [1.749 sec/step, loss=0.57138, avg_loss=0.56771]\n",
            "Writing summary at step: 83500\n",
            "Step 83501   [1.739 sec/step, loss=0.56649, avg_loss=0.56816]\n",
            "Step 83502   [1.707 sec/step, loss=0.57766, avg_loss=0.56976]\n",
            "Step 83503   [1.711 sec/step, loss=0.59930, avg_loss=0.56986]\n",
            "Step 83504   [1.712 sec/step, loss=0.58068, avg_loss=0.56949]\n",
            "Generated 16 batches of size 16 in 9.955 sec\n",
            "Step 83505   [1.763 sec/step, loss=0.53013, avg_loss=0.56889]\n",
            "Step 83506   [1.758 sec/step, loss=0.60572, avg_loss=0.56911]\n",
            "Step 83507   [1.756 sec/step, loss=0.55349, avg_loss=0.56870]\n",
            "Step 83508   [1.747 sec/step, loss=0.51368, avg_loss=0.56840]\n",
            "Step 83509   [1.761 sec/step, loss=0.50118, avg_loss=0.56717]\n",
            "Step 83510   [1.755 sec/step, loss=0.59843, avg_loss=0.56706]\n",
            "Step 83511   [1.749 sec/step, loss=0.57711, avg_loss=0.56709]\n",
            "Step 83512   [1.750 sec/step, loss=0.54697, avg_loss=0.56671]\n",
            "Step 83513   [1.746 sec/step, loss=0.57813, avg_loss=0.56705]\n",
            "Step 83514   [1.747 sec/step, loss=0.59522, avg_loss=0.56697]\n",
            "Step 83515   [1.741 sec/step, loss=0.56814, avg_loss=0.56691]\n",
            "Step 83516   [1.746 sec/step, loss=0.53971, avg_loss=0.56639]\n",
            "Step 83517   [1.737 sec/step, loss=0.61103, avg_loss=0.56713]\n",
            "Step 83518   [1.736 sec/step, loss=0.56777, avg_loss=0.56694]\n",
            "Step 83519   [1.772 sec/step, loss=0.40826, avg_loss=0.56607]\n",
            "Step 83520   [1.773 sec/step, loss=0.60059, avg_loss=0.56636]\n",
            "Generated 16 batches of size 16 in 10.322 sec\n",
            "Step 83521   [1.788 sec/step, loss=0.56081, avg_loss=0.56587]\n",
            "Step 83522   [1.787 sec/step, loss=0.54761, avg_loss=0.56525]\n",
            "Step 83523   [1.791 sec/step, loss=0.55835, avg_loss=0.56495]\n",
            "Step 83524   [1.788 sec/step, loss=0.53123, avg_loss=0.56494]\n",
            "Step 83525   [1.785 sec/step, loss=0.59486, avg_loss=0.56486]\n",
            "Step 83526   [1.791 sec/step, loss=0.53793, avg_loss=0.56467]\n",
            "Step 83527   [1.812 sec/step, loss=0.50804, avg_loss=0.56392]\n",
            "Step 83528   [1.804 sec/step, loss=0.59472, avg_loss=0.56410]\n",
            "Step 83529   [1.764 sec/step, loss=0.56770, avg_loss=0.56523]\n",
            "Step 83530   [1.760 sec/step, loss=0.58181, avg_loss=0.56501]\n",
            "Step 83531   [1.769 sec/step, loss=0.55777, avg_loss=0.56441]\n",
            "Step 83532   [1.775 sec/step, loss=0.57265, avg_loss=0.56460]\n",
            "Step 83533   [1.773 sec/step, loss=0.59384, avg_loss=0.56448]\n",
            "Step 83534   [1.774 sec/step, loss=0.55465, avg_loss=0.56440]\n",
            "Step 83535   [1.733 sec/step, loss=0.61832, avg_loss=0.56644]\n",
            "Step 83536   [1.755 sec/step, loss=0.52227, avg_loss=0.56566]\n",
            "Step 83537   [1.754 sec/step, loss=0.55848, avg_loss=0.56499]\n",
            "Step 83538   [1.754 sec/step, loss=0.60102, avg_loss=0.56524]\n",
            "Step 83539   [1.747 sec/step, loss=0.58084, avg_loss=0.56525]\n",
            "Generated 16 batches of size 16 in 10.409 sec\n",
            "Step 83540   [1.751 sec/step, loss=0.56166, avg_loss=0.56497]\n",
            "Step 83541   [1.750 sec/step, loss=0.55270, avg_loss=0.56488]\n",
            "Step 83542   [1.744 sec/step, loss=0.53717, avg_loss=0.56511]\n",
            "Step 83543   [1.748 sec/step, loss=0.53819, avg_loss=0.56515]\n",
            "Step 83544   [1.750 sec/step, loss=0.55952, avg_loss=0.56514]\n",
            "Step 83545   [1.743 sec/step, loss=0.58650, avg_loss=0.56561]\n",
            "Step 83546   [1.749 sec/step, loss=0.52296, avg_loss=0.56489]\n",
            "Step 83547   [1.735 sec/step, loss=0.56838, avg_loss=0.56495]\n",
            "Step 83548   [1.735 sec/step, loss=0.57759, avg_loss=0.56492]\n",
            "Step 83549   [1.729 sec/step, loss=0.53562, avg_loss=0.56461]\n",
            "Step 83550   [1.744 sec/step, loss=0.49388, avg_loss=0.56347]\n",
            "Step 83551   [1.755 sec/step, loss=0.51508, avg_loss=0.56312]\n",
            "Generated 16 batches of size 16 in 9.992 sec\n",
            "Step 83552   [1.797 sec/step, loss=0.48341, avg_loss=0.56221]\n",
            "Step 83553   [1.798 sec/step, loss=0.58472, avg_loss=0.56207]\n",
            "Step 83554   [1.801 sec/step, loss=0.60042, avg_loss=0.56200]\n",
            "Step 83555   [1.752 sec/step, loss=0.56663, avg_loss=0.56348]\n",
            "Step 83556   [1.756 sec/step, loss=0.60440, avg_loss=0.56370]\n",
            "Step 83557   [1.746 sec/step, loss=0.56431, avg_loss=0.56333]\n",
            "Step 83558   [1.746 sec/step, loss=0.55382, avg_loss=0.56288]\n",
            "Step 83559   [1.745 sec/step, loss=0.57637, avg_loss=0.56294]\n",
            "Step 83560   [1.745 sec/step, loss=0.57703, avg_loss=0.56282]\n",
            "Step 83561   [1.736 sec/step, loss=0.60033, avg_loss=0.56363]\n",
            "Step 83562   [1.726 sec/step, loss=0.62470, avg_loss=0.56422]\n",
            "Step 83563   [1.732 sec/step, loss=0.57495, avg_loss=0.56421]\n",
            "Step 83564   [1.730 sec/step, loss=0.54584, avg_loss=0.56444]\n",
            "Step 83565   [1.727 sec/step, loss=0.58045, avg_loss=0.56420]\n",
            "Step 83566   [1.733 sec/step, loss=0.58489, avg_loss=0.56421]\n",
            "Step 83567   [1.734 sec/step, loss=0.53775, avg_loss=0.56405]\n",
            "Step 83568   [1.742 sec/step, loss=0.58127, avg_loss=0.56428]\n",
            "Generated 16 batches of size 16 in 7.263 sec\n",
            "Step 83569   [1.743 sec/step, loss=0.60289, avg_loss=0.56477]\n",
            "Step 83570   [1.746 sec/step, loss=0.53220, avg_loss=0.56437]\n",
            "Step 83571   [1.695 sec/step, loss=0.55338, avg_loss=0.56517]\n",
            "Step 83572   [1.732 sec/step, loss=0.42669, avg_loss=0.56356]\n",
            "Step 83573   [1.731 sec/step, loss=0.59791, avg_loss=0.56353]\n",
            "Step 83574   [1.731 sec/step, loss=0.56607, avg_loss=0.56356]\n",
            "Step 83575   [1.734 sec/step, loss=0.57872, avg_loss=0.56352]\n",
            "Step 83576   [1.731 sec/step, loss=0.59955, avg_loss=0.56342]\n",
            "Step 83577   [1.732 sec/step, loss=0.58703, avg_loss=0.56357]\n",
            "Step 83578   [1.740 sec/step, loss=0.60217, avg_loss=0.56388]\n",
            "Step 83579   [1.750 sec/step, loss=0.54657, avg_loss=0.56334]\n",
            "Step 83580   [1.707 sec/step, loss=0.53037, avg_loss=0.56478]\n",
            "Step 83581   [1.701 sec/step, loss=0.54002, avg_loss=0.56438]\n",
            "Step 83582   [1.697 sec/step, loss=0.59102, avg_loss=0.56411]\n",
            "Step 83583   [1.708 sec/step, loss=0.60600, avg_loss=0.56477]\n",
            "Generated 16 batches of size 16 in 7.406 sec\n",
            "Step 83584   [1.756 sec/step, loss=0.45487, avg_loss=0.56368]\n",
            "Step 83585   [1.758 sec/step, loss=0.60736, avg_loss=0.56373]\n",
            "Step 83586   [1.760 sec/step, loss=0.61697, avg_loss=0.56399]\n",
            "Step 83587   [1.749 sec/step, loss=0.55404, avg_loss=0.56433]\n",
            "Step 83588   [1.745 sec/step, loss=0.58762, avg_loss=0.56449]\n",
            "Step 83589   [1.736 sec/step, loss=0.61009, avg_loss=0.56473]\n",
            "Step 83590   [1.740 sec/step, loss=0.59283, avg_loss=0.56485]\n",
            "Step 83591   [1.739 sec/step, loss=0.57286, avg_loss=0.56495]\n",
            "Step 83592   [1.776 sec/step, loss=0.38175, avg_loss=0.56296]\n",
            "Step 83593   [1.772 sec/step, loss=0.59613, avg_loss=0.56305]\n",
            "Step 83594   [1.771 sec/step, loss=0.55036, avg_loss=0.56284]\n",
            "Step 83595   [1.768 sec/step, loss=0.53421, avg_loss=0.56261]\n",
            "Step 83596   [1.759 sec/step, loss=0.59352, avg_loss=0.56294]\n",
            "Step 83597   [1.758 sec/step, loss=0.58979, avg_loss=0.56314]\n",
            "Step 83598   [1.770 sec/step, loss=0.57462, avg_loss=0.56250]\n",
            "Step 83599   [1.776 sec/step, loss=0.57304, avg_loss=0.56275]\n",
            "Step 83600   [1.775 sec/step, loss=0.58320, avg_loss=0.56287]\n",
            "Writing summary at step: 83600\n",
            "Generated 16 batches of size 16 in 7.748 sec\n",
            "Step 83601   [1.789 sec/step, loss=0.51793, avg_loss=0.56238]\n",
            "Step 83602   [1.783 sec/step, loss=0.55404, avg_loss=0.56215]\n",
            "Step 83603   [1.781 sec/step, loss=0.58225, avg_loss=0.56198]\n",
            "Step 83604   [1.778 sec/step, loss=0.56747, avg_loss=0.56185]\n",
            "Step 83605   [1.728 sec/step, loss=0.61964, avg_loss=0.56274]\n",
            "Step 83606   [1.732 sec/step, loss=0.54653, avg_loss=0.56215]\n",
            "Step 83607   [1.733 sec/step, loss=0.58994, avg_loss=0.56251]\n",
            "Step 83608   [1.728 sec/step, loss=0.58936, avg_loss=0.56327]\n",
            "Step 83609   [1.716 sec/step, loss=0.56485, avg_loss=0.56391]\n",
            "Step 83610   [1.718 sec/step, loss=0.60042, avg_loss=0.56393]\n",
            "Step 83611   [1.732 sec/step, loss=0.50427, avg_loss=0.56320]\n",
            "Step 83612   [1.729 sec/step, loss=0.58080, avg_loss=0.56354]\n",
            "Step 83613   [1.730 sec/step, loss=0.57949, avg_loss=0.56355]\n",
            "Step 83614   [1.737 sec/step, loss=0.57034, avg_loss=0.56330]\n",
            "Step 83615   [1.750 sec/step, loss=0.52930, avg_loss=0.56291]\n",
            "Generated 16 batches of size 16 in 7.761 sec\n",
            "Step 83616   [1.762 sec/step, loss=0.55044, avg_loss=0.56302]\n",
            "Step 83617   [1.762 sec/step, loss=0.60391, avg_loss=0.56295]\n",
            "Step 83618   [1.758 sec/step, loss=0.58102, avg_loss=0.56308]\n",
            "Step 83619   [1.734 sec/step, loss=0.54696, avg_loss=0.56447]\n",
            "Step 83620   [1.732 sec/step, loss=0.56272, avg_loss=0.56409]\n",
            "Step 83621   [1.720 sec/step, loss=0.56077, avg_loss=0.56409]\n",
            "Step 83622   [1.729 sec/step, loss=0.52102, avg_loss=0.56382]\n",
            "Step 83623   [1.722 sec/step, loss=0.58387, avg_loss=0.56408]\n",
            "Step 83624   [1.716 sec/step, loss=0.60956, avg_loss=0.56486]\n",
            "Step 83625   [1.712 sec/step, loss=0.57477, avg_loss=0.56466]\n",
            "Step 83626   [1.709 sec/step, loss=0.60684, avg_loss=0.56535]\n",
            "Step 83627   [1.682 sec/step, loss=0.56574, avg_loss=0.56593]\n",
            "Step 83628   [1.697 sec/step, loss=0.55600, avg_loss=0.56554]\n",
            "Step 83629   [1.699 sec/step, loss=0.55722, avg_loss=0.56544]\n",
            "Step 83630   [1.701 sec/step, loss=0.61627, avg_loss=0.56578]\n",
            "Step 83631   [1.704 sec/step, loss=0.51955, avg_loss=0.56540]\n",
            "Step 83632   [1.702 sec/step, loss=0.55025, avg_loss=0.56517]\n",
            "Generated 16 batches of size 16 in 9.625 sec\n",
            "Step 83633   [1.743 sec/step, loss=0.42645, avg_loss=0.56350]\n",
            "Step 83634   [1.742 sec/step, loss=0.58264, avg_loss=0.56378]\n",
            "Step 83635   [1.743 sec/step, loss=0.58458, avg_loss=0.56344]\n",
            "Step 83636   [1.721 sec/step, loss=0.57705, avg_loss=0.56399]\n",
            "Step 83637   [1.720 sec/step, loss=0.58267, avg_loss=0.56423]\n",
            "Step 83638   [1.725 sec/step, loss=0.59066, avg_loss=0.56413]\n",
            "Step 83639   [1.720 sec/step, loss=0.60282, avg_loss=0.56435]\n",
            "Step 83640   [1.708 sec/step, loss=0.59096, avg_loss=0.56464]\n",
            "Step 83641   [1.701 sec/step, loss=0.59739, avg_loss=0.56509]\n",
            "Step 83642   [1.709 sec/step, loss=0.55333, avg_loss=0.56525]\n",
            "Step 83643   [1.744 sec/step, loss=0.40651, avg_loss=0.56393]\n",
            "Step 83644   [1.744 sec/step, loss=0.60668, avg_loss=0.56440]\n",
            "Step 83645   [1.762 sec/step, loss=0.55851, avg_loss=0.56412]\n",
            "Step 83646   [1.757 sec/step, loss=0.57447, avg_loss=0.56464]\n",
            "Step 83647   [1.768 sec/step, loss=0.51531, avg_loss=0.56411]\n",
            "Step 83648   [1.771 sec/step, loss=0.58849, avg_loss=0.56422]\n",
            "Step 83649   [1.773 sec/step, loss=0.60326, avg_loss=0.56489]\n",
            "Generated 16 batches of size 16 in 9.823 sec\n",
            "Step 83650   [1.755 sec/step, loss=0.61078, avg_loss=0.56606]\n",
            "Step 83651   [1.744 sec/step, loss=0.55562, avg_loss=0.56647]\n",
            "Step 83652   [1.701 sec/step, loss=0.55685, avg_loss=0.56720]\n",
            "Step 83653   [1.706 sec/step, loss=0.57184, avg_loss=0.56707]\n",
            "Step 83654   [1.718 sec/step, loss=0.51544, avg_loss=0.56622]\n",
            "Step 83655   [1.720 sec/step, loss=0.55183, avg_loss=0.56608]\n",
            "Step 83656   [1.716 sec/step, loss=0.62040, avg_loss=0.56624]\n",
            "Step 83657   [1.720 sec/step, loss=0.56557, avg_loss=0.56625]\n",
            "Step 83658   [1.725 sec/step, loss=0.54871, avg_loss=0.56620]\n",
            "Step 83659   [1.731 sec/step, loss=0.52101, avg_loss=0.56564]\n",
            "Step 83660   [1.736 sec/step, loss=0.55693, avg_loss=0.56544]\n",
            "Step 83661   [1.733 sec/step, loss=0.61863, avg_loss=0.56563]\n",
            "Step 83662   [1.733 sec/step, loss=0.59761, avg_loss=0.56536]\n",
            "Step 83663   [1.736 sec/step, loss=0.59051, avg_loss=0.56551]\n",
            "Step 83664   [1.728 sec/step, loss=0.55169, avg_loss=0.56557]\n",
            "Step 83665   [1.731 sec/step, loss=0.60740, avg_loss=0.56584]\n",
            "Step 83666   [1.728 sec/step, loss=0.55695, avg_loss=0.56556]\n",
            "Step 83667   [1.724 sec/step, loss=0.60301, avg_loss=0.56621]\n",
            "Generated 16 batches of size 16 in 10.325 sec\n",
            "Step 83668   [1.746 sec/step, loss=0.46224, avg_loss=0.56502]\n",
            "Step 83669   [1.740 sec/step, loss=0.57286, avg_loss=0.56472]\n",
            "Step 83670   [1.740 sec/step, loss=0.55219, avg_loss=0.56492]\n",
            "Step 83671   [1.743 sec/step, loss=0.56850, avg_loss=0.56507]\n",
            "Step 83672   [1.705 sec/step, loss=0.52098, avg_loss=0.56602]\n",
            "Step 83673   [1.703 sec/step, loss=0.60402, avg_loss=0.56608]\n",
            "Step 83674   [1.718 sec/step, loss=0.52384, avg_loss=0.56565]\n",
            "Step 83675   [1.721 sec/step, loss=0.60221, avg_loss=0.56589]\n",
            "Step 83676   [1.719 sec/step, loss=0.58455, avg_loss=0.56574]\n",
            "Step 83677   [1.721 sec/step, loss=0.57845, avg_loss=0.56565]\n",
            "Step 83678   [1.718 sec/step, loss=0.59541, avg_loss=0.56559]\n",
            "Step 83679   [1.706 sec/step, loss=0.61308, avg_loss=0.56625]\n",
            "Step 83680   [1.713 sec/step, loss=0.56616, avg_loss=0.56661]\n",
            "Step 83681   [1.709 sec/step, loss=0.58477, avg_loss=0.56706]\n",
            "Step 83682   [1.721 sec/step, loss=0.56240, avg_loss=0.56677]\n",
            "Generated 16 batches of size 16 in 10.040 sec\n",
            "Step 83683   [1.758 sec/step, loss=0.45777, avg_loss=0.56529]\n",
            "Step 83684   [1.710 sec/step, loss=0.53828, avg_loss=0.56612]\n",
            "Step 83685   [1.711 sec/step, loss=0.56141, avg_loss=0.56566]\n",
            "Step 83686   [1.710 sec/step, loss=0.62025, avg_loss=0.56570]\n",
            "Step 83687   [1.704 sec/step, loss=0.59524, avg_loss=0.56611]\n",
            "Step 83688   [1.710 sec/step, loss=0.59172, avg_loss=0.56615]\n",
            "Step 83689   [1.721 sec/step, loss=0.52026, avg_loss=0.56525]\n",
            "Step 83690   [1.719 sec/step, loss=0.57588, avg_loss=0.56508]\n",
            "Step 83691   [1.715 sec/step, loss=0.58882, avg_loss=0.56524]\n",
            "Step 83692   [1.679 sec/step, loss=0.58169, avg_loss=0.56724]\n",
            "Step 83693   [1.681 sec/step, loss=0.56700, avg_loss=0.56695]\n",
            "Step 83694   [1.717 sec/step, loss=0.48453, avg_loss=0.56629]\n",
            "Step 83695   [1.722 sec/step, loss=0.50783, avg_loss=0.56603]\n",
            "Step 83696   [1.731 sec/step, loss=0.52883, avg_loss=0.56538]\n",
            "Generated 16 batches of size 16 in 10.633 sec\n",
            "Step 83697   [1.732 sec/step, loss=0.56997, avg_loss=0.56518]\n",
            "Step 83698   [1.720 sec/step, loss=0.57325, avg_loss=0.56517]\n",
            "Step 83699   [1.707 sec/step, loss=0.59428, avg_loss=0.56538]\n",
            "Step 83700   [1.706 sec/step, loss=0.60109, avg_loss=0.56556]\n",
            "Writing summary at step: 83700\n",
            "Step 83701   [1.693 sec/step, loss=0.56022, avg_loss=0.56598]\n",
            "Step 83702   [1.689 sec/step, loss=0.54435, avg_loss=0.56588]\n",
            "Step 83703   [1.695 sec/step, loss=0.59528, avg_loss=0.56602]\n",
            "Step 83704   [1.700 sec/step, loss=0.54954, avg_loss=0.56584]\n",
            "Step 83705   [1.714 sec/step, loss=0.51184, avg_loss=0.56476]\n",
            "Step 83706   [1.709 sec/step, loss=0.56836, avg_loss=0.56498]\n",
            "Step 83707   [1.712 sec/step, loss=0.63611, avg_loss=0.56544]\n",
            "Step 83708   [1.721 sec/step, loss=0.54692, avg_loss=0.56501]\n",
            "Step 83709   [1.726 sec/step, loss=0.56842, avg_loss=0.56505]\n",
            "Step 83710   [1.739 sec/step, loss=0.57721, avg_loss=0.56482]\n",
            "Generated 16 batches of size 16 in 10.536 sec\n",
            "Step 83711   [1.772 sec/step, loss=0.50069, avg_loss=0.56478]\n",
            "Step 83712   [1.773 sec/step, loss=0.60385, avg_loss=0.56501]\n",
            "Step 83713   [1.768 sec/step, loss=0.58473, avg_loss=0.56506]\n",
            "Step 83714   [1.764 sec/step, loss=0.59862, avg_loss=0.56535]\n",
            "Step 83715   [1.751 sec/step, loss=0.58993, avg_loss=0.56595]\n",
            "Step 83716   [1.738 sec/step, loss=0.59651, avg_loss=0.56641]\n",
            "Step 83717   [1.752 sec/step, loss=0.54741, avg_loss=0.56585]\n",
            "Step 83718   [1.756 sec/step, loss=0.62247, avg_loss=0.56626]\n",
            "Step 83719   [1.769 sec/step, loss=0.50335, avg_loss=0.56583]\n",
            "Step 83720   [1.767 sec/step, loss=0.60522, avg_loss=0.56625]\n",
            "Step 83721   [1.764 sec/step, loss=0.59371, avg_loss=0.56658]\n",
            "Step 83722   [1.762 sec/step, loss=0.59084, avg_loss=0.56728]\n",
            "Step 83723   [1.765 sec/step, loss=0.56717, avg_loss=0.56711]\n",
            "Step 83724   [1.773 sec/step, loss=0.51044, avg_loss=0.56612]\n",
            "Step 83725   [1.774 sec/step, loss=0.56910, avg_loss=0.56606]\n",
            "Step 83726   [1.775 sec/step, loss=0.59250, avg_loss=0.56592]\n",
            "Step 83727   [1.770 sec/step, loss=0.58841, avg_loss=0.56615]\n",
            "Step 83728   [1.770 sec/step, loss=0.54918, avg_loss=0.56608]\n",
            "Generated 16 batches of size 16 in 10.132 sec\n",
            "Step 83729   [1.777 sec/step, loss=0.60043, avg_loss=0.56651]\n",
            "Step 83730   [1.774 sec/step, loss=0.57711, avg_loss=0.56612]\n",
            "Step 83731   [1.770 sec/step, loss=0.59584, avg_loss=0.56688]\n",
            "Step 83732   [1.767 sec/step, loss=0.60361, avg_loss=0.56742]\n",
            "Step 83733   [1.728 sec/step, loss=0.58602, avg_loss=0.56901]\n",
            "Step 83734   [1.762 sec/step, loss=0.46151, avg_loss=0.56780]\n",
            "Step 83735   [1.759 sec/step, loss=0.58173, avg_loss=0.56777]\n",
            "Step 83736   [1.766 sec/step, loss=0.52295, avg_loss=0.56723]\n",
            "Step 83737   [1.766 sec/step, loss=0.61888, avg_loss=0.56759]\n",
            "Step 83738   [1.767 sec/step, loss=0.54188, avg_loss=0.56711]\n",
            "Step 83739   [1.769 sec/step, loss=0.57850, avg_loss=0.56686]\n",
            "Step 83740   [1.771 sec/step, loss=0.61924, avg_loss=0.56715]\n",
            "Step 83741   [1.777 sec/step, loss=0.57976, avg_loss=0.56697]\n",
            "Step 83742   [1.789 sec/step, loss=0.52618, avg_loss=0.56670]\n",
            "Step 83743   [1.762 sec/step, loss=0.55486, avg_loss=0.56818]\n",
            "Step 83744   [1.762 sec/step, loss=0.58260, avg_loss=0.56794]\n",
            "Generated 16 batches of size 16 in 10.012 sec\n",
            "Step 83745   [1.750 sec/step, loss=0.52054, avg_loss=0.56756]\n",
            "Step 83746   [1.748 sec/step, loss=0.58388, avg_loss=0.56766]\n",
            "Step 83747   [1.738 sec/step, loss=0.59523, avg_loss=0.56845]\n",
            "Step 83748   [1.736 sec/step, loss=0.56436, avg_loss=0.56821]\n",
            "Step 83749   [1.762 sec/step, loss=0.51490, avg_loss=0.56733]\n",
            "Step 83750   [1.768 sec/step, loss=0.56409, avg_loss=0.56686]\n",
            "Step 83751   [1.770 sec/step, loss=0.60462, avg_loss=0.56735]\n",
            "Step 83752   [1.778 sec/step, loss=0.54703, avg_loss=0.56725]\n",
            "Step 83753   [1.773 sec/step, loss=0.58250, avg_loss=0.56736]\n",
            "Step 83754   [1.755 sec/step, loss=0.56670, avg_loss=0.56787]\n",
            "Step 83755   [1.756 sec/step, loss=0.58922, avg_loss=0.56825]\n",
            "Step 83756   [1.757 sec/step, loss=0.57090, avg_loss=0.56775]\n",
            "Step 83757   [1.763 sec/step, loss=0.53472, avg_loss=0.56744]\n",
            "Step 83758   [1.755 sec/step, loss=0.55712, avg_loss=0.56753]\n",
            "Step 83759   [1.751 sec/step, loss=0.59059, avg_loss=0.56822]\n",
            "Step 83760   [1.749 sec/step, loss=0.56920, avg_loss=0.56835]\n",
            "Generated 16 batches of size 16 in 8.651 sec\n",
            "Step 83761   [1.766 sec/step, loss=0.52448, avg_loss=0.56741]\n",
            "Step 83762   [1.767 sec/step, loss=0.57041, avg_loss=0.56713]\n",
            "Step 83763   [1.763 sec/step, loss=0.55849, avg_loss=0.56681]\n",
            "Step 83764   [1.759 sec/step, loss=0.55042, avg_loss=0.56680]\n",
            "Step 83765   [1.763 sec/step, loss=0.56357, avg_loss=0.56636]\n",
            "Step 83766   [1.759 sec/step, loss=0.54978, avg_loss=0.56629]\n",
            "Step 83767   [1.763 sec/step, loss=0.59670, avg_loss=0.56623]\n",
            "Step 83768   [1.733 sec/step, loss=0.51807, avg_loss=0.56679]\n",
            "Step 83769   [1.738 sec/step, loss=0.58240, avg_loss=0.56688]\n",
            "Step 83770   [1.736 sec/step, loss=0.62663, avg_loss=0.56763]\n",
            "Step 83771   [1.733 sec/step, loss=0.60326, avg_loss=0.56797]\n",
            "Step 83772   [1.732 sec/step, loss=0.51446, avg_loss=0.56791]\n",
            "Step 83773   [1.733 sec/step, loss=0.58922, avg_loss=0.56776]\n",
            "Step 83774   [1.720 sec/step, loss=0.55928, avg_loss=0.56811]\n",
            "Step 83775   [1.715 sec/step, loss=0.58876, avg_loss=0.56798]\n",
            "Step 83776   [1.725 sec/step, loss=0.54265, avg_loss=0.56756]\n",
            "Step 83777   [1.726 sec/step, loss=0.57754, avg_loss=0.56755]\n",
            "Generated 16 batches of size 16 in 8.659 sec\n",
            "Step 83778   [1.738 sec/step, loss=0.52544, avg_loss=0.56685]\n",
            "Step 83779   [1.772 sec/step, loss=0.39664, avg_loss=0.56469]\n",
            "Step 83780   [1.768 sec/step, loss=0.56587, avg_loss=0.56468]\n",
            "Step 83781   [1.768 sec/step, loss=0.59099, avg_loss=0.56475]\n",
            "Step 83782   [1.792 sec/step, loss=0.51125, avg_loss=0.56423]\n",
            "Step 83783   [1.752 sec/step, loss=0.55385, avg_loss=0.56520]\n",
            "Step 83784   [1.761 sec/step, loss=0.55906, avg_loss=0.56540]\n",
            "Step 83785   [1.764 sec/step, loss=0.56199, avg_loss=0.56541]\n",
            "Step 83786   [1.765 sec/step, loss=0.55538, avg_loss=0.56476]\n",
            "Step 83787   [1.761 sec/step, loss=0.59449, avg_loss=0.56475]\n",
            "Step 83788   [1.780 sec/step, loss=0.53700, avg_loss=0.56421]\n",
            "Step 83789   [1.775 sec/step, loss=0.60581, avg_loss=0.56506]\n",
            "Generated 16 batches of size 16 in 7.689 sec\n",
            "Step 83790   [1.791 sec/step, loss=0.51625, avg_loss=0.56447]\n",
            "Step 83791   [1.795 sec/step, loss=0.58285, avg_loss=0.56441]\n",
            "Step 83792   [1.796 sec/step, loss=0.59036, avg_loss=0.56449]\n",
            "Step 83793   [1.795 sec/step, loss=0.58379, avg_loss=0.56466]\n",
            "Step 83794   [1.753 sec/step, loss=0.57564, avg_loss=0.56557]\n",
            "Step 83795   [1.750 sec/step, loss=0.55487, avg_loss=0.56604]\n",
            "Step 83796   [1.738 sec/step, loss=0.58046, avg_loss=0.56656]\n",
            "Step 83797   [1.731 sec/step, loss=0.60591, avg_loss=0.56692]\n",
            "Step 83798   [1.728 sec/step, loss=0.55932, avg_loss=0.56678]\n",
            "Step 83799   [1.734 sec/step, loss=0.58410, avg_loss=0.56668]\n",
            "Step 83800   [1.742 sec/step, loss=0.58145, avg_loss=0.56648]\n",
            "Writing summary at step: 83800\n",
            "Step 83801   [1.742 sec/step, loss=0.53942, avg_loss=0.56627]\n",
            "Step 83802   [1.744 sec/step, loss=0.56864, avg_loss=0.56651]\n",
            "Step 83803   [1.752 sec/step, loss=0.53109, avg_loss=0.56587]\n",
            "Step 83804   [1.752 sec/step, loss=0.60971, avg_loss=0.56647]\n",
            "Step 83805   [1.757 sec/step, loss=0.57490, avg_loss=0.56710]\n",
            "Generated 16 batches of size 16 in 8.424 sec\n",
            "Step 83806   [1.764 sec/step, loss=0.54611, avg_loss=0.56688]\n",
            "Step 83807   [1.763 sec/step, loss=0.60031, avg_loss=0.56652]\n",
            "Step 83808   [1.760 sec/step, loss=0.59678, avg_loss=0.56702]\n",
            "Step 83809   [1.747 sec/step, loss=0.55291, avg_loss=0.56687]\n",
            "Step 83810   [1.769 sec/step, loss=0.44272, avg_loss=0.56552]\n",
            "Step 83811   [1.722 sec/step, loss=0.58644, avg_loss=0.56638]\n",
            "Step 83812   [1.725 sec/step, loss=0.53683, avg_loss=0.56571]\n",
            "Step 83813   [1.732 sec/step, loss=0.52554, avg_loss=0.56512]\n",
            "Step 83814   [1.729 sec/step, loss=0.55704, avg_loss=0.56470]\n",
            "Step 83815   [1.728 sec/step, loss=0.55734, avg_loss=0.56438]\n",
            "Step 83816   [1.732 sec/step, loss=0.58447, avg_loss=0.56426]\n",
            "Step 83817   [1.715 sec/step, loss=0.58305, avg_loss=0.56461]\n",
            "Step 83818   [1.716 sec/step, loss=0.58729, avg_loss=0.56426]\n",
            "Step 83819   [1.687 sec/step, loss=0.55315, avg_loss=0.56476]\n",
            "Step 83820   [1.691 sec/step, loss=0.58609, avg_loss=0.56457]\n",
            "Step 83821   [1.709 sec/step, loss=0.50828, avg_loss=0.56371]\n",
            "Step 83822   [1.711 sec/step, loss=0.59769, avg_loss=0.56378]\n",
            "Generated 16 batches of size 16 in 9.107 sec\n",
            "Step 83823   [1.750 sec/step, loss=0.40409, avg_loss=0.56215]\n",
            "Step 83824   [1.746 sec/step, loss=0.59786, avg_loss=0.56302]\n",
            "Step 83825   [1.747 sec/step, loss=0.59473, avg_loss=0.56328]\n",
            "Step 83826   [1.743 sec/step, loss=0.61237, avg_loss=0.56348]\n",
            "Step 83827   [1.742 sec/step, loss=0.57948, avg_loss=0.56339]\n",
            "Step 83828   [1.743 sec/step, loss=0.54479, avg_loss=0.56335]\n",
            "Step 83829   [1.733 sec/step, loss=0.58837, avg_loss=0.56323]\n",
            "Step 83830   [1.732 sec/step, loss=0.55552, avg_loss=0.56301]\n",
            "Step 83831   [1.730 sec/step, loss=0.58631, avg_loss=0.56291]\n",
            "Step 83832   [1.729 sec/step, loss=0.61917, avg_loss=0.56307]\n",
            "Step 83833   [1.736 sec/step, loss=0.55449, avg_loss=0.56276]\n",
            "Step 83834   [1.704 sec/step, loss=0.55807, avg_loss=0.56372]\n",
            "Step 83835   [1.714 sec/step, loss=0.53805, avg_loss=0.56328]\n",
            "Step 83836   [1.710 sec/step, loss=0.57567, avg_loss=0.56381]\n",
            "Step 83837   [1.714 sec/step, loss=0.60945, avg_loss=0.56372]\n",
            "Step 83838   [1.707 sec/step, loss=0.54976, avg_loss=0.56380]\n",
            "Step 83839   [1.707 sec/step, loss=0.61308, avg_loss=0.56414]\n",
            "Step 83840   [1.712 sec/step, loss=0.54992, avg_loss=0.56345]\n",
            "Generated 16 batches of size 16 in 10.114 sec\n",
            "Step 83841   [1.713 sec/step, loss=0.57477, avg_loss=0.56340]\n",
            "Step 83842   [1.695 sec/step, loss=0.53943, avg_loss=0.56353]\n",
            "Step 83843   [1.722 sec/step, loss=0.46388, avg_loss=0.56262]\n",
            "Step 83844   [1.719 sec/step, loss=0.57159, avg_loss=0.56251]\n",
            "Step 83845   [1.715 sec/step, loss=0.57538, avg_loss=0.56306]\n",
            "Step 83846   [1.720 sec/step, loss=0.58015, avg_loss=0.56302]\n",
            "Step 83847   [1.762 sec/step, loss=0.46546, avg_loss=0.56172]\n",
            "Step 83848   [1.774 sec/step, loss=0.51557, avg_loss=0.56124]\n",
            "Step 83849   [1.749 sec/step, loss=0.58866, avg_loss=0.56197]\n",
            "Step 83850   [1.739 sec/step, loss=0.58585, avg_loss=0.56219]\n",
            "Step 83851   [1.739 sec/step, loss=0.61970, avg_loss=0.56234]\n",
            "Step 83852   [1.735 sec/step, loss=0.57590, avg_loss=0.56263]\n",
            "Step 83853   [1.742 sec/step, loss=0.59877, avg_loss=0.56279]\n",
            "Step 83854   [1.748 sec/step, loss=0.59124, avg_loss=0.56304]\n",
            "Step 83855   [1.745 sec/step, loss=0.55772, avg_loss=0.56272]\n",
            "Step 83856   [1.753 sec/step, loss=0.59126, avg_loss=0.56293]\n",
            "Generated 16 batches of size 16 in 10.222 sec\n",
            "Step 83857   [1.757 sec/step, loss=0.57427, avg_loss=0.56332]\n",
            "Step 83858   [1.755 sec/step, loss=0.59668, avg_loss=0.56372]\n",
            "Step 83859   [1.755 sec/step, loss=0.55115, avg_loss=0.56332]\n",
            "Step 83860   [1.754 sec/step, loss=0.57645, avg_loss=0.56340]\n",
            "Step 83861   [1.737 sec/step, loss=0.58139, avg_loss=0.56397]\n",
            "Step 83862   [1.735 sec/step, loss=0.57515, avg_loss=0.56401]\n",
            "Step 83863   [1.731 sec/step, loss=0.59319, avg_loss=0.56436]\n",
            "Step 83864   [1.732 sec/step, loss=0.56165, avg_loss=0.56447]\n",
            "Step 83865   [1.732 sec/step, loss=0.52602, avg_loss=0.56410]\n",
            "Step 83866   [1.738 sec/step, loss=0.53050, avg_loss=0.56390]\n",
            "Step 83867   [1.733 sec/step, loss=0.60059, avg_loss=0.56394]\n",
            "Step 83868   [1.723 sec/step, loss=0.56736, avg_loss=0.56444]\n",
            "Step 83869   [1.730 sec/step, loss=0.56789, avg_loss=0.56429]\n",
            "Step 83870   [1.739 sec/step, loss=0.60822, avg_loss=0.56411]\n",
            "Step 83871   [1.761 sec/step, loss=0.50830, avg_loss=0.56316]\n",
            "Generated 16 batches of size 16 in 10.311 sec\n",
            "Step 83872   [1.765 sec/step, loss=0.58543, avg_loss=0.56387]\n",
            "Step 83873   [1.799 sec/step, loss=0.50473, avg_loss=0.56302]\n",
            "Step 83874   [1.794 sec/step, loss=0.56712, avg_loss=0.56310]\n",
            "Step 83875   [1.795 sec/step, loss=0.57256, avg_loss=0.56294]\n",
            "Step 83876   [1.790 sec/step, loss=0.58979, avg_loss=0.56341]\n",
            "Step 83877   [1.788 sec/step, loss=0.54833, avg_loss=0.56312]\n",
            "Step 83878   [1.774 sec/step, loss=0.57660, avg_loss=0.56363]\n",
            "Step 83879   [1.782 sec/step, loss=0.41637, avg_loss=0.56383]\n",
            "Step 83880   [1.794 sec/step, loss=0.53367, avg_loss=0.56351]\n",
            "Step 83881   [1.802 sec/step, loss=0.55540, avg_loss=0.56315]\n",
            "Step 83882   [1.763 sec/step, loss=0.60886, avg_loss=0.56413]\n",
            "Step 83883   [1.771 sec/step, loss=0.56751, avg_loss=0.56426]\n",
            "Step 83884   [1.765 sec/step, loss=0.59023, avg_loss=0.56457]\n",
            "Step 83885   [1.768 sec/step, loss=0.54085, avg_loss=0.56436]\n",
            "Step 83886   [1.775 sec/step, loss=0.58276, avg_loss=0.56464]\n",
            "Step 83887   [1.777 sec/step, loss=0.56618, avg_loss=0.56435]\n",
            "Generated 16 batches of size 16 in 10.157 sec\n",
            "Step 83888   [1.758 sec/step, loss=0.58581, avg_loss=0.56484]\n",
            "Step 83889   [1.751 sec/step, loss=0.57003, avg_loss=0.56448]\n",
            "Step 83890   [1.735 sec/step, loss=0.58534, avg_loss=0.56517]\n",
            "Step 83891   [1.731 sec/step, loss=0.60289, avg_loss=0.56537]\n",
            "Step 83892   [1.726 sec/step, loss=0.58995, avg_loss=0.56537]\n",
            "Step 83893   [1.727 sec/step, loss=0.55751, avg_loss=0.56511]\n",
            "Step 83894   [1.730 sec/step, loss=0.61854, avg_loss=0.56554]\n",
            "Step 83895   [1.730 sec/step, loss=0.56559, avg_loss=0.56564]\n",
            "Step 83896   [1.731 sec/step, loss=0.59219, avg_loss=0.56576]\n",
            "Step 83897   [1.730 sec/step, loss=0.60407, avg_loss=0.56574]\n",
            "Step 83898   [1.768 sec/step, loss=0.42071, avg_loss=0.56436]\n",
            "Step 83899   [1.774 sec/step, loss=0.54491, avg_loss=0.56396]\n",
            "Step 83900   [1.767 sec/step, loss=0.59557, avg_loss=0.56411]\n",
            "Writing summary at step: 83900\n",
            "Step 83901   [1.762 sec/step, loss=0.59380, avg_loss=0.56465]\n",
            "Step 83902   [1.765 sec/step, loss=0.58991, avg_loss=0.56486]\n",
            "Step 83903   [1.757 sec/step, loss=0.57379, avg_loss=0.56529]\n",
            "Generated 16 batches of size 16 in 10.629 sec\n",
            "Step 83904   [1.763 sec/step, loss=0.51146, avg_loss=0.56431]\n",
            "Step 83905   [1.752 sec/step, loss=0.55728, avg_loss=0.56413]\n",
            "Step 83906   [1.747 sec/step, loss=0.58321, avg_loss=0.56450]\n",
            "Step 83907   [1.753 sec/step, loss=0.55429, avg_loss=0.56404]\n",
            "Step 83908   [1.751 sec/step, loss=0.57597, avg_loss=0.56383]\n",
            "Step 83909   [1.753 sec/step, loss=0.60375, avg_loss=0.56434]\n",
            "Step 83910   [1.720 sec/step, loss=0.57334, avg_loss=0.56565]\n",
            "Step 83911   [1.761 sec/step, loss=0.48818, avg_loss=0.56467]\n",
            "Step 83912   [1.756 sec/step, loss=0.60045, avg_loss=0.56530]\n",
            "Step 83913   [1.762 sec/step, loss=0.57817, avg_loss=0.56583]\n",
            "Step 83914   [1.764 sec/step, loss=0.56075, avg_loss=0.56587]\n",
            "Step 83915   [1.768 sec/step, loss=0.60975, avg_loss=0.56639]\n",
            "Step 83916   [1.762 sec/step, loss=0.60044, avg_loss=0.56655]\n",
            "Step 83917   [1.775 sec/step, loss=0.51538, avg_loss=0.56587]\n",
            "Step 83918   [1.778 sec/step, loss=0.59349, avg_loss=0.56593]\n",
            "Generated 16 batches of size 16 in 9.528 sec\n",
            "Step 83919   [1.789 sec/step, loss=0.55261, avg_loss=0.56593]\n",
            "Step 83920   [1.789 sec/step, loss=0.53716, avg_loss=0.56544]\n",
            "Step 83921   [1.771 sec/step, loss=0.58813, avg_loss=0.56624]\n",
            "Step 83922   [1.770 sec/step, loss=0.56143, avg_loss=0.56588]\n",
            "Step 83923   [1.736 sec/step, loss=0.52421, avg_loss=0.56708]\n",
            "Step 83924   [1.734 sec/step, loss=0.59484, avg_loss=0.56705]\n",
            "Step 83925   [1.731 sec/step, loss=0.60003, avg_loss=0.56710]\n",
            "Step 83926   [1.731 sec/step, loss=0.63079, avg_loss=0.56728]\n",
            "Step 83927   [1.735 sec/step, loss=0.54279, avg_loss=0.56692]\n",
            "Step 83928   [1.722 sec/step, loss=0.62745, avg_loss=0.56774]\n",
            "Step 83929   [1.731 sec/step, loss=0.55030, avg_loss=0.56736]\n",
            "Step 83930   [1.741 sec/step, loss=0.57784, avg_loss=0.56759]\n",
            "Step 83931   [1.737 sec/step, loss=0.56303, avg_loss=0.56735]\n",
            "Step 83932   [1.739 sec/step, loss=0.60869, avg_loss=0.56725]\n",
            "Step 83933   [1.731 sec/step, loss=0.57907, avg_loss=0.56749]\n",
            "Step 83934   [1.728 sec/step, loss=0.58183, avg_loss=0.56773]\n",
            "Step 83935   [1.722 sec/step, loss=0.59412, avg_loss=0.56829]\n",
            "Step 83936   [1.720 sec/step, loss=0.54698, avg_loss=0.56801]\n",
            "Generated 16 batches of size 16 in 10.438 sec\n",
            "Step 83937   [1.724 sec/step, loss=0.56814, avg_loss=0.56759]\n",
            "Step 83938   [1.764 sec/step, loss=0.32742, avg_loss=0.56537]\n",
            "Step 83939   [1.760 sec/step, loss=0.57964, avg_loss=0.56503]\n",
            "Step 83940   [1.765 sec/step, loss=0.58459, avg_loss=0.56538]\n",
            "Step 83941   [1.797 sec/step, loss=0.49665, avg_loss=0.56460]\n",
            "Step 83942   [1.807 sec/step, loss=0.55206, avg_loss=0.56473]\n",
            "Step 83943   [1.768 sec/step, loss=0.59794, avg_loss=0.56607]\n",
            "Step 83944   [1.766 sec/step, loss=0.58088, avg_loss=0.56616]\n",
            "Step 83945   [1.763 sec/step, loss=0.59775, avg_loss=0.56638]\n",
            "Step 83946   [1.771 sec/step, loss=0.56499, avg_loss=0.56623]\n",
            "Step 83947   [1.740 sec/step, loss=0.60659, avg_loss=0.56764]\n",
            "Step 83948   [1.726 sec/step, loss=0.55537, avg_loss=0.56804]\n",
            "Step 83949   [1.728 sec/step, loss=0.55365, avg_loss=0.56769]\n",
            "Step 83950   [1.732 sec/step, loss=0.59355, avg_loss=0.56777]\n",
            "Generated 16 batches of size 16 in 10.220 sec\n",
            "Step 83951   [1.730 sec/step, loss=0.56839, avg_loss=0.56726]\n",
            "Step 83952   [1.731 sec/step, loss=0.57989, avg_loss=0.56730]\n",
            "Step 83953   [1.729 sec/step, loss=0.59496, avg_loss=0.56726]\n",
            "Step 83954   [1.727 sec/step, loss=0.57683, avg_loss=0.56711]\n",
            "Step 83955   [1.732 sec/step, loss=0.58476, avg_loss=0.56738]\n",
            "Step 83956   [1.722 sec/step, loss=0.60604, avg_loss=0.56753]\n",
            "Step 83957   [1.708 sec/step, loss=0.61073, avg_loss=0.56790]\n",
            "Step 83958   [1.715 sec/step, loss=0.56289, avg_loss=0.56756]\n",
            "Step 83959   [1.720 sec/step, loss=0.58090, avg_loss=0.56786]\n",
            "Step 83960   [1.718 sec/step, loss=0.59283, avg_loss=0.56802]\n",
            "Step 83961   [1.723 sec/step, loss=0.59183, avg_loss=0.56812]\n",
            "Step 83962   [1.730 sec/step, loss=0.60467, avg_loss=0.56842]\n",
            "Step 83963   [1.750 sec/step, loss=0.54255, avg_loss=0.56791]\n",
            "Step 83964   [1.756 sec/step, loss=0.58364, avg_loss=0.56813]\n",
            "Generated 16 batches of size 16 in 6.941 sec\n",
            "Step 83965   [1.754 sec/step, loss=0.56424, avg_loss=0.56851]\n",
            "Step 83966   [1.749 sec/step, loss=0.57182, avg_loss=0.56893]\n",
            "Step 83967   [1.750 sec/step, loss=0.57331, avg_loss=0.56865]\n",
            "Step 83968   [1.792 sec/step, loss=0.40254, avg_loss=0.56701]\n",
            "Step 83969   [1.780 sec/step, loss=0.56737, avg_loss=0.56700]\n",
            "Step 83970   [1.770 sec/step, loss=0.54094, avg_loss=0.56633]\n",
            "Step 83971   [1.749 sec/step, loss=0.58526, avg_loss=0.56710]\n",
            "Step 83972   [1.738 sec/step, loss=0.56346, avg_loss=0.56688]\n",
            "Step 83973   [1.711 sec/step, loss=0.55022, avg_loss=0.56733]\n",
            "Step 83974   [1.725 sec/step, loss=0.53509, avg_loss=0.56701]\n",
            "Step 83975   [1.726 sec/step, loss=0.60730, avg_loss=0.56736]\n",
            "Step 83976   [1.730 sec/step, loss=0.57298, avg_loss=0.56719]\n",
            "Step 83977   [1.734 sec/step, loss=0.58677, avg_loss=0.56758]\n",
            "Step 83978   [1.785 sec/step, loss=0.43459, avg_loss=0.56616]\n",
            "Generated 16 batches of size 16 in 7.304 sec\n",
            "Step 83979   [1.747 sec/step, loss=0.58920, avg_loss=0.56789]\n",
            "Step 83980   [1.740 sec/step, loss=0.56014, avg_loss=0.56815]\n",
            "Step 83981   [1.736 sec/step, loss=0.54930, avg_loss=0.56809]\n",
            "Step 83982   [1.736 sec/step, loss=0.55494, avg_loss=0.56755]\n",
            "Step 83983   [1.724 sec/step, loss=0.59843, avg_loss=0.56786]\n",
            "Step 83984   [1.721 sec/step, loss=0.59778, avg_loss=0.56793]\n",
            "Step 83985   [1.712 sec/step, loss=0.57525, avg_loss=0.56828]\n",
            "Step 83986   [1.703 sec/step, loss=0.59106, avg_loss=0.56836]\n",
            "Step 83987   [1.706 sec/step, loss=0.55560, avg_loss=0.56826]\n",
            "Step 83988   [1.700 sec/step, loss=0.57902, avg_loss=0.56819]\n",
            "Step 83989   [1.702 sec/step, loss=0.58382, avg_loss=0.56833]\n",
            "Step 83990   [1.699 sec/step, loss=0.61249, avg_loss=0.56860]\n",
            "Step 83991   [1.701 sec/step, loss=0.57390, avg_loss=0.56831]\n",
            "Step 83992   [1.719 sec/step, loss=0.54232, avg_loss=0.56783]\n",
            "Step 83993   [1.722 sec/step, loss=0.57426, avg_loss=0.56800]\n",
            "Step 83994   [1.724 sec/step, loss=0.58699, avg_loss=0.56768]\n",
            "Step 83995   [1.730 sec/step, loss=0.56188, avg_loss=0.56765]\n",
            "Step 83996   [1.736 sec/step, loss=0.58031, avg_loss=0.56753]\n",
            "Generated 16 batches of size 16 in 7.575 sec\n",
            "Step 83997   [1.752 sec/step, loss=0.54489, avg_loss=0.56693]\n",
            "Step 83998   [1.727 sec/step, loss=0.55973, avg_loss=0.56833]\n",
            "Step 83999   [1.760 sec/step, loss=0.45383, avg_loss=0.56741]\n",
            "Step 84000   [1.759 sec/step, loss=0.55733, avg_loss=0.56703]\n",
            "Writing summary at step: 84000\n",
            "Saving checkpoint to: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/model.ckpt-84000\n",
            "Step 84001   [1.758 sec/step, loss=0.59365, avg_loss=0.56703]\n",
            "Step 84002   [1.760 sec/step, loss=0.55963, avg_loss=0.56673]\n",
            "Step 84003   [1.754 sec/step, loss=0.59253, avg_loss=0.56692]\n",
            "Step 84004   [1.746 sec/step, loss=0.58591, avg_loss=0.56766]\n",
            "Step 84005   [1.743 sec/step, loss=0.60238, avg_loss=0.56811]\n",
            "Step 84006   [1.745 sec/step, loss=0.57095, avg_loss=0.56799]\n",
            "Step 84007   [1.740 sec/step, loss=0.57708, avg_loss=0.56822]\n",
            "Step 84008   [1.746 sec/step, loss=0.56115, avg_loss=0.56807]\n",
            "Step 84009   [1.754 sec/step, loss=0.60275, avg_loss=0.56806]\n",
            "Step 84010   [1.752 sec/step, loss=0.52738, avg_loss=0.56760]\n",
            "Step 84011   [1.752 sec/step, loss=0.48711, avg_loss=0.56759]\n",
            "Generated 16 batches of size 16 in 8.843 sec\n",
            "Step 84012   [1.759 sec/step, loss=0.58329, avg_loss=0.56742]\n",
            "Step 84013   [1.761 sec/step, loss=0.54123, avg_loss=0.56705]\n",
            "Step 84014   [1.762 sec/step, loss=0.56488, avg_loss=0.56709]\n",
            "Step 84015   [1.758 sec/step, loss=0.61594, avg_loss=0.56715]\n",
            "Step 84016   [1.758 sec/step, loss=0.53727, avg_loss=0.56652]\n",
            "Step 84017   [1.756 sec/step, loss=0.55753, avg_loss=0.56694]\n",
            "Step 84018   [1.756 sec/step, loss=0.57436, avg_loss=0.56675]\n",
            "Step 84019   [1.748 sec/step, loss=0.55371, avg_loss=0.56676]\n",
            "Step 84020   [1.751 sec/step, loss=0.56932, avg_loss=0.56708]\n",
            "Step 84021   [1.760 sec/step, loss=0.57289, avg_loss=0.56693]\n",
            "Step 84022   [1.758 sec/step, loss=0.56841, avg_loss=0.56700]\n",
            "Step 84023   [1.750 sec/step, loss=0.59957, avg_loss=0.56775]\n",
            "Step 84024   [1.749 sec/step, loss=0.58625, avg_loss=0.56767]\n",
            "Step 84025   [1.752 sec/step, loss=0.57497, avg_loss=0.56742]\n",
            "Step 84026   [1.796 sec/step, loss=0.50442, avg_loss=0.56615]\n",
            "Step 84027   [1.792 sec/step, loss=0.56355, avg_loss=0.56636]\n",
            "Step 84028   [1.793 sec/step, loss=0.56428, avg_loss=0.56573]\n",
            "Generated 16 batches of size 16 in 9.529 sec\n",
            "Step 84029   [1.787 sec/step, loss=0.53730, avg_loss=0.56560]\n",
            "Step 84030   [1.781 sec/step, loss=0.58636, avg_loss=0.56568]\n",
            "Step 84031   [1.784 sec/step, loss=0.58743, avg_loss=0.56593]\n",
            "Step 84032   [1.780 sec/step, loss=0.60795, avg_loss=0.56592]\n",
            "Step 84033   [1.780 sec/step, loss=0.58431, avg_loss=0.56597]\n",
            "Step 84034   [1.774 sec/step, loss=0.56650, avg_loss=0.56582]\n",
            "Step 84035   [1.772 sec/step, loss=0.57259, avg_loss=0.56560]\n",
            "Step 84036   [1.775 sec/step, loss=0.56094, avg_loss=0.56574]\n",
            "Step 84037   [1.775 sec/step, loss=0.58749, avg_loss=0.56594]\n",
            "Step 84038   [1.742 sec/step, loss=0.54337, avg_loss=0.56810]\n",
            "Step 84039   [1.785 sec/step, loss=0.45078, avg_loss=0.56681]\n",
            "Step 84040   [1.778 sec/step, loss=0.60710, avg_loss=0.56703]\n",
            "Step 84041   [1.759 sec/step, loss=0.55729, avg_loss=0.56764]\n",
            "Step 84042   [1.747 sec/step, loss=0.58986, avg_loss=0.56802]\n",
            "Step 84043   [1.757 sec/step, loss=0.56646, avg_loss=0.56770]\n",
            "Step 84044   [1.762 sec/step, loss=0.59076, avg_loss=0.56780]\n",
            "Step 84045   [1.770 sec/step, loss=0.57319, avg_loss=0.56756]\n",
            "Generated 16 batches of size 16 in 10.147 sec\n",
            "Step 84046   [1.764 sec/step, loss=0.56963, avg_loss=0.56760]\n",
            "Step 84047   [1.751 sec/step, loss=0.57830, avg_loss=0.56732]\n",
            "Step 84048   [1.749 sec/step, loss=0.61010, avg_loss=0.56787]\n",
            "Step 84049   [1.741 sec/step, loss=0.59732, avg_loss=0.56830]\n",
            "Step 84050   [1.743 sec/step, loss=0.58924, avg_loss=0.56826]\n",
            "Step 84051   [1.739 sec/step, loss=0.57653, avg_loss=0.56834]\n",
            "Step 84052   [1.742 sec/step, loss=0.56805, avg_loss=0.56822]\n",
            "Step 84053   [1.739 sec/step, loss=0.59370, avg_loss=0.56821]\n",
            "Step 84054   [1.741 sec/step, loss=0.59948, avg_loss=0.56844]\n",
            "Step 84055   [1.735 sec/step, loss=0.59286, avg_loss=0.56852]\n",
            "Step 84056   [1.737 sec/step, loss=0.59000, avg_loss=0.56836]\n",
            "Step 84057   [1.743 sec/step, loss=0.56047, avg_loss=0.56785]\n",
            "Step 84058   [1.743 sec/step, loss=0.52292, avg_loss=0.56746]\n",
            "Step 84059   [1.740 sec/step, loss=0.61365, avg_loss=0.56778]\n",
            "Step 84060   [1.784 sec/step, loss=0.50228, avg_loss=0.56688]\n",
            "Generated 16 batches of size 16 in 10.130 sec\n",
            "Step 84061   [1.792 sec/step, loss=0.51751, avg_loss=0.56613]\n",
            "Step 84062   [1.794 sec/step, loss=0.54579, avg_loss=0.56555]\n",
            "Step 84063   [1.784 sec/step, loss=0.58309, avg_loss=0.56595]\n",
            "Step 84064   [1.774 sec/step, loss=0.57652, avg_loss=0.56588]\n",
            "Step 84065   [1.771 sec/step, loss=0.58821, avg_loss=0.56612]\n",
            "Step 84066   [1.770 sec/step, loss=0.59626, avg_loss=0.56636]\n",
            "Step 84067   [1.767 sec/step, loss=0.58783, avg_loss=0.56651]\n",
            "Step 84068   [1.727 sec/step, loss=0.57520, avg_loss=0.56824]\n",
            "Step 84069   [1.729 sec/step, loss=0.60828, avg_loss=0.56864]\n",
            "Step 84070   [1.732 sec/step, loss=0.57723, avg_loss=0.56901]\n",
            "Step 84071   [1.731 sec/step, loss=0.54634, avg_loss=0.56862]\n",
            "Step 84072   [1.742 sec/step, loss=0.54964, avg_loss=0.56848]\n",
            "Step 84073   [1.743 sec/step, loss=0.58145, avg_loss=0.56879]\n",
            "Step 84074   [1.736 sec/step, loss=0.57153, avg_loss=0.56916]\n",
            "Step 84075   [1.739 sec/step, loss=0.61912, avg_loss=0.56927]\n",
            "Generated 16 batches of size 16 in 10.345 sec\n",
            "Step 84076   [1.776 sec/step, loss=0.45958, avg_loss=0.56814]\n",
            "Step 84077   [1.772 sec/step, loss=0.58626, avg_loss=0.56814]\n",
            "Step 84078   [1.722 sec/step, loss=0.56942, avg_loss=0.56948]\n",
            "Step 84079   [1.730 sec/step, loss=0.55577, avg_loss=0.56915]\n",
            "Step 84080   [1.724 sec/step, loss=0.59960, avg_loss=0.56954]\n",
            "Step 84081   [1.727 sec/step, loss=0.60648, avg_loss=0.57012]\n",
            "Step 84082   [1.729 sec/step, loss=0.63006, avg_loss=0.57087]\n",
            "Step 84083   [1.764 sec/step, loss=0.47738, avg_loss=0.56966]\n",
            "Step 84084   [1.763 sec/step, loss=0.58036, avg_loss=0.56948]\n",
            "Step 84085   [1.765 sec/step, loss=0.57340, avg_loss=0.56946]\n",
            "Step 84086   [1.769 sec/step, loss=0.50543, avg_loss=0.56861]\n",
            "Step 84087   [1.765 sec/step, loss=0.57821, avg_loss=0.56883]\n",
            "Step 84088   [1.770 sec/step, loss=0.54570, avg_loss=0.56850]\n",
            "Step 84089   [1.776 sec/step, loss=0.57121, avg_loss=0.56837]\n",
            "Step 84090   [1.789 sec/step, loss=0.50196, avg_loss=0.56727]\n",
            "Step 84091   [1.808 sec/step, loss=0.48181, avg_loss=0.56635]\n",
            "Step 84092   [1.789 sec/step, loss=0.55920, avg_loss=0.56652]\n",
            "Generated 16 batches of size 16 in 10.209 sec\n",
            "Step 84093   [1.799 sec/step, loss=0.55128, avg_loss=0.56629]\n",
            "Step 84094   [1.795 sec/step, loss=0.60306, avg_loss=0.56645]\n",
            "Step 84095   [1.789 sec/step, loss=0.57649, avg_loss=0.56659]\n",
            "Step 84096   [1.783 sec/step, loss=0.62170, avg_loss=0.56701]\n",
            "Step 84097   [1.768 sec/step, loss=0.56531, avg_loss=0.56721]\n",
            "Step 84098   [1.761 sec/step, loss=0.57149, avg_loss=0.56733]\n",
            "Step 84099   [1.724 sec/step, loss=0.54291, avg_loss=0.56822]\n",
            "Step 84100   [1.726 sec/step, loss=0.56572, avg_loss=0.56830]\n",
            "Writing summary at step: 84100\n",
            "Step 84101   [1.730 sec/step, loss=0.57948, avg_loss=0.56816]\n",
            "Step 84102   [1.725 sec/step, loss=0.56410, avg_loss=0.56821]\n",
            "Step 84103   [1.723 sec/step, loss=0.59280, avg_loss=0.56821]\n",
            "Step 84104   [1.727 sec/step, loss=0.57243, avg_loss=0.56808]\n",
            "Step 84105   [1.773 sec/step, loss=0.38683, avg_loss=0.56592]\n",
            "Step 84106   [1.783 sec/step, loss=0.51191, avg_loss=0.56533]\n",
            "Generated 16 batches of size 16 in 10.307 sec\n",
            "Step 84107   [1.781 sec/step, loss=0.61387, avg_loss=0.56570]\n",
            "Step 84108   [1.769 sec/step, loss=0.59406, avg_loss=0.56603]\n",
            "Step 84109   [1.774 sec/step, loss=0.56061, avg_loss=0.56561]\n",
            "Step 84110   [1.777 sec/step, loss=0.55791, avg_loss=0.56591]\n",
            "Step 84111   [1.738 sec/step, loss=0.58443, avg_loss=0.56688]\n",
            "Step 84112   [1.737 sec/step, loss=0.54244, avg_loss=0.56648]\n",
            "Step 84113   [1.725 sec/step, loss=0.58384, avg_loss=0.56690]\n",
            "Step 84114   [1.721 sec/step, loss=0.58393, avg_loss=0.56709]\n",
            "Step 84115   [1.728 sec/step, loss=0.49684, avg_loss=0.56590]\n",
            "Step 84116   [1.736 sec/step, loss=0.56148, avg_loss=0.56614]\n",
            "Step 84117   [1.729 sec/step, loss=0.58731, avg_loss=0.56644]\n",
            "Step 84118   [1.721 sec/step, loss=0.60640, avg_loss=0.56676]\n",
            "Step 84119   [1.706 sec/step, loss=0.61165, avg_loss=0.56734]\n",
            "Step 84120   [1.702 sec/step, loss=0.58687, avg_loss=0.56752]\n",
            "Step 84121   [1.697 sec/step, loss=0.57401, avg_loss=0.56753]\n",
            "Step 84122   [1.704 sec/step, loss=0.59596, avg_loss=0.56780]\n",
            "Step 84123   [1.702 sec/step, loss=0.56996, avg_loss=0.56751]\n",
            "Generated 16 batches of size 16 in 10.022 sec\n",
            "Step 84124   [1.752 sec/step, loss=0.44022, avg_loss=0.56605]\n",
            "Step 84125   [1.753 sec/step, loss=0.57715, avg_loss=0.56607]\n",
            "Step 84126   [1.726 sec/step, loss=0.52830, avg_loss=0.56631]\n",
            "Step 84127   [1.737 sec/step, loss=0.57375, avg_loss=0.56641]\n",
            "Step 84128   [1.735 sec/step, loss=0.55117, avg_loss=0.56628]\n",
            "Step 84129   [1.731 sec/step, loss=0.55407, avg_loss=0.56645]\n",
            "Step 84130   [1.727 sec/step, loss=0.59050, avg_loss=0.56649]\n",
            "Step 84131   [1.761 sec/step, loss=0.40951, avg_loss=0.56471]\n",
            "Step 84132   [1.776 sec/step, loss=0.55220, avg_loss=0.56415]\n",
            "Step 84133   [1.776 sec/step, loss=0.57278, avg_loss=0.56403]\n",
            "Step 84134   [1.782 sec/step, loss=0.53826, avg_loss=0.56375]\n",
            "Step 84135   [1.782 sec/step, loss=0.62053, avg_loss=0.56423]\n",
            "Step 84136   [1.793 sec/step, loss=0.57633, avg_loss=0.56439]\n",
            "Step 84137   [1.788 sec/step, loss=0.60229, avg_loss=0.56453]\n",
            "Step 84138   [1.785 sec/step, loss=0.58516, avg_loss=0.56495]\n",
            "Step 84139   [1.751 sec/step, loss=0.50510, avg_loss=0.56549]\n",
            "Step 84140   [1.752 sec/step, loss=0.57074, avg_loss=0.56513]\n",
            "Step 84141   [1.733 sec/step, loss=0.60193, avg_loss=0.56558]\n",
            "Generated 16 batches of size 16 in 10.009 sec\n",
            "Step 84142   [1.733 sec/step, loss=0.62140, avg_loss=0.56589]\n",
            "Step 84143   [1.727 sec/step, loss=0.52903, avg_loss=0.56552]\n",
            "Step 84144   [1.727 sec/step, loss=0.57323, avg_loss=0.56534]\n",
            "Step 84145   [1.720 sec/step, loss=0.57334, avg_loss=0.56534]\n",
            "Step 84146   [1.719 sec/step, loss=0.56782, avg_loss=0.56533]\n",
            "Step 84147   [1.722 sec/step, loss=0.59250, avg_loss=0.56547]\n",
            "Step 84148   [1.723 sec/step, loss=0.60717, avg_loss=0.56544]\n",
            "Step 84149   [1.735 sec/step, loss=0.56436, avg_loss=0.56511]\n",
            "Step 84150   [1.733 sec/step, loss=0.59887, avg_loss=0.56521]\n",
            "Step 84151   [1.737 sec/step, loss=0.55913, avg_loss=0.56503]\n",
            "Step 84152   [1.737 sec/step, loss=0.54923, avg_loss=0.56484]\n",
            "Step 84153   [1.737 sec/step, loss=0.58947, avg_loss=0.56480]\n",
            "Step 84154   [1.734 sec/step, loss=0.58342, avg_loss=0.56464]\n",
            "Generated 16 batches of size 16 in 9.032 sec\n",
            "Step 84155   [1.785 sec/step, loss=0.36008, avg_loss=0.56231]\n",
            "Step 84156   [1.782 sec/step, loss=0.55975, avg_loss=0.56201]\n",
            "Step 84157   [1.786 sec/step, loss=0.56416, avg_loss=0.56205]\n",
            "Step 84158   [1.782 sec/step, loss=0.59796, avg_loss=0.56280]\n",
            "Step 84159   [1.781 sec/step, loss=0.60862, avg_loss=0.56275]\n",
            "Step 84160   [1.743 sec/step, loss=0.56646, avg_loss=0.56339]\n",
            "Step 84161   [1.740 sec/step, loss=0.52484, avg_loss=0.56346]\n",
            "Step 84162   [1.739 sec/step, loss=0.57407, avg_loss=0.56375]\n",
            "Step 84163   [1.739 sec/step, loss=0.53488, avg_loss=0.56326]\n",
            "Step 84164   [1.741 sec/step, loss=0.58620, avg_loss=0.56336]\n",
            "Step 84165   [1.742 sec/step, loss=0.59245, avg_loss=0.56340]\n",
            "Step 84166   [1.740 sec/step, loss=0.60673, avg_loss=0.56351]\n",
            "Step 84167   [1.746 sec/step, loss=0.57015, avg_loss=0.56333]\n",
            "Step 84168   [1.799 sec/step, loss=0.46269, avg_loss=0.56220]\n",
            "Generated 16 batches of size 16 in 7.230 sec\n",
            "Step 84169   [1.803 sec/step, loss=0.58726, avg_loss=0.56199]\n",
            "Step 84170   [1.796 sec/step, loss=0.58741, avg_loss=0.56210]\n",
            "Step 84171   [1.801 sec/step, loss=0.54723, avg_loss=0.56211]\n",
            "Step 84172   [1.793 sec/step, loss=0.60097, avg_loss=0.56262]\n",
            "Step 84173   [1.786 sec/step, loss=0.61681, avg_loss=0.56297]\n",
            "Step 84174   [1.778 sec/step, loss=0.58335, avg_loss=0.56309]\n",
            "Step 84175   [1.773 sec/step, loss=0.60643, avg_loss=0.56296]\n",
            "Step 84176   [1.736 sec/step, loss=0.58056, avg_loss=0.56417]\n",
            "Step 84177   [1.739 sec/step, loss=0.59397, avg_loss=0.56425]\n",
            "Step 84178   [1.743 sec/step, loss=0.55480, avg_loss=0.56410]\n",
            "Step 84179   [1.730 sec/step, loss=0.59267, avg_loss=0.56447]\n",
            "Step 84180   [1.730 sec/step, loss=0.58292, avg_loss=0.56431]\n",
            "Step 84181   [1.723 sec/step, loss=0.58993, avg_loss=0.56414]\n",
            "Step 84182   [1.720 sec/step, loss=0.59987, avg_loss=0.56384]\n",
            "Step 84183   [1.682 sec/step, loss=0.57485, avg_loss=0.56481]\n",
            "Step 84184   [1.732 sec/step, loss=0.41428, avg_loss=0.56315]\n",
            "Step 84185   [1.735 sec/step, loss=0.57949, avg_loss=0.56321]\n",
            "Generated 16 batches of size 16 in 7.822 sec\n",
            "Step 84186   [1.737 sec/step, loss=0.54200, avg_loss=0.56358]\n",
            "Step 84187   [1.751 sec/step, loss=0.55575, avg_loss=0.56335]\n",
            "Step 84188   [1.749 sec/step, loss=0.56414, avg_loss=0.56354]\n",
            "Step 84189   [1.740 sec/step, loss=0.55946, avg_loss=0.56342]\n",
            "Step 84190   [1.738 sec/step, loss=0.59567, avg_loss=0.56436]\n",
            "Step 84191   [1.720 sec/step, loss=0.58734, avg_loss=0.56541]\n",
            "Step 84192   [1.726 sec/step, loss=0.56758, avg_loss=0.56550]\n",
            "Step 84193   [1.716 sec/step, loss=0.50195, avg_loss=0.56500]\n",
            "Step 84194   [1.720 sec/step, loss=0.60949, avg_loss=0.56507]\n",
            "Step 84195   [1.718 sec/step, loss=0.61421, avg_loss=0.56545]\n",
            "Step 84196   [1.718 sec/step, loss=0.59598, avg_loss=0.56519]\n",
            "Step 84197   [1.716 sec/step, loss=0.54269, avg_loss=0.56496]\n",
            "Step 84198   [1.730 sec/step, loss=0.52919, avg_loss=0.56454]\n",
            "Step 84199   [1.723 sec/step, loss=0.59154, avg_loss=0.56503]\n",
            "Step 84200   [1.725 sec/step, loss=0.58711, avg_loss=0.56524]\n",
            "Writing summary at step: 84200\n",
            "Step 84201   [1.724 sec/step, loss=0.56273, avg_loss=0.56507]\n",
            "Step 84202   [1.731 sec/step, loss=0.58268, avg_loss=0.56526]\n",
            "Step 84203   [1.744 sec/step, loss=0.54815, avg_loss=0.56481]\n",
            "Generated 16 batches of size 16 in 8.510 sec\n",
            "Step 84204   [1.742 sec/step, loss=0.56197, avg_loss=0.56471]\n",
            "Step 84205   [1.699 sec/step, loss=0.57700, avg_loss=0.56661]\n",
            "Step 84206   [1.721 sec/step, loss=0.50091, avg_loss=0.56650]\n",
            "Step 84207   [1.720 sec/step, loss=0.55374, avg_loss=0.56590]\n",
            "Step 84208   [1.721 sec/step, loss=0.58645, avg_loss=0.56582]\n",
            "Step 84209   [1.709 sec/step, loss=0.60335, avg_loss=0.56625]\n",
            "Step 84210   [1.702 sec/step, loss=0.56102, avg_loss=0.56628]\n",
            "Step 84211   [1.709 sec/step, loss=0.55737, avg_loss=0.56601]\n",
            "Step 84212   [1.710 sec/step, loss=0.58891, avg_loss=0.56647]\n",
            "Step 84213   [1.714 sec/step, loss=0.57562, avg_loss=0.56639]\n",
            "Step 84214   [1.756 sec/step, loss=0.46753, avg_loss=0.56523]\n",
            "Step 84215   [1.767 sec/step, loss=0.53564, avg_loss=0.56562]\n",
            "Step 84216   [1.766 sec/step, loss=0.59106, avg_loss=0.56591]\n",
            "Step 84217   [1.765 sec/step, loss=0.59842, avg_loss=0.56602]\n",
            "Step 84218   [1.768 sec/step, loss=0.57096, avg_loss=0.56567]\n",
            "Step 84219   [1.774 sec/step, loss=0.55239, avg_loss=0.56508]\n",
            "Generated 16 batches of size 16 in 9.008 sec\n",
            "Step 84220   [1.777 sec/step, loss=0.58856, avg_loss=0.56509]\n",
            "Step 84221   [1.776 sec/step, loss=0.59283, avg_loss=0.56528]\n",
            "Step 84222   [1.782 sec/step, loss=0.52004, avg_loss=0.56452]\n",
            "Step 84223   [1.782 sec/step, loss=0.57839, avg_loss=0.56461]\n",
            "Step 84224   [1.736 sec/step, loss=0.54002, avg_loss=0.56560]\n",
            "Step 84225   [1.732 sec/step, loss=0.56135, avg_loss=0.56545]\n",
            "Step 84226   [1.716 sec/step, loss=0.55269, avg_loss=0.56569]\n",
            "Step 84227   [1.705 sec/step, loss=0.59009, avg_loss=0.56585]\n",
            "Step 84228   [1.721 sec/step, loss=0.51593, avg_loss=0.56550]\n",
            "Step 84229   [1.725 sec/step, loss=0.58101, avg_loss=0.56577]\n",
            "Step 84230   [1.724 sec/step, loss=0.60275, avg_loss=0.56589]\n",
            "Step 84231   [1.688 sec/step, loss=0.59585, avg_loss=0.56776]\n",
            "Step 84232   [1.675 sec/step, loss=0.60128, avg_loss=0.56825]\n",
            "Step 84233   [1.683 sec/step, loss=0.59116, avg_loss=0.56843]\n",
            "Step 84234   [1.686 sec/step, loss=0.60346, avg_loss=0.56908]\n",
            "Step 84235   [1.696 sec/step, loss=0.57419, avg_loss=0.56862]\n",
            "Generated 16 batches of size 16 in 9.361 sec\n",
            "Step 84236   [1.687 sec/step, loss=0.62104, avg_loss=0.56907]\n",
            "Step 84237   [1.688 sec/step, loss=0.60226, avg_loss=0.56907]\n",
            "Step 84238   [1.725 sec/step, loss=0.45841, avg_loss=0.56780]\n",
            "Step 84239   [1.727 sec/step, loss=0.54196, avg_loss=0.56817]\n",
            "Step 84240   [1.726 sec/step, loss=0.56157, avg_loss=0.56808]\n",
            "Step 84241   [1.730 sec/step, loss=0.55250, avg_loss=0.56758]\n",
            "Step 84242   [1.736 sec/step, loss=0.57639, avg_loss=0.56713]\n",
            "Step 84243   [1.772 sec/step, loss=0.44074, avg_loss=0.56625]\n",
            "Step 84244   [1.770 sec/step, loss=0.58183, avg_loss=0.56633]\n",
            "Step 84245   [1.773 sec/step, loss=0.58387, avg_loss=0.56644]\n",
            "Step 84246   [1.771 sec/step, loss=0.62832, avg_loss=0.56704]\n",
            "Step 84247   [1.785 sec/step, loss=0.53506, avg_loss=0.56647]\n",
            "Step 84248   [1.789 sec/step, loss=0.59802, avg_loss=0.56638]\n",
            "Step 84249   [1.787 sec/step, loss=0.56958, avg_loss=0.56643]\n",
            "Step 84250   [1.787 sec/step, loss=0.59949, avg_loss=0.56644]\n",
            "Step 84251   [1.784 sec/step, loss=0.59334, avg_loss=0.56678]\n",
            "Generated 16 batches of size 16 in 9.339 sec\n",
            "Step 84252   [1.778 sec/step, loss=0.57231, avg_loss=0.56701]\n",
            "Step 84253   [1.775 sec/step, loss=0.58600, avg_loss=0.56698]\n",
            "Step 84254   [1.790 sec/step, loss=0.54190, avg_loss=0.56656]\n",
            "Step 84255   [1.739 sec/step, loss=0.62002, avg_loss=0.56916]\n",
            "Step 84256   [1.745 sec/step, loss=0.54180, avg_loss=0.56898]\n",
            "Step 84257   [1.736 sec/step, loss=0.59839, avg_loss=0.56932]\n",
            "Step 84258   [1.737 sec/step, loss=0.58405, avg_loss=0.56918]\n",
            "Step 84259   [1.739 sec/step, loss=0.56468, avg_loss=0.56874]\n",
            "Step 84260   [1.735 sec/step, loss=0.53686, avg_loss=0.56845]\n",
            "Step 84261   [1.722 sec/step, loss=0.57179, avg_loss=0.56892]\n",
            "Step 84262   [1.716 sec/step, loss=0.58943, avg_loss=0.56907]\n",
            "Step 84263   [1.707 sec/step, loss=0.57440, avg_loss=0.56947]\n",
            "Step 84264   [1.720 sec/step, loss=0.55531, avg_loss=0.56916]\n",
            "Step 84265   [1.729 sec/step, loss=0.52149, avg_loss=0.56845]\n",
            "Step 84266   [1.746 sec/step, loss=0.52841, avg_loss=0.56766]\n",
            "Step 84267   [1.745 sec/step, loss=0.57500, avg_loss=0.56771]\n",
            "Generated 16 batches of size 16 in 9.857 sec\n",
            "Step 84268   [1.696 sec/step, loss=0.59219, avg_loss=0.56901]\n",
            "Step 84269   [1.721 sec/step, loss=0.44947, avg_loss=0.56763]\n",
            "Step 84270   [1.720 sec/step, loss=0.60945, avg_loss=0.56785]\n",
            "Step 84271   [1.716 sec/step, loss=0.59043, avg_loss=0.56828]\n",
            "Step 84272   [1.721 sec/step, loss=0.56556, avg_loss=0.56793]\n",
            "Step 84273   [1.723 sec/step, loss=0.54914, avg_loss=0.56725]\n",
            "Step 84274   [1.729 sec/step, loss=0.58491, avg_loss=0.56727]\n",
            "Step 84275   [1.732 sec/step, loss=0.56472, avg_loss=0.56685]\n",
            "Step 84276   [1.733 sec/step, loss=0.54221, avg_loss=0.56647]\n",
            "Step 84277   [1.730 sec/step, loss=0.60986, avg_loss=0.56663]\n",
            "Step 84278   [1.726 sec/step, loss=0.56207, avg_loss=0.56670]\n",
            "Step 84279   [1.733 sec/step, loss=0.55132, avg_loss=0.56629]\n",
            "Step 84280   [1.735 sec/step, loss=0.58860, avg_loss=0.56634]\n",
            "Step 84281   [1.737 sec/step, loss=0.60867, avg_loss=0.56653]\n",
            "Step 84282   [1.741 sec/step, loss=0.56521, avg_loss=0.56618]\n",
            "Step 84283   [1.744 sec/step, loss=0.59488, avg_loss=0.56638]\n",
            "Generated 16 batches of size 16 in 9.908 sec\n",
            "Step 84284   [1.740 sec/step, loss=0.44044, avg_loss=0.56664]\n",
            "Step 84285   [1.733 sec/step, loss=0.58542, avg_loss=0.56670]\n",
            "Step 84286   [1.744 sec/step, loss=0.53662, avg_loss=0.56665]\n",
            "Step 84287   [1.732 sec/step, loss=0.60028, avg_loss=0.56710]\n",
            "Step 84288   [1.735 sec/step, loss=0.55043, avg_loss=0.56696]\n",
            "Step 84289   [1.741 sec/step, loss=0.57402, avg_loss=0.56710]\n",
            "Step 84290   [1.734 sec/step, loss=0.57898, avg_loss=0.56694]\n",
            "Step 84291   [1.734 sec/step, loss=0.59821, avg_loss=0.56705]\n",
            "Step 84292   [1.732 sec/step, loss=0.52468, avg_loss=0.56662]\n",
            "Step 84293   [1.737 sec/step, loss=0.53447, avg_loss=0.56694]\n",
            "Step 84294   [1.762 sec/step, loss=0.49309, avg_loss=0.56578]\n",
            "Step 84295   [1.763 sec/step, loss=0.54041, avg_loss=0.56504]\n",
            "Step 84296   [1.769 sec/step, loss=0.59395, avg_loss=0.56502]\n",
            "Step 84297   [1.775 sec/step, loss=0.60815, avg_loss=0.56567]\n",
            "Step 84298   [1.758 sec/step, loss=0.57658, avg_loss=0.56615]\n",
            "Step 84299   [1.761 sec/step, loss=0.57541, avg_loss=0.56599]\n",
            "Generated 16 batches of size 16 in 9.633 sec\n",
            "Step 84300   [1.778 sec/step, loss=0.53863, avg_loss=0.56550]\n",
            "Writing summary at step: 84300\n",
            "Step 84301   [1.778 sec/step, loss=0.57244, avg_loss=0.56560]\n",
            "Step 84302   [1.771 sec/step, loss=0.61200, avg_loss=0.56589]\n",
            "Step 84303   [1.757 sec/step, loss=0.59693, avg_loss=0.56638]\n",
            "Step 84304   [1.753 sec/step, loss=0.62412, avg_loss=0.56700]\n",
            "Step 84305   [1.745 sec/step, loss=0.61758, avg_loss=0.56741]\n",
            "Step 84306   [1.709 sec/step, loss=0.55509, avg_loss=0.56795]\n",
            "Step 84307   [1.717 sec/step, loss=0.55697, avg_loss=0.56798]\n",
            "Step 84308   [1.717 sec/step, loss=0.59076, avg_loss=0.56802]\n",
            "Step 84309   [1.718 sec/step, loss=0.56697, avg_loss=0.56766]\n",
            "Step 84310   [1.721 sec/step, loss=0.59553, avg_loss=0.56801]\n",
            "Step 84311   [1.736 sec/step, loss=0.52564, avg_loss=0.56769]\n",
            "Step 84312   [1.736 sec/step, loss=0.57133, avg_loss=0.56751]\n",
            "Step 84313   [1.742 sec/step, loss=0.51921, avg_loss=0.56695]\n",
            "Generated 16 batches of size 16 in 9.627 sec\n",
            "Step 84314   [1.709 sec/step, loss=0.58071, avg_loss=0.56808]\n",
            "Step 84315   [1.695 sec/step, loss=0.60138, avg_loss=0.56874]\n",
            "Step 84316   [1.726 sec/step, loss=0.46111, avg_loss=0.56744]\n",
            "Step 84317   [1.724 sec/step, loss=0.57034, avg_loss=0.56716]\n",
            "Step 84318   [1.728 sec/step, loss=0.51893, avg_loss=0.56664]\n",
            "Step 84319   [1.725 sec/step, loss=0.58187, avg_loss=0.56693]\n",
            "Step 84320   [1.726 sec/step, loss=0.55568, avg_loss=0.56660]\n",
            "Step 84321   [1.725 sec/step, loss=0.58267, avg_loss=0.56650]\n",
            "Step 84322   [1.712 sec/step, loss=0.57096, avg_loss=0.56701]\n",
            "Step 84323   [1.722 sec/step, loss=0.53709, avg_loss=0.56660]\n",
            "Step 84324   [1.760 sec/step, loss=0.37079, avg_loss=0.56491]\n",
            "Step 84325   [1.762 sec/step, loss=0.61549, avg_loss=0.56545]\n",
            "Step 84326   [1.763 sec/step, loss=0.58281, avg_loss=0.56575]\n",
            "Step 84327   [1.764 sec/step, loss=0.58396, avg_loss=0.56569]\n",
            "Step 84328   [1.759 sec/step, loss=0.55387, avg_loss=0.56607]\n",
            "Step 84329   [1.755 sec/step, loss=0.59740, avg_loss=0.56623]\n",
            "Step 84330   [1.760 sec/step, loss=0.56570, avg_loss=0.56586]\n",
            "Step 84331   [1.761 sec/step, loss=0.57094, avg_loss=0.56561]\n",
            "Step 84332   [1.762 sec/step, loss=0.54783, avg_loss=0.56508]\n",
            "Generated 16 batches of size 16 in 10.229 sec\n",
            "Step 84333   [1.755 sec/step, loss=0.57724, avg_loss=0.56494]\n",
            "Step 84334   [1.752 sec/step, loss=0.51678, avg_loss=0.56407]\n",
            "Step 84335   [1.741 sec/step, loss=0.58711, avg_loss=0.56420]\n",
            "Step 84336   [1.743 sec/step, loss=0.54523, avg_loss=0.56344]\n",
            "Step 84337   [1.741 sec/step, loss=0.63106, avg_loss=0.56373]\n",
            "Step 84338   [1.740 sec/step, loss=0.50628, avg_loss=0.56421]\n",
            "Step 84339   [1.729 sec/step, loss=0.61105, avg_loss=0.56490]\n",
            "Step 84340   [1.726 sec/step, loss=0.59746, avg_loss=0.56526]\n",
            "Step 84341   [1.731 sec/step, loss=0.53363, avg_loss=0.56507]\n",
            "Step 84342   [1.728 sec/step, loss=0.60374, avg_loss=0.56534]\n",
            "Step 84343   [1.687 sec/step, loss=0.59315, avg_loss=0.56687]\n",
            "Step 84344   [1.687 sec/step, loss=0.57844, avg_loss=0.56683]\n",
            "Step 84345   [1.694 sec/step, loss=0.57746, avg_loss=0.56677]\n",
            "Step 84346   [1.696 sec/step, loss=0.58741, avg_loss=0.56636]\n",
            "Step 84347   [1.688 sec/step, loss=0.55858, avg_loss=0.56659]\n",
            "Generated 16 batches of size 16 in 10.510 sec\n",
            "Step 84348   [1.692 sec/step, loss=0.55575, avg_loss=0.56617]\n",
            "Step 84349   [1.684 sec/step, loss=0.58631, avg_loss=0.56634]\n",
            "Step 84350   [1.696 sec/step, loss=0.57411, avg_loss=0.56609]\n",
            "Step 84351   [1.695 sec/step, loss=0.56157, avg_loss=0.56577]\n",
            "Step 84352   [1.696 sec/step, loss=0.57721, avg_loss=0.56582]\n",
            "Step 84353   [1.704 sec/step, loss=0.56481, avg_loss=0.56560]\n",
            "Step 84354   [1.692 sec/step, loss=0.56499, avg_loss=0.56584]\n",
            "Step 84355   [1.707 sec/step, loss=0.54958, avg_loss=0.56513]\n",
            "Step 84356   [1.704 sec/step, loss=0.58587, avg_loss=0.56557]\n",
            "Step 84357   [1.706 sec/step, loss=0.53177, avg_loss=0.56491]\n",
            "Step 84358   [1.712 sec/step, loss=0.55166, avg_loss=0.56458]\n",
            "Step 84359   [1.709 sec/step, loss=0.53693, avg_loss=0.56430]\n",
            "Step 84360   [1.724 sec/step, loss=0.55751, avg_loss=0.56451]\n",
            "Generated 16 batches of size 16 in 7.112 sec\n",
            "Step 84361   [1.728 sec/step, loss=0.58888, avg_loss=0.56468]\n",
            "Step 84362   [1.726 sec/step, loss=0.59454, avg_loss=0.56473]\n",
            "Step 84363   [1.768 sec/step, loss=0.46409, avg_loss=0.56363]\n",
            "Step 84364   [1.757 sec/step, loss=0.60205, avg_loss=0.56410]\n",
            "Step 84365   [1.746 sec/step, loss=0.58118, avg_loss=0.56469]\n",
            "Step 84366   [1.738 sec/step, loss=0.57536, avg_loss=0.56516]\n",
            "Step 84367   [1.742 sec/step, loss=0.56508, avg_loss=0.56506]\n",
            "Step 84368   [1.736 sec/step, loss=0.58018, avg_loss=0.56494]\n",
            "Step 84369   [1.710 sec/step, loss=0.57106, avg_loss=0.56616]\n",
            "Step 84370   [1.715 sec/step, loss=0.55102, avg_loss=0.56558]\n",
            "Step 84371   [1.719 sec/step, loss=0.57642, avg_loss=0.56544]\n",
            "Step 84372   [1.716 sec/step, loss=0.59623, avg_loss=0.56574]\n",
            "Step 84373   [1.712 sec/step, loss=0.59513, avg_loss=0.56620]\n",
            "Step 84374   [1.720 sec/step, loss=0.53368, avg_loss=0.56569]\n",
            "Step 84375   [1.719 sec/step, loss=0.60434, avg_loss=0.56609]\n",
            "Step 84376   [1.714 sec/step, loss=0.57812, avg_loss=0.56645]\n",
            "Step 84377   [1.723 sec/step, loss=0.59981, avg_loss=0.56634]\n",
            "Generated 16 batches of size 16 in 7.030 sec\n",
            "Step 84378   [1.722 sec/step, loss=0.59869, avg_loss=0.56671]\n",
            "Step 84379   [1.729 sec/step, loss=0.51100, avg_loss=0.56631]\n",
            "Step 84380   [1.729 sec/step, loss=0.56738, avg_loss=0.56610]\n",
            "Step 84381   [1.756 sec/step, loss=0.51205, avg_loss=0.56513]\n",
            "Step 84382   [1.753 sec/step, loss=0.58046, avg_loss=0.56528]\n",
            "Step 84383   [1.749 sec/step, loss=0.54547, avg_loss=0.56479]\n",
            "Step 84384   [1.702 sec/step, loss=0.51770, avg_loss=0.56556]\n",
            "Step 84385   [1.709 sec/step, loss=0.56680, avg_loss=0.56537]\n",
            "Step 84386   [1.694 sec/step, loss=0.60442, avg_loss=0.56605]\n",
            "Step 84387   [1.693 sec/step, loss=0.58753, avg_loss=0.56592]\n",
            "Step 84388   [1.690 sec/step, loss=0.57664, avg_loss=0.56619]\n",
            "Step 84389   [1.683 sec/step, loss=0.58619, avg_loss=0.56631]\n",
            "Step 84390   [1.687 sec/step, loss=0.54372, avg_loss=0.56596]\n",
            "Step 84391   [1.690 sec/step, loss=0.58575, avg_loss=0.56583]\n",
            "Step 84392   [1.709 sec/step, loss=0.54524, avg_loss=0.56604]\n",
            "Generated 16 batches of size 16 in 7.811 sec\n",
            "Step 84393   [1.702 sec/step, loss=0.58575, avg_loss=0.56655]\n",
            "Step 84394   [1.672 sec/step, loss=0.54286, avg_loss=0.56705]\n",
            "Step 84395   [1.710 sec/step, loss=0.46913, avg_loss=0.56633]\n",
            "Step 84396   [1.710 sec/step, loss=0.54566, avg_loss=0.56585]\n",
            "Step 84397   [1.715 sec/step, loss=0.58429, avg_loss=0.56561]\n",
            "Step 84398   [1.718 sec/step, loss=0.56739, avg_loss=0.56552]\n",
            "Step 84399   [1.720 sec/step, loss=0.56656, avg_loss=0.56543]\n",
            "Step 84400   [1.703 sec/step, loss=0.56464, avg_loss=0.56569]\n",
            "Writing summary at step: 84400\n",
            "Step 84401   [1.702 sec/step, loss=0.60415, avg_loss=0.56601]\n",
            "Step 84402   [1.709 sec/step, loss=0.59962, avg_loss=0.56589]\n",
            "Step 84403   [1.719 sec/step, loss=0.54959, avg_loss=0.56541]\n",
            "Step 84404   [1.722 sec/step, loss=0.56509, avg_loss=0.56482]\n",
            "Step 84405   [1.725 sec/step, loss=0.57806, avg_loss=0.56443]\n",
            "Step 84406   [1.728 sec/step, loss=0.56993, avg_loss=0.56458]\n",
            "Step 84407   [1.740 sec/step, loss=0.51308, avg_loss=0.56414]\n",
            "Step 84408   [1.747 sec/step, loss=0.52535, avg_loss=0.56348]\n",
            "Generated 16 batches of size 16 in 7.918 sec\n",
            "Step 84409   [1.788 sec/step, loss=0.36464, avg_loss=0.56146]\n",
            "Step 84410   [1.783 sec/step, loss=0.57263, avg_loss=0.56123]\n",
            "Step 84411   [1.761 sec/step, loss=0.57574, avg_loss=0.56173]\n",
            "Step 84412   [1.761 sec/step, loss=0.52472, avg_loss=0.56127]\n",
            "Step 84413   [1.750 sec/step, loss=0.57210, avg_loss=0.56179]\n",
            "Step 84414   [1.748 sec/step, loss=0.59405, avg_loss=0.56193]\n",
            "Step 84415   [1.744 sec/step, loss=0.59695, avg_loss=0.56188]\n",
            "Step 84416   [1.707 sec/step, loss=0.56918, avg_loss=0.56296]\n",
            "Step 84417   [1.714 sec/step, loss=0.53902, avg_loss=0.56265]\n",
            "Step 84418   [1.711 sec/step, loss=0.61477, avg_loss=0.56361]\n",
            "Step 84419   [1.725 sec/step, loss=0.48578, avg_loss=0.56265]\n",
            "Step 84420   [1.719 sec/step, loss=0.58913, avg_loss=0.56298]\n",
            "Step 84421   [1.722 sec/step, loss=0.58713, avg_loss=0.56303]\n",
            "Step 84422   [1.727 sec/step, loss=0.57605, avg_loss=0.56308]\n",
            "Step 84423   [1.762 sec/step, loss=0.43343, avg_loss=0.56204]\n",
            "Generated 16 batches of size 16 in 9.318 sec\n",
            "Step 84424   [1.719 sec/step, loss=0.60559, avg_loss=0.56439]\n",
            "Step 84425   [1.727 sec/step, loss=0.57297, avg_loss=0.56396]\n",
            "Step 84426   [1.728 sec/step, loss=0.55674, avg_loss=0.56370]\n",
            "Step 84427   [1.728 sec/step, loss=0.53140, avg_loss=0.56318]\n",
            "Step 84428   [1.716 sec/step, loss=0.59793, avg_loss=0.56362]\n",
            "Step 84429   [1.719 sec/step, loss=0.57807, avg_loss=0.56343]\n",
            "Step 84430   [1.715 sec/step, loss=0.58980, avg_loss=0.56367]\n",
            "Step 84431   [1.710 sec/step, loss=0.56333, avg_loss=0.56359]\n",
            "Step 84432   [1.712 sec/step, loss=0.53619, avg_loss=0.56347]\n",
            "Step 84433   [1.720 sec/step, loss=0.56550, avg_loss=0.56336]\n",
            "Step 84434   [1.715 sec/step, loss=0.58060, avg_loss=0.56400]\n",
            "Step 84435   [1.758 sec/step, loss=0.49848, avg_loss=0.56311]\n",
            "Step 84436   [1.755 sec/step, loss=0.59901, avg_loss=0.56365]\n",
            "Step 84437   [1.778 sec/step, loss=0.54706, avg_loss=0.56281]\n",
            "Step 84438   [1.741 sec/step, loss=0.61151, avg_loss=0.56386]\n",
            "Step 84439   [1.743 sec/step, loss=0.58321, avg_loss=0.56358]\n",
            "Step 84440   [1.743 sec/step, loss=0.57534, avg_loss=0.56336]\n",
            "Step 84441   [1.742 sec/step, loss=0.59521, avg_loss=0.56398]\n",
            "Generated 16 batches of size 16 in 9.532 sec\n",
            "Step 84442   [1.742 sec/step, loss=0.59205, avg_loss=0.56386]\n",
            "Step 84443   [1.743 sec/step, loss=0.56244, avg_loss=0.56355]\n",
            "Step 84444   [1.749 sec/step, loss=0.57362, avg_loss=0.56350]\n",
            "Step 84445   [1.746 sec/step, loss=0.54353, avg_loss=0.56316]\n",
            "Step 84446   [1.739 sec/step, loss=0.56370, avg_loss=0.56293]\n",
            "Step 84447   [1.751 sec/step, loss=0.48756, avg_loss=0.56222]\n",
            "Step 84448   [1.751 sec/step, loss=0.54886, avg_loss=0.56215]\n",
            "Step 84449   [1.753 sec/step, loss=0.55601, avg_loss=0.56184]\n",
            "Step 84450   [1.738 sec/step, loss=0.57550, avg_loss=0.56186]\n",
            "Step 84451   [1.780 sec/step, loss=0.51291, avg_loss=0.56137]\n",
            "Step 84452   [1.780 sec/step, loss=0.59754, avg_loss=0.56157]\n",
            "Step 84453   [1.779 sec/step, loss=0.57187, avg_loss=0.56165]\n",
            "Step 84454   [1.785 sec/step, loss=0.57475, avg_loss=0.56174]\n",
            "Step 84455   [1.772 sec/step, loss=0.62268, avg_loss=0.56247]\n",
            "Step 84456   [1.776 sec/step, loss=0.55069, avg_loss=0.56212]\n",
            "Step 84457   [1.779 sec/step, loss=0.57580, avg_loss=0.56256]\n",
            "Step 84458   [1.773 sec/step, loss=0.59056, avg_loss=0.56295]\n",
            "Generated 16 batches of size 16 in 10.249 sec\n",
            "Step 84459   [1.774 sec/step, loss=0.56926, avg_loss=0.56327]\n",
            "Step 84460   [1.768 sec/step, loss=0.57520, avg_loss=0.56345]\n",
            "Step 84461   [1.763 sec/step, loss=0.60322, avg_loss=0.56360]\n",
            "Step 84462   [1.766 sec/step, loss=0.58066, avg_loss=0.56346]\n",
            "Step 84463   [1.727 sec/step, loss=0.55119, avg_loss=0.56433]\n",
            "Step 84464   [1.730 sec/step, loss=0.59506, avg_loss=0.56426]\n",
            "Step 84465   [1.730 sec/step, loss=0.59074, avg_loss=0.56435]\n",
            "Step 84466   [1.721 sec/step, loss=0.58985, avg_loss=0.56450]\n",
            "Step 84467   [1.718 sec/step, loss=0.60385, avg_loss=0.56489]\n",
            "Step 84468   [1.725 sec/step, loss=0.52341, avg_loss=0.56432]\n",
            "Step 84469   [1.722 sec/step, loss=0.55384, avg_loss=0.56415]\n",
            "Step 84470   [1.721 sec/step, loss=0.59981, avg_loss=0.56463]\n",
            "Step 84471   [1.729 sec/step, loss=0.58030, avg_loss=0.56467]\n",
            "Step 84472   [1.734 sec/step, loss=0.59701, avg_loss=0.56468]\n",
            "Step 84473   [1.738 sec/step, loss=0.58663, avg_loss=0.56460]\n",
            "Generated 16 batches of size 16 in 10.427 sec\n",
            "Step 84474   [1.770 sec/step, loss=0.45275, avg_loss=0.56379]\n",
            "Step 84475   [1.773 sec/step, loss=0.58850, avg_loss=0.56363]\n",
            "Step 84476   [1.784 sec/step, loss=0.52770, avg_loss=0.56312]\n",
            "Step 84477   [1.776 sec/step, loss=0.58371, avg_loss=0.56296]\n",
            "Step 84478   [1.773 sec/step, loss=0.59601, avg_loss=0.56294]\n",
            "Step 84479   [1.769 sec/step, loss=0.54812, avg_loss=0.56331]\n",
            "Step 84480   [1.768 sec/step, loss=0.60244, avg_loss=0.56366]\n",
            "Step 84481   [1.742 sec/step, loss=0.56717, avg_loss=0.56421]\n",
            "Step 84482   [1.761 sec/step, loss=0.50544, avg_loss=0.56346]\n",
            "Step 84483   [1.761 sec/step, loss=0.55181, avg_loss=0.56352]\n",
            "Step 84484   [1.763 sec/step, loss=0.60972, avg_loss=0.56444]\n",
            "Step 84485   [1.802 sec/step, loss=0.48721, avg_loss=0.56365]\n",
            "Step 84486   [1.809 sec/step, loss=0.56348, avg_loss=0.56324]\n",
            "Step 84487   [1.810 sec/step, loss=0.57118, avg_loss=0.56307]\n",
            "Generated 16 batches of size 16 in 10.415 sec\n",
            "Step 84488   [1.817 sec/step, loss=0.55594, avg_loss=0.56287]\n",
            "Step 84489   [1.816 sec/step, loss=0.59830, avg_loss=0.56299]\n",
            "Step 84490   [1.809 sec/step, loss=0.58687, avg_loss=0.56342]\n",
            "Step 84491   [1.808 sec/step, loss=0.57811, avg_loss=0.56334]\n",
            "Step 84492   [1.790 sec/step, loss=0.59417, avg_loss=0.56383]\n",
            "Step 84493   [1.795 sec/step, loss=0.56736, avg_loss=0.56365]\n",
            "Step 84494   [1.811 sec/step, loss=0.53351, avg_loss=0.56355]\n",
            "Step 84495   [1.773 sec/step, loss=0.60058, avg_loss=0.56487]\n",
            "Step 84496   [1.764 sec/step, loss=0.60991, avg_loss=0.56551]\n",
            "Step 84497   [1.760 sec/step, loss=0.56331, avg_loss=0.56530]\n",
            "Step 84498   [1.757 sec/step, loss=0.57949, avg_loss=0.56542]\n",
            "Step 84499   [1.763 sec/step, loss=0.50978, avg_loss=0.56485]\n",
            "Step 84500   [1.761 sec/step, loss=0.61710, avg_loss=0.56538]\n",
            "Writing summary at step: 84500\n",
            "Step 84501   [1.767 sec/step, loss=0.58854, avg_loss=0.56522]\n",
            "Step 84502   [1.760 sec/step, loss=0.57997, avg_loss=0.56503]\n",
            "Step 84503   [1.751 sec/step, loss=0.58429, avg_loss=0.56537]\n",
            "Step 84504   [1.758 sec/step, loss=0.56201, avg_loss=0.56534]\n",
            "Generated 16 batches of size 16 in 10.013 sec\n",
            "Step 84505   [1.800 sec/step, loss=0.41020, avg_loss=0.56366]\n",
            "Step 84506   [1.800 sec/step, loss=0.57942, avg_loss=0.56376]\n",
            "Step 84507   [1.786 sec/step, loss=0.58252, avg_loss=0.56445]\n",
            "Step 84508   [1.783 sec/step, loss=0.58200, avg_loss=0.56502]\n",
            "Step 84509   [1.739 sec/step, loss=0.57447, avg_loss=0.56712]\n",
            "Step 84510   [1.741 sec/step, loss=0.57141, avg_loss=0.56711]\n",
            "Step 84511   [1.745 sec/step, loss=0.56271, avg_loss=0.56698]\n",
            "Step 84512   [1.746 sec/step, loss=0.55728, avg_loss=0.56730]\n",
            "Step 84513   [1.747 sec/step, loss=0.58168, avg_loss=0.56740]\n",
            "Step 84514   [1.740 sec/step, loss=0.57404, avg_loss=0.56720]\n",
            "Step 84515   [1.744 sec/step, loss=0.58221, avg_loss=0.56705]\n",
            "Step 84516   [1.747 sec/step, loss=0.59338, avg_loss=0.56729]\n",
            "Step 84517   [1.744 sec/step, loss=0.61474, avg_loss=0.56805]\n",
            "Step 84518   [1.743 sec/step, loss=0.59730, avg_loss=0.56787]\n",
            "Step 84519   [1.735 sec/step, loss=0.56040, avg_loss=0.56862]\n",
            "Step 84520   [1.749 sec/step, loss=0.58712, avg_loss=0.56860]\n",
            "Step 84521   [1.747 sec/step, loss=0.56865, avg_loss=0.56841]\n",
            "Generated 16 batches of size 16 in 10.245 sec\n",
            "Step 84522   [1.746 sec/step, loss=0.52803, avg_loss=0.56793]\n",
            "Step 84523   [1.734 sec/step, loss=0.40955, avg_loss=0.56770]\n",
            "Step 84524   [1.746 sec/step, loss=0.56422, avg_loss=0.56728]\n",
            "Step 84525   [1.738 sec/step, loss=0.57614, avg_loss=0.56731]\n",
            "Step 84526   [1.741 sec/step, loss=0.57327, avg_loss=0.56748]\n",
            "Step 84527   [1.742 sec/step, loss=0.58503, avg_loss=0.56802]\n",
            "Step 84528   [1.777 sec/step, loss=0.46600, avg_loss=0.56670]\n",
            "Step 84529   [1.774 sec/step, loss=0.56168, avg_loss=0.56653]\n",
            "Step 84530   [1.782 sec/step, loss=0.53384, avg_loss=0.56597]\n",
            "Step 84531   [1.791 sec/step, loss=0.52938, avg_loss=0.56563]\n",
            "Step 84532   [1.787 sec/step, loss=0.58359, avg_loss=0.56611]\n",
            "Step 84533   [1.780 sec/step, loss=0.57433, avg_loss=0.56620]\n",
            "Step 84534   [1.786 sec/step, loss=0.57664, avg_loss=0.56616]\n",
            "Step 84535   [1.761 sec/step, loss=0.49320, avg_loss=0.56610]\n",
            "Step 84536   [1.767 sec/step, loss=0.54311, avg_loss=0.56554]\n",
            "Generated 16 batches of size 16 in 10.126 sec\n",
            "Step 84537   [1.750 sec/step, loss=0.55633, avg_loss=0.56564]\n",
            "Step 84538   [1.748 sec/step, loss=0.58024, avg_loss=0.56532]\n",
            "Step 84539   [1.748 sec/step, loss=0.62153, avg_loss=0.56571]\n",
            "Step 84540   [1.749 sec/step, loss=0.58244, avg_loss=0.56578]\n",
            "Step 84541   [1.743 sec/step, loss=0.57808, avg_loss=0.56561]\n",
            "Step 84542   [1.739 sec/step, loss=0.63034, avg_loss=0.56599]\n",
            "Step 84543   [1.736 sec/step, loss=0.59599, avg_loss=0.56633]\n",
            "Step 84544   [1.729 sec/step, loss=0.58084, avg_loss=0.56640]\n",
            "Step 84545   [1.722 sec/step, loss=0.58499, avg_loss=0.56681]\n",
            "Step 84546   [1.728 sec/step, loss=0.57832, avg_loss=0.56696]\n",
            "Step 84547   [1.709 sec/step, loss=0.58704, avg_loss=0.56795]\n",
            "Step 84548   [1.701 sec/step, loss=0.57290, avg_loss=0.56819]\n",
            "Step 84549   [1.711 sec/step, loss=0.52086, avg_loss=0.56784]\n",
            "Generated 16 batches of size 16 in 8.789 sec\n",
            "Step 84550   [1.760 sec/step, loss=0.44474, avg_loss=0.56653]\n",
            "Step 84551   [1.733 sec/step, loss=0.56021, avg_loss=0.56701]\n",
            "Step 84552   [1.735 sec/step, loss=0.57395, avg_loss=0.56677]\n",
            "Step 84553   [1.737 sec/step, loss=0.55949, avg_loss=0.56665]\n",
            "Step 84554   [1.732 sec/step, loss=0.57200, avg_loss=0.56662]\n",
            "Step 84555   [1.737 sec/step, loss=0.55865, avg_loss=0.56598]\n",
            "Step 84556   [1.729 sec/step, loss=0.55882, avg_loss=0.56606]\n",
            "Step 84557   [1.734 sec/step, loss=0.55132, avg_loss=0.56582]\n",
            "Step 84558   [1.737 sec/step, loss=0.57415, avg_loss=0.56565]\n",
            "Step 84559   [1.737 sec/step, loss=0.58410, avg_loss=0.56580]\n",
            "Step 84560   [1.727 sec/step, loss=0.56706, avg_loss=0.56572]\n",
            "Step 84561   [1.730 sec/step, loss=0.62271, avg_loss=0.56591]\n",
            "Step 84562   [1.771 sec/step, loss=0.41183, avg_loss=0.56423]\n",
            "Step 84563   [1.770 sec/step, loss=0.58314, avg_loss=0.56455]\n",
            "Step 84564   [1.784 sec/step, loss=0.58350, avg_loss=0.56443]\n",
            "Step 84565   [1.794 sec/step, loss=0.55230, avg_loss=0.56405]\n",
            "Step 84566   [1.806 sec/step, loss=0.59280, avg_loss=0.56408]\n",
            "Generated 16 batches of size 16 in 7.230 sec\n",
            "Step 84567   [1.802 sec/step, loss=0.54705, avg_loss=0.56351]\n",
            "Step 84568   [1.794 sec/step, loss=0.58026, avg_loss=0.56408]\n",
            "Step 84569   [1.794 sec/step, loss=0.56113, avg_loss=0.56415]\n",
            "Step 84570   [1.790 sec/step, loss=0.57631, avg_loss=0.56391]\n",
            "Step 84571   [1.783 sec/step, loss=0.58683, avg_loss=0.56398]\n",
            "Step 84572   [1.774 sec/step, loss=0.60783, avg_loss=0.56409]\n",
            "Step 84573   [1.768 sec/step, loss=0.59127, avg_loss=0.56413]\n",
            "Step 84574   [1.730 sec/step, loss=0.55238, avg_loss=0.56513]\n",
            "Step 84575   [1.725 sec/step, loss=0.58063, avg_loss=0.56505]\n",
            "Step 84576   [1.710 sec/step, loss=0.59309, avg_loss=0.56571]\n",
            "Step 84577   [1.710 sec/step, loss=0.59613, avg_loss=0.56583]\n",
            "Step 84578   [1.714 sec/step, loss=0.58636, avg_loss=0.56573]\n",
            "Step 84579   [1.708 sec/step, loss=0.52805, avg_loss=0.56553]\n",
            "Step 84580   [1.741 sec/step, loss=0.53157, avg_loss=0.56482]\n",
            "Step 84581   [1.755 sec/step, loss=0.58492, avg_loss=0.56500]\n",
            "Generated 16 batches of size 16 in 7.239 sec\n",
            "Step 84582   [1.740 sec/step, loss=0.54472, avg_loss=0.56539]\n",
            "Step 84583   [1.753 sec/step, loss=0.57234, avg_loss=0.56560]\n",
            "Step 84584   [1.755 sec/step, loss=0.59852, avg_loss=0.56549]\n",
            "Step 84585   [1.714 sec/step, loss=0.58027, avg_loss=0.56642]\n",
            "Step 84586   [1.714 sec/step, loss=0.53706, avg_loss=0.56615]\n",
            "Step 84587   [1.712 sec/step, loss=0.61168, avg_loss=0.56656]\n",
            "Step 84588   [1.704 sec/step, loss=0.59683, avg_loss=0.56697]\n",
            "Step 84589   [1.706 sec/step, loss=0.59582, avg_loss=0.56694]\n",
            "Step 84590   [1.717 sec/step, loss=0.50002, avg_loss=0.56607]\n",
            "Step 84591   [1.715 sec/step, loss=0.56489, avg_loss=0.56594]\n",
            "Step 84592   [1.716 sec/step, loss=0.51504, avg_loss=0.56515]\n",
            "Step 84593   [1.712 sec/step, loss=0.54489, avg_loss=0.56493]\n",
            "Step 84594   [1.700 sec/step, loss=0.58455, avg_loss=0.56544]\n",
            "Step 84595   [1.698 sec/step, loss=0.59960, avg_loss=0.56543]\n",
            "Step 84596   [1.700 sec/step, loss=0.58795, avg_loss=0.56521]\n",
            "Step 84597   [1.697 sec/step, loss=0.59820, avg_loss=0.56556]\n",
            "Generated 16 batches of size 16 in 8.313 sec\n",
            "Step 84598   [1.751 sec/step, loss=0.34384, avg_loss=0.56320]\n",
            "Step 84599   [1.742 sec/step, loss=0.58043, avg_loss=0.56391]\n",
            "Step 84600   [1.739 sec/step, loss=0.59184, avg_loss=0.56365]\n",
            "Writing summary at step: 84600\n",
            "Step 84601   [1.740 sec/step, loss=0.54314, avg_loss=0.56320]\n",
            "Step 84602   [1.740 sec/step, loss=0.56372, avg_loss=0.56304]\n",
            "Step 84603   [1.748 sec/step, loss=0.57589, avg_loss=0.56295]\n",
            "Step 84604   [1.741 sec/step, loss=0.58181, avg_loss=0.56315]\n",
            "Step 84605   [1.693 sec/step, loss=0.60554, avg_loss=0.56510]\n",
            "Step 84606   [1.712 sec/step, loss=0.53763, avg_loss=0.56469]\n",
            "Step 84607   [1.708 sec/step, loss=0.53784, avg_loss=0.56424]\n",
            "Step 84608   [1.706 sec/step, loss=0.60270, avg_loss=0.56445]\n",
            "Step 84609   [1.747 sec/step, loss=0.47612, avg_loss=0.56346]\n",
            "Step 84610   [1.746 sec/step, loss=0.60271, avg_loss=0.56378]\n",
            "Step 84611   [1.749 sec/step, loss=0.53516, avg_loss=0.56350]\n",
            "Step 84612   [1.750 sec/step, loss=0.58132, avg_loss=0.56374]\n",
            "Step 84613   [1.751 sec/step, loss=0.57553, avg_loss=0.56368]\n",
            "Step 84614   [1.764 sec/step, loss=0.57143, avg_loss=0.56365]\n",
            "Generated 16 batches of size 16 in 8.546 sec\n",
            "Step 84615   [1.772 sec/step, loss=0.58360, avg_loss=0.56367]\n",
            "Step 84616   [1.774 sec/step, loss=0.56237, avg_loss=0.56336]\n",
            "Step 84617   [1.781 sec/step, loss=0.55988, avg_loss=0.56281]\n",
            "Step 84618   [1.780 sec/step, loss=0.59871, avg_loss=0.56282]\n",
            "Step 84619   [1.772 sec/step, loss=0.59973, avg_loss=0.56322]\n",
            "Step 84620   [1.758 sec/step, loss=0.56046, avg_loss=0.56295]\n",
            "Step 84621   [1.755 sec/step, loss=0.62364, avg_loss=0.56350]\n",
            "Step 84622   [1.748 sec/step, loss=0.57144, avg_loss=0.56393]\n",
            "Step 84623   [1.719 sec/step, loss=0.61318, avg_loss=0.56597]\n",
            "Step 84624   [1.745 sec/step, loss=0.50022, avg_loss=0.56533]\n",
            "Step 84625   [1.747 sec/step, loss=0.55312, avg_loss=0.56510]\n",
            "Step 84626   [1.742 sec/step, loss=0.59848, avg_loss=0.56535]\n",
            "Step 84627   [1.762 sec/step, loss=0.52134, avg_loss=0.56471]\n",
            "Step 84628   [1.731 sec/step, loss=0.58797, avg_loss=0.56593]\n",
            "Step 84629   [1.740 sec/step, loss=0.55553, avg_loss=0.56587]\n",
            "Step 84630   [1.735 sec/step, loss=0.58198, avg_loss=0.56635]\n",
            "Generated 16 batches of size 16 in 8.916 sec\n",
            "Step 84631   [1.734 sec/step, loss=0.60263, avg_loss=0.56709]\n",
            "Step 84632   [1.735 sec/step, loss=0.56322, avg_loss=0.56688]\n",
            "Step 84633   [1.732 sec/step, loss=0.57660, avg_loss=0.56691]\n",
            "Step 84634   [1.734 sec/step, loss=0.55692, avg_loss=0.56671]\n",
            "Step 84635   [1.724 sec/step, loss=0.57535, avg_loss=0.56753]\n",
            "Step 84636   [1.715 sec/step, loss=0.60133, avg_loss=0.56811]\n",
            "Step 84637   [1.751 sec/step, loss=0.51768, avg_loss=0.56773]\n",
            "Step 84638   [1.752 sec/step, loss=0.57302, avg_loss=0.56765]\n",
            "Step 84639   [1.762 sec/step, loss=0.54643, avg_loss=0.56690]\n",
            "Step 84640   [1.762 sec/step, loss=0.60893, avg_loss=0.56717]\n",
            "Step 84641   [1.758 sec/step, loss=0.58872, avg_loss=0.56727]\n",
            "Step 84642   [1.760 sec/step, loss=0.57773, avg_loss=0.56675]\n",
            "Step 84643   [1.766 sec/step, loss=0.53470, avg_loss=0.56613]\n",
            "Step 84644   [1.791 sec/step, loss=0.51569, avg_loss=0.56548]\n",
            "Step 84645   [1.802 sec/step, loss=0.57867, avg_loss=0.56542]\n",
            "Step 84646   [1.803 sec/step, loss=0.57406, avg_loss=0.56538]\n",
            "Generated 16 batches of size 16 in 10.132 sec\n",
            "Step 84647   [1.812 sec/step, loss=0.57593, avg_loss=0.56527]\n",
            "Step 84648   [1.817 sec/step, loss=0.55064, avg_loss=0.56504]\n",
            "Step 84649   [1.804 sec/step, loss=0.59044, avg_loss=0.56574]\n",
            "Step 84650   [1.754 sec/step, loss=0.58411, avg_loss=0.56713]\n",
            "Step 84651   [1.739 sec/step, loss=0.61726, avg_loss=0.56770]\n",
            "Step 84652   [1.741 sec/step, loss=0.55585, avg_loss=0.56752]\n",
            "Step 84653   [1.734 sec/step, loss=0.54110, avg_loss=0.56734]\n",
            "Step 84654   [1.732 sec/step, loss=0.59547, avg_loss=0.56757]\n",
            "Step 84655   [1.727 sec/step, loss=0.55471, avg_loss=0.56753]\n",
            "Step 84656   [1.736 sec/step, loss=0.53864, avg_loss=0.56733]\n",
            "Step 84657   [1.759 sec/step, loss=0.47055, avg_loss=0.56652]\n",
            "Step 84658   [1.754 sec/step, loss=0.61170, avg_loss=0.56690]\n",
            "Step 84659   [1.754 sec/step, loss=0.56616, avg_loss=0.56672]\n",
            "Step 84660   [1.755 sec/step, loss=0.58219, avg_loss=0.56687]\n",
            "Step 84661   [1.752 sec/step, loss=0.56456, avg_loss=0.56629]\n",
            "Step 84662   [1.726 sec/step, loss=0.57941, avg_loss=0.56797]\n",
            "Step 84663   [1.731 sec/step, loss=0.60789, avg_loss=0.56821]\n",
            "Step 84664   [1.717 sec/step, loss=0.58066, avg_loss=0.56819]\n",
            "Generated 16 batches of size 16 in 9.932 sec\n",
            "Step 84665   [1.708 sec/step, loss=0.54315, avg_loss=0.56809]\n",
            "Step 84666   [1.713 sec/step, loss=0.54323, avg_loss=0.56760]\n",
            "Step 84667   [1.720 sec/step, loss=0.55255, avg_loss=0.56765]\n",
            "Step 84668   [1.724 sec/step, loss=0.49663, avg_loss=0.56682]\n",
            "Step 84669   [1.727 sec/step, loss=0.51263, avg_loss=0.56633]\n",
            "Step 84670   [1.737 sec/step, loss=0.54510, avg_loss=0.56602]\n",
            "Step 84671   [1.737 sec/step, loss=0.54580, avg_loss=0.56561]\n",
            "Step 84672   [1.781 sec/step, loss=0.39034, avg_loss=0.56343]\n",
            "Step 84673   [1.782 sec/step, loss=0.56928, avg_loss=0.56321]\n",
            "Step 84674   [1.773 sec/step, loss=0.57772, avg_loss=0.56347]\n",
            "Step 84675   [1.777 sec/step, loss=0.59800, avg_loss=0.56364]\n",
            "Step 84676   [1.779 sec/step, loss=0.53529, avg_loss=0.56306]\n",
            "Step 84677   [1.784 sec/step, loss=0.61563, avg_loss=0.56326]\n",
            "Step 84678   [1.785 sec/step, loss=0.60041, avg_loss=0.56340]\n",
            "Step 84679   [1.780 sec/step, loss=0.57546, avg_loss=0.56387]\n",
            "Step 84680   [1.753 sec/step, loss=0.55581, avg_loss=0.56412]\n",
            "Step 84681   [1.742 sec/step, loss=0.60225, avg_loss=0.56429]\n",
            "Generated 16 batches of size 16 in 10.233 sec\n",
            "Step 84682   [1.742 sec/step, loss=0.60438, avg_loss=0.56489]\n",
            "Step 84683   [1.741 sec/step, loss=0.57967, avg_loss=0.56496]\n",
            "Step 84684   [1.744 sec/step, loss=0.59195, avg_loss=0.56489]\n",
            "Step 84685   [1.780 sec/step, loss=0.48478, avg_loss=0.56394]\n",
            "Step 84686   [1.776 sec/step, loss=0.61023, avg_loss=0.56467]\n",
            "Step 84687   [1.788 sec/step, loss=0.52742, avg_loss=0.56383]\n",
            "Step 84688   [1.791 sec/step, loss=0.55706, avg_loss=0.56343]\n",
            "Step 84689   [1.788 sec/step, loss=0.59275, avg_loss=0.56340]\n",
            "Step 84690   [1.777 sec/step, loss=0.60076, avg_loss=0.56441]\n",
            "Step 84691   [1.787 sec/step, loss=0.56322, avg_loss=0.56439]\n",
            "Step 84692   [1.784 sec/step, loss=0.58854, avg_loss=0.56512]\n",
            "Step 84693   [1.779 sec/step, loss=0.58045, avg_loss=0.56548]\n",
            "Step 84694   [1.779 sec/step, loss=0.58587, avg_loss=0.56549]\n",
            "Step 84695   [1.803 sec/step, loss=0.48196, avg_loss=0.56432]\n",
            "Generated 16 batches of size 16 in 10.043 sec\n",
            "Step 84696   [1.805 sec/step, loss=0.56880, avg_loss=0.56413]\n",
            "Step 84697   [1.802 sec/step, loss=0.58516, avg_loss=0.56400]\n",
            "Step 84698   [1.747 sec/step, loss=0.56803, avg_loss=0.56624]\n",
            "Step 84699   [1.751 sec/step, loss=0.57837, avg_loss=0.56622]\n",
            "Step 84700   [1.759 sec/step, loss=0.57923, avg_loss=0.56609]\n",
            "Writing summary at step: 84700\n",
            "Step 84701   [1.751 sec/step, loss=0.55333, avg_loss=0.56619]\n",
            "Step 84702   [1.752 sec/step, loss=0.60481, avg_loss=0.56660]\n",
            "Step 84703   [1.742 sec/step, loss=0.54746, avg_loss=0.56632]\n",
            "Step 84704   [1.742 sec/step, loss=0.59169, avg_loss=0.56642]\n",
            "Step 84705   [1.744 sec/step, loss=0.56656, avg_loss=0.56603]\n",
            "Step 84706   [1.728 sec/step, loss=0.57272, avg_loss=0.56638]\n",
            "Step 84707   [1.732 sec/step, loss=0.57599, avg_loss=0.56676]\n",
            "Step 84708   [1.784 sec/step, loss=0.45980, avg_loss=0.56533]\n",
            "Generated 16 batches of size 16 in 10.029 sec\n",
            "Step 84709   [1.751 sec/step, loss=0.56522, avg_loss=0.56622]\n",
            "Step 84710   [1.753 sec/step, loss=0.57884, avg_loss=0.56598]\n",
            "Step 84711   [1.750 sec/step, loss=0.55262, avg_loss=0.56616]\n",
            "Step 84712   [1.751 sec/step, loss=0.56143, avg_loss=0.56596]\n",
            "Step 84713   [1.749 sec/step, loss=0.61184, avg_loss=0.56632]\n",
            "Step 84714   [1.751 sec/step, loss=0.48715, avg_loss=0.56548]\n",
            "Step 84715   [1.739 sec/step, loss=0.55274, avg_loss=0.56517]\n",
            "Step 84716   [1.739 sec/step, loss=0.57589, avg_loss=0.56531]\n",
            "Step 84717   [1.736 sec/step, loss=0.54975, avg_loss=0.56521]\n",
            "Step 84718   [1.737 sec/step, loss=0.57024, avg_loss=0.56492]\n",
            "Step 84719   [1.736 sec/step, loss=0.55930, avg_loss=0.56452]\n",
            "Step 84720   [1.739 sec/step, loss=0.58924, avg_loss=0.56480]\n",
            "Step 84721   [1.780 sec/step, loss=0.38742, avg_loss=0.56244]\n",
            "Step 84722   [1.786 sec/step, loss=0.56688, avg_loss=0.56240]\n",
            "Step 84723   [1.783 sec/step, loss=0.57196, avg_loss=0.56198]\n",
            "Step 84724   [1.745 sec/step, loss=0.60389, avg_loss=0.56302]\n",
            "Step 84725   [1.757 sec/step, loss=0.52853, avg_loss=0.56277]\n",
            "Step 84726   [1.762 sec/step, loss=0.62852, avg_loss=0.56307]\n",
            "Generated 16 batches of size 16 in 9.825 sec\n",
            "Step 84727   [1.749 sec/step, loss=0.53874, avg_loss=0.56325]\n",
            "Step 84728   [1.747 sec/step, loss=0.58869, avg_loss=0.56326]\n",
            "Step 84729   [1.738 sec/step, loss=0.59767, avg_loss=0.56368]\n",
            "Step 84730   [1.750 sec/step, loss=0.54559, avg_loss=0.56331]\n",
            "Step 84731   [1.743 sec/step, loss=0.56402, avg_loss=0.56293]\n",
            "Step 84732   [1.746 sec/step, loss=0.52950, avg_loss=0.56259]\n",
            "Step 84733   [1.748 sec/step, loss=0.56094, avg_loss=0.56243]\n",
            "Step 84734   [1.749 sec/step, loss=0.52579, avg_loss=0.56212]\n",
            "Step 84735   [1.745 sec/step, loss=0.61372, avg_loss=0.56251]\n",
            "Step 84736   [1.744 sec/step, loss=0.60505, avg_loss=0.56254]\n",
            "Step 84737   [1.708 sec/step, loss=0.60942, avg_loss=0.56346]\n",
            "Step 84738   [1.753 sec/step, loss=0.42073, avg_loss=0.56194]\n",
            "Step 84739   [1.744 sec/step, loss=0.56803, avg_loss=0.56215]\n",
            "Step 84740   [1.743 sec/step, loss=0.54748, avg_loss=0.56154]\n",
            "Step 84741   [1.749 sec/step, loss=0.56728, avg_loss=0.56132]\n",
            "Generated 16 batches of size 16 in 10.646 sec\n",
            "Step 84742   [1.750 sec/step, loss=0.57703, avg_loss=0.56132]\n",
            "Step 84743   [1.755 sec/step, loss=0.54795, avg_loss=0.56145]\n",
            "Step 84744   [1.733 sec/step, loss=0.57232, avg_loss=0.56202]\n",
            "Step 84745   [1.722 sec/step, loss=0.60384, avg_loss=0.56227]\n",
            "Step 84746   [1.720 sec/step, loss=0.55978, avg_loss=0.56213]\n",
            "Step 84747   [1.715 sec/step, loss=0.60939, avg_loss=0.56246]\n",
            "Step 84748   [1.716 sec/step, loss=0.55118, avg_loss=0.56247]\n",
            "Step 84749   [1.718 sec/step, loss=0.58443, avg_loss=0.56241]\n",
            "Step 84750   [1.717 sec/step, loss=0.59951, avg_loss=0.56256]\n",
            "Step 84751   [1.757 sec/step, loss=0.44688, avg_loss=0.56086]\n",
            "Step 84752   [1.758 sec/step, loss=0.54577, avg_loss=0.56075]\n",
            "Step 84753   [1.760 sec/step, loss=0.56830, avg_loss=0.56103]\n",
            "Step 84754   [1.762 sec/step, loss=0.58996, avg_loss=0.56097]\n",
            "Step 84755   [1.761 sec/step, loss=0.58733, avg_loss=0.56130]\n",
            "Step 84756   [1.771 sec/step, loss=0.57418, avg_loss=0.56165]\n",
            "Generated 16 batches of size 16 in 7.763 sec\n",
            "Step 84757   [1.762 sec/step, loss=0.54182, avg_loss=0.56237]\n",
            "Step 84758   [1.762 sec/step, loss=0.58734, avg_loss=0.56212]\n",
            "Step 84759   [1.765 sec/step, loss=0.57998, avg_loss=0.56226]\n",
            "Step 84760   [1.763 sec/step, loss=0.55834, avg_loss=0.56202]\n",
            "Step 84761   [1.760 sec/step, loss=0.56271, avg_loss=0.56200]\n",
            "Step 84762   [1.755 sec/step, loss=0.53056, avg_loss=0.56152]\n",
            "Step 84763   [1.752 sec/step, loss=0.53149, avg_loss=0.56075]\n",
            "Step 84764   [1.745 sec/step, loss=0.55837, avg_loss=0.56053]\n",
            "Step 84765   [1.744 sec/step, loss=0.56658, avg_loss=0.56076]\n",
            "Step 84766   [1.736 sec/step, loss=0.58044, avg_loss=0.56113]\n",
            "Step 84767   [1.730 sec/step, loss=0.57250, avg_loss=0.56133]\n",
            "Step 84768   [1.733 sec/step, loss=0.58747, avg_loss=0.56224]\n",
            "Step 84769   [1.729 sec/step, loss=0.59470, avg_loss=0.56306]\n",
            "Step 84770   [1.741 sec/step, loss=0.52193, avg_loss=0.56283]\n",
            "Step 84771   [1.738 sec/step, loss=0.59991, avg_loss=0.56337]\n",
            "Step 84772   [1.701 sec/step, loss=0.61196, avg_loss=0.56559]\n",
            "Generated 16 batches of size 16 in 7.409 sec\n",
            "Step 84773   [1.707 sec/step, loss=0.55926, avg_loss=0.56549]\n",
            "Step 84774   [1.712 sec/step, loss=0.53786, avg_loss=0.56509]\n",
            "Step 84775   [1.720 sec/step, loss=0.54544, avg_loss=0.56456]\n",
            "Step 84776   [1.721 sec/step, loss=0.59474, avg_loss=0.56516]\n",
            "Step 84777   [1.751 sec/step, loss=0.43021, avg_loss=0.56330]\n",
            "Step 84778   [1.745 sec/step, loss=0.59662, avg_loss=0.56327]\n",
            "Step 84779   [1.744 sec/step, loss=0.60732, avg_loss=0.56359]\n",
            "Step 84780   [1.737 sec/step, loss=0.60834, avg_loss=0.56411]\n",
            "Step 84781   [1.733 sec/step, loss=0.59102, avg_loss=0.56400]\n",
            "Step 84782   [1.737 sec/step, loss=0.56927, avg_loss=0.56365]\n",
            "Step 84783   [1.724 sec/step, loss=0.59390, avg_loss=0.56379]\n",
            "Step 84784   [1.728 sec/step, loss=0.55043, avg_loss=0.56337]\n",
            "Step 84785   [1.691 sec/step, loss=0.54933, avg_loss=0.56402]\n",
            "Step 84786   [1.697 sec/step, loss=0.58463, avg_loss=0.56376]\n",
            "Step 84787   [1.707 sec/step, loss=0.53638, avg_loss=0.56385]\n",
            "Step 84788   [1.707 sec/step, loss=0.58719, avg_loss=0.56415]\n",
            "Step 84789   [1.706 sec/step, loss=0.60961, avg_loss=0.56432]\n",
            "Generated 16 batches of size 16 in 8.241 sec\n",
            "Step 84790   [1.742 sec/step, loss=0.47496, avg_loss=0.56307]\n",
            "Step 84791   [1.741 sec/step, loss=0.47709, avg_loss=0.56220]\n",
            "Step 84792   [1.739 sec/step, loss=0.58464, avg_loss=0.56217]\n",
            "Step 84793   [1.741 sec/step, loss=0.60488, avg_loss=0.56241]\n",
            "Step 84794   [1.743 sec/step, loss=0.61808, avg_loss=0.56273]\n",
            "Step 84795   [1.719 sec/step, loss=0.57319, avg_loss=0.56364]\n",
            "Step 84796   [1.714 sec/step, loss=0.57786, avg_loss=0.56373]\n",
            "Step 84797   [1.720 sec/step, loss=0.55282, avg_loss=0.56341]\n",
            "Step 84798   [1.727 sec/step, loss=0.58681, avg_loss=0.56360]\n",
            "Step 84799   [1.723 sec/step, loss=0.58269, avg_loss=0.56364]\n",
            "Step 84800   [1.722 sec/step, loss=0.52308, avg_loss=0.56308]\n",
            "Writing summary at step: 84800\n",
            "Step 84801   [1.726 sec/step, loss=0.56236, avg_loss=0.56317]\n",
            "Step 84802   [1.726 sec/step, loss=0.57661, avg_loss=0.56289]\n",
            "Step 84803   [1.747 sec/step, loss=0.50438, avg_loss=0.56246]\n",
            "Step 84804   [1.749 sec/step, loss=0.57841, avg_loss=0.56233]\n",
            "Step 84805   [1.754 sec/step, loss=0.57381, avg_loss=0.56240]\n",
            "Generated 16 batches of size 16 in 9.309 sec\n",
            "Step 84806   [1.756 sec/step, loss=0.57666, avg_loss=0.56244]\n",
            "Step 84807   [1.758 sec/step, loss=0.50545, avg_loss=0.56173]\n",
            "Step 84808   [1.742 sec/step, loss=0.49069, avg_loss=0.56204]\n",
            "Step 84809   [1.739 sec/step, loss=0.52449, avg_loss=0.56163]\n",
            "Step 84810   [1.741 sec/step, loss=0.59055, avg_loss=0.56175]\n",
            "Step 84811   [1.741 sec/step, loss=0.58183, avg_loss=0.56204]\n",
            "Step 84812   [1.733 sec/step, loss=0.58058, avg_loss=0.56223]\n",
            "Step 84813   [1.739 sec/step, loss=0.58199, avg_loss=0.56194]\n",
            "Step 84814   [1.728 sec/step, loss=0.53196, avg_loss=0.56238]\n",
            "Step 84815   [1.732 sec/step, loss=0.61641, avg_loss=0.56302]\n",
            "Step 84816   [1.728 sec/step, loss=0.55402, avg_loss=0.56280]\n",
            "Step 84817   [1.737 sec/step, loss=0.53297, avg_loss=0.56263]\n",
            "Step 84818   [1.757 sec/step, loss=0.57803, avg_loss=0.56271]\n",
            "Step 84819   [1.766 sec/step, loss=0.58123, avg_loss=0.56293]\n",
            "Generated 16 batches of size 16 in 8.730 sec\n",
            "Step 84820   [1.764 sec/step, loss=0.58410, avg_loss=0.56288]\n",
            "Step 84821   [1.764 sec/step, loss=0.48516, avg_loss=0.56386]\n",
            "Step 84822   [1.758 sec/step, loss=0.56111, avg_loss=0.56380]\n",
            "Step 84823   [1.764 sec/step, loss=0.51909, avg_loss=0.56327]\n",
            "Step 84824   [1.764 sec/step, loss=0.59506, avg_loss=0.56318]\n",
            "Step 84825   [1.749 sec/step, loss=0.56346, avg_loss=0.56353]\n",
            "Step 84826   [1.785 sec/step, loss=0.41674, avg_loss=0.56141]\n",
            "Step 84827   [1.779 sec/step, loss=0.57836, avg_loss=0.56181]\n",
            "Step 84828   [1.779 sec/step, loss=0.58149, avg_loss=0.56174]\n",
            "Step 84829   [1.788 sec/step, loss=0.52042, avg_loss=0.56097]\n",
            "Step 84830   [1.789 sec/step, loss=0.52845, avg_loss=0.56079]\n",
            "Step 84831   [1.789 sec/step, loss=0.55554, avg_loss=0.56071]\n",
            "Step 84832   [1.783 sec/step, loss=0.58658, avg_loss=0.56128]\n",
            "Step 84833   [1.796 sec/step, loss=0.57616, avg_loss=0.56143]\n",
            "Step 84834   [1.790 sec/step, loss=0.55780, avg_loss=0.56175]\n",
            "Step 84835   [1.788 sec/step, loss=0.61264, avg_loss=0.56174]\n",
            "Step 84836   [1.791 sec/step, loss=0.58473, avg_loss=0.56154]\n",
            "Step 84837   [1.796 sec/step, loss=0.53912, avg_loss=0.56084]\n",
            "Generated 16 batches of size 16 in 9.109 sec\n",
            "Step 84838   [1.752 sec/step, loss=0.59244, avg_loss=0.56255]\n",
            "Step 84839   [1.753 sec/step, loss=0.54374, avg_loss=0.56231]\n",
            "Step 84840   [1.757 sec/step, loss=0.56542, avg_loss=0.56249]\n",
            "Step 84841   [1.755 sec/step, loss=0.54125, avg_loss=0.56223]\n",
            "Step 84842   [1.755 sec/step, loss=0.57943, avg_loss=0.56225]\n",
            "Step 84843   [1.744 sec/step, loss=0.59155, avg_loss=0.56269]\n",
            "Step 84844   [1.746 sec/step, loss=0.56644, avg_loss=0.56263]\n",
            "Step 84845   [1.744 sec/step, loss=0.56307, avg_loss=0.56222]\n",
            "Step 84846   [1.746 sec/step, loss=0.55713, avg_loss=0.56220]\n",
            "Step 84847   [1.754 sec/step, loss=0.55897, avg_loss=0.56169]\n",
            "Step 84848   [1.755 sec/step, loss=0.57004, avg_loss=0.56188]\n",
            "Step 84849   [1.753 sec/step, loss=0.54836, avg_loss=0.56152]\n",
            "Step 84850   [1.763 sec/step, loss=0.56996, avg_loss=0.56122]\n",
            "Step 84851   [1.727 sec/step, loss=0.57968, avg_loss=0.56255]\n",
            "Step 84852   [1.724 sec/step, loss=0.59699, avg_loss=0.56306]\n",
            "Step 84853   [1.728 sec/step, loss=0.58738, avg_loss=0.56325]\n",
            "Generated 16 batches of size 16 in 10.139 sec\n",
            "Step 84854   [1.755 sec/step, loss=0.48491, avg_loss=0.56220]\n",
            "Step 84855   [1.753 sec/step, loss=0.59055, avg_loss=0.56224]\n",
            "Step 84856   [1.737 sec/step, loss=0.56216, avg_loss=0.56212]\n",
            "Step 84857   [1.713 sec/step, loss=0.59897, avg_loss=0.56269]\n",
            "Step 84858   [1.720 sec/step, loss=0.59172, avg_loss=0.56273]\n",
            "Step 84859   [1.715 sec/step, loss=0.60786, avg_loss=0.56301]\n",
            "Step 84860   [1.731 sec/step, loss=0.52860, avg_loss=0.56271]\n",
            "Step 84861   [1.739 sec/step, loss=0.58954, avg_loss=0.56298]\n",
            "Step 84862   [1.731 sec/step, loss=0.57479, avg_loss=0.56342]\n",
            "Step 84863   [1.727 sec/step, loss=0.54537, avg_loss=0.56356]\n",
            "Step 84864   [1.727 sec/step, loss=0.61038, avg_loss=0.56408]\n",
            "Step 84865   [1.742 sec/step, loss=0.57749, avg_loss=0.56419]\n",
            "Step 84866   [1.736 sec/step, loss=0.59643, avg_loss=0.56435]\n",
            "Step 84867   [1.741 sec/step, loss=0.60117, avg_loss=0.56464]\n",
            "Step 84868   [1.736 sec/step, loss=0.55095, avg_loss=0.56427]\n",
            "Generated 16 batches of size 16 in 10.123 sec\n",
            "Step 84869   [1.787 sec/step, loss=0.49814, avg_loss=0.56331]\n",
            "Step 84870   [1.765 sec/step, loss=0.55314, avg_loss=0.56362]\n",
            "Step 84871   [1.759 sec/step, loss=0.56502, avg_loss=0.56327]\n",
            "Step 84872   [1.758 sec/step, loss=0.52277, avg_loss=0.56238]\n",
            "Step 84873   [1.756 sec/step, loss=0.58041, avg_loss=0.56259]\n",
            "Step 84874   [1.753 sec/step, loss=0.56856, avg_loss=0.56290]\n",
            "Step 84875   [1.742 sec/step, loss=0.59742, avg_loss=0.56342]\n",
            "Step 84876   [1.738 sec/step, loss=0.57412, avg_loss=0.56321]\n",
            "Step 84877   [1.703 sec/step, loss=0.56662, avg_loss=0.56457]\n",
            "Step 84878   [1.721 sec/step, loss=0.51444, avg_loss=0.56375]\n",
            "Step 84879   [1.733 sec/step, loss=0.56427, avg_loss=0.56332]\n",
            "Step 84880   [1.735 sec/step, loss=0.59002, avg_loss=0.56314]\n",
            "Step 84881   [1.739 sec/step, loss=0.59273, avg_loss=0.56316]\n",
            "Step 84882   [1.734 sec/step, loss=0.59621, avg_loss=0.56343]\n",
            "Step 84883   [1.789 sec/step, loss=0.42436, avg_loss=0.56173]\n",
            "Generated 16 batches of size 16 in 10.718 sec\n",
            "Step 84884   [1.789 sec/step, loss=0.53344, avg_loss=0.56156]\n",
            "Step 84885   [1.793 sec/step, loss=0.58611, avg_loss=0.56193]\n",
            "Step 84886   [1.790 sec/step, loss=0.59378, avg_loss=0.56202]\n",
            "Step 84887   [1.770 sec/step, loss=0.60553, avg_loss=0.56271]\n",
            "Step 84888   [1.767 sec/step, loss=0.58937, avg_loss=0.56273]\n",
            "Step 84889   [1.766 sec/step, loss=0.59868, avg_loss=0.56262]\n",
            "Step 84890   [1.732 sec/step, loss=0.59022, avg_loss=0.56378]\n",
            "Step 84891   [1.727 sec/step, loss=0.58956, avg_loss=0.56490]\n",
            "Step 84892   [1.728 sec/step, loss=0.58826, avg_loss=0.56494]\n",
            "Step 84893   [1.727 sec/step, loss=0.60492, avg_loss=0.56494]\n",
            "Step 84894   [1.727 sec/step, loss=0.58539, avg_loss=0.56461]\n",
            "Step 84895   [1.730 sec/step, loss=0.59654, avg_loss=0.56484]\n",
            "Step 84896   [1.773 sec/step, loss=0.44747, avg_loss=0.56354]\n",
            "Step 84897   [1.775 sec/step, loss=0.57809, avg_loss=0.56379]\n",
            "Step 84898   [1.780 sec/step, loss=0.53302, avg_loss=0.56326]\n",
            "Step 84899   [1.799 sec/step, loss=0.51618, avg_loss=0.56259]\n",
            "Step 84900   [1.793 sec/step, loss=0.56377, avg_loss=0.56300]\n",
            "Writing summary at step: 84900\n",
            "Generated 16 batches of size 16 in 10.118 sec\n",
            "Step 84901   [1.789 sec/step, loss=0.59850, avg_loss=0.56336]\n",
            "Step 84902   [1.791 sec/step, loss=0.61175, avg_loss=0.56371]\n",
            "Step 84903   [1.774 sec/step, loss=0.59727, avg_loss=0.56464]\n",
            "Step 84904   [1.773 sec/step, loss=0.61795, avg_loss=0.56503]\n",
            "Step 84905   [1.770 sec/step, loss=0.60233, avg_loss=0.56532]\n",
            "Step 84906   [1.765 sec/step, loss=0.60048, avg_loss=0.56556]\n",
            "Step 84907   [1.755 sec/step, loss=0.55836, avg_loss=0.56609]\n",
            "Step 84908   [1.730 sec/step, loss=0.56528, avg_loss=0.56683]\n",
            "Step 84909   [1.728 sec/step, loss=0.55180, avg_loss=0.56711]\n",
            "Step 84910   [1.739 sec/step, loss=0.56093, avg_loss=0.56681]\n",
            "Step 84911   [1.738 sec/step, loss=0.58639, avg_loss=0.56686]\n",
            "Step 84912   [1.739 sec/step, loss=0.61645, avg_loss=0.56721]\n",
            "Step 84913   [1.735 sec/step, loss=0.58467, avg_loss=0.56724]\n",
            "Step 84914   [1.739 sec/step, loss=0.53309, avg_loss=0.56725]\n",
            "Step 84915   [1.746 sec/step, loss=0.53278, avg_loss=0.56642]\n",
            "Generated 16 batches of size 16 in 10.408 sec\n",
            "Step 84916   [1.789 sec/step, loss=0.41898, avg_loss=0.56507]\n",
            "Step 84917   [1.776 sec/step, loss=0.55333, avg_loss=0.56527]\n",
            "Step 84918   [1.762 sec/step, loss=0.54571, avg_loss=0.56495]\n",
            "Step 84919   [1.754 sec/step, loss=0.57637, avg_loss=0.56490]\n",
            "Step 84920   [1.754 sec/step, loss=0.59556, avg_loss=0.56501]\n",
            "Step 84921   [1.713 sec/step, loss=0.55688, avg_loss=0.56573]\n",
            "Step 84922   [1.715 sec/step, loss=0.62870, avg_loss=0.56641]\n",
            "Step 84923   [1.714 sec/step, loss=0.55833, avg_loss=0.56680]\n",
            "Step 84924   [1.714 sec/step, loss=0.59383, avg_loss=0.56679]\n",
            "Step 84925   [1.714 sec/step, loss=0.58530, avg_loss=0.56700]\n",
            "Step 84926   [1.675 sec/step, loss=0.58183, avg_loss=0.56865]\n",
            "Step 84927   [1.670 sec/step, loss=0.59146, avg_loss=0.56879]\n",
            "Step 84928   [1.674 sec/step, loss=0.59090, avg_loss=0.56888]\n",
            "Step 84929   [1.680 sec/step, loss=0.57822, avg_loss=0.56946]\n",
            "Step 84930   [1.675 sec/step, loss=0.57334, avg_loss=0.56991]\n",
            "Step 84931   [1.681 sec/step, loss=0.56482, avg_loss=0.57000]\n",
            "Step 84932   [1.691 sec/step, loss=0.57070, avg_loss=0.56984]\n",
            "Generated 16 batches of size 16 in 10.023 sec\n",
            "Step 84933   [1.681 sec/step, loss=0.58829, avg_loss=0.56996]\n",
            "Step 84934   [1.696 sec/step, loss=0.53246, avg_loss=0.56971]\n",
            "Step 84935   [1.692 sec/step, loss=0.56935, avg_loss=0.56928]\n",
            "Step 84936   [1.728 sec/step, loss=0.45753, avg_loss=0.56800]\n",
            "Step 84937   [1.716 sec/step, loss=0.58370, avg_loss=0.56845]\n",
            "Step 84938   [1.712 sec/step, loss=0.58142, avg_loss=0.56834]\n",
            "Step 84939   [1.713 sec/step, loss=0.56750, avg_loss=0.56858]\n",
            "Step 84940   [1.708 sec/step, loss=0.58806, avg_loss=0.56880]\n",
            "Step 84941   [1.707 sec/step, loss=0.57900, avg_loss=0.56918]\n",
            "Step 84942   [1.713 sec/step, loss=0.55538, avg_loss=0.56894]\n",
            "Step 84943   [1.715 sec/step, loss=0.55516, avg_loss=0.56858]\n",
            "Step 84944   [1.760 sec/step, loss=0.43896, avg_loss=0.56730]\n",
            "Step 84945   [1.782 sec/step, loss=0.51399, avg_loss=0.56681]\n",
            "Generated 16 batches of size 16 in 9.769 sec\n",
            "Step 84946   [1.781 sec/step, loss=0.56828, avg_loss=0.56692]\n",
            "Step 84947   [1.777 sec/step, loss=0.56043, avg_loss=0.56694]\n",
            "Step 84948   [1.769 sec/step, loss=0.56144, avg_loss=0.56685]\n",
            "Step 84949   [1.768 sec/step, loss=0.56389, avg_loss=0.56701]\n",
            "Step 84950   [1.760 sec/step, loss=0.60945, avg_loss=0.56740]\n",
            "Step 84951   [1.759 sec/step, loss=0.54340, avg_loss=0.56704]\n",
            "Step 84952   [1.761 sec/step, loss=0.55008, avg_loss=0.56657]\n",
            "Step 84953   [1.755 sec/step, loss=0.59357, avg_loss=0.56663]\n",
            "Step 84954   [1.722 sec/step, loss=0.53073, avg_loss=0.56709]\n",
            "Step 84955   [1.723 sec/step, loss=0.57569, avg_loss=0.56694]\n",
            "Step 84956   [1.762 sec/step, loss=0.45969, avg_loss=0.56592]\n",
            "Step 84957   [1.781 sec/step, loss=0.50400, avg_loss=0.56497]\n",
            "Step 84958   [1.776 sec/step, loss=0.60643, avg_loss=0.56511]\n",
            "Step 84959   [1.780 sec/step, loss=0.56809, avg_loss=0.56472]\n",
            "Step 84960   [1.780 sec/step, loss=0.54305, avg_loss=0.56486]\n",
            "Step 84961   [1.776 sec/step, loss=0.58937, avg_loss=0.56486]\n",
            "Step 84962   [1.778 sec/step, loss=0.58852, avg_loss=0.56500]\n",
            "Generated 16 batches of size 16 in 7.062 sec\n",
            "Step 84963   [1.790 sec/step, loss=0.52539, avg_loss=0.56480]\n",
            "Step 84964   [1.795 sec/step, loss=0.54583, avg_loss=0.56415]\n",
            "Step 84965   [1.785 sec/step, loss=0.61431, avg_loss=0.56452]\n",
            "Step 84966   [1.785 sec/step, loss=0.59897, avg_loss=0.56454]\n",
            "Step 84967   [1.782 sec/step, loss=0.62358, avg_loss=0.56477]\n",
            "Step 84968   [1.787 sec/step, loss=0.55069, avg_loss=0.56477]\n",
            "Step 84969   [1.735 sec/step, loss=0.56660, avg_loss=0.56545]\n",
            "Step 84970   [1.737 sec/step, loss=0.58349, avg_loss=0.56575]\n",
            "Step 84971   [1.746 sec/step, loss=0.52348, avg_loss=0.56534]\n",
            "Step 84972   [1.755 sec/step, loss=0.53485, avg_loss=0.56546]\n",
            "Step 84973   [1.750 sec/step, loss=0.61228, avg_loss=0.56578]\n",
            "Step 84974   [1.751 sec/step, loss=0.59471, avg_loss=0.56604]\n",
            "Step 84975   [1.762 sec/step, loss=0.57666, avg_loss=0.56583]\n",
            "Step 84976   [1.766 sec/step, loss=0.60505, avg_loss=0.56614]\n",
            "Step 84977   [1.772 sec/step, loss=0.56823, avg_loss=0.56616]\n",
            "Step 84978   [1.764 sec/step, loss=0.56959, avg_loss=0.56671]\n",
            "Step 84979   [1.762 sec/step, loss=0.58356, avg_loss=0.56690]\n",
            "Generated 16 batches of size 16 in 7.491 sec\n",
            "Step 84980   [1.763 sec/step, loss=0.57894, avg_loss=0.56679]\n",
            "Step 84981   [1.764 sec/step, loss=0.53971, avg_loss=0.56626]\n",
            "Step 84982   [1.766 sec/step, loss=0.58262, avg_loss=0.56612]\n",
            "Step 84983   [1.757 sec/step, loss=0.42379, avg_loss=0.56612]\n",
            "Step 84984   [1.747 sec/step, loss=0.58514, avg_loss=0.56664]\n",
            "Step 84985   [1.742 sec/step, loss=0.58958, avg_loss=0.56667]\n",
            "Step 84986   [1.744 sec/step, loss=0.54914, avg_loss=0.56622]\n",
            "Step 84987   [1.752 sec/step, loss=0.56816, avg_loss=0.56585]\n",
            "Step 84988   [1.751 sec/step, loss=0.57929, avg_loss=0.56575]\n",
            "Step 84989   [1.753 sec/step, loss=0.60753, avg_loss=0.56584]\n",
            "Step 84990   [1.753 sec/step, loss=0.57281, avg_loss=0.56566]\n",
            "Step 84991   [1.754 sec/step, loss=0.56379, avg_loss=0.56541]\n",
            "Step 84992   [1.762 sec/step, loss=0.60063, avg_loss=0.56553]\n",
            "Step 84993   [1.762 sec/step, loss=0.58776, avg_loss=0.56536]\n",
            "Step 84994   [1.761 sec/step, loss=0.58684, avg_loss=0.56537]\n",
            "Step 84995   [1.761 sec/step, loss=0.56374, avg_loss=0.56505]\n",
            "Generated 16 batches of size 16 in 7.427 sec\n",
            "Step 84996   [1.729 sec/step, loss=0.55102, avg_loss=0.56608]\n",
            "Step 84997   [1.722 sec/step, loss=0.58213, avg_loss=0.56612]\n",
            "Step 84998   [1.727 sec/step, loss=0.52980, avg_loss=0.56609]\n",
            "Step 84999   [1.711 sec/step, loss=0.57958, avg_loss=0.56672]\n",
            "Step 85000   [1.749 sec/step, loss=0.50761, avg_loss=0.56616]\n",
            "Writing summary at step: 85000\n",
            "Saving checkpoint to: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/model.ckpt-85000\n",
            "Saving audio and alignment...\n",
            "  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n",
            " [*] Plot saved: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/train-step-000085000-align000.png\n",
            "100% 1/1 [00:04<00:00,  4.17s/it]\n",
            "Test finished for step 85000.\n",
            "  0% 0/4 [00:00<?, ?it/s]Training korean : Use jamo\n",
            " [*] Plot saved: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/test-step-000085000-align000.png\n",
            " 25% 1/4 [00:04<00:13,  4.47s/it]Training korean : Use jamo\n",
            " [*] Plot saved: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/test-step-000085000-align001.png\n",
            " 50% 2/4 [00:09<00:09,  4.51s/it]Training korean : Use jamo\n",
            " [*] Plot saved: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/test-step-000085000-align002.png\n",
            " 75% 3/4 [00:13<00:04,  4.36s/it]Training korean : Use jamo\n",
            " [*] Plot saved: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/test-step-000085000-align003.png\n",
            "100% 4/4 [00:17<00:00,  4.35s/it]\n",
            "Test finished for step 85000.\n",
            "Step 85001   [1.752 sec/step, loss=0.56052, avg_loss=0.56578]\n",
            "Step 85002   [1.756 sec/step, loss=0.56092, avg_loss=0.56527]\n",
            "Step 85003   [1.758 sec/step, loss=0.54037, avg_loss=0.56470]\n",
            "Step 85004   [1.755 sec/step, loss=0.57206, avg_loss=0.56425]\n",
            "Step 85005   [1.765 sec/step, loss=0.54040, avg_loss=0.56363]\n",
            "Step 85006   [1.768 sec/step, loss=0.59387, avg_loss=0.56356]\n",
            "Step 85007   [1.773 sec/step, loss=0.60777, avg_loss=0.56405]\n",
            "Step 85008   [1.773 sec/step, loss=0.57264, avg_loss=0.56413]\n",
            "Step 85009   [1.771 sec/step, loss=0.57839, avg_loss=0.56439]\n",
            "Step 85010   [1.760 sec/step, loss=0.58581, avg_loss=0.56464]\n",
            "Generated 16 batches of size 16 in 8.740 sec\n",
            "Step 85011   [1.765 sec/step, loss=0.54506, avg_loss=0.56423]\n",
            "Step 85012   [1.767 sec/step, loss=0.54598, avg_loss=0.56352]\n",
            "Step 85013   [1.801 sec/step, loss=0.44399, avg_loss=0.56212]\n",
            "Step 85014   [1.795 sec/step, loss=0.59112, avg_loss=0.56270]\n",
            "Step 85015   [1.785 sec/step, loss=0.58130, avg_loss=0.56318]\n",
            "Step 85016   [1.740 sec/step, loss=0.60018, avg_loss=0.56499]\n",
            "Step 85017   [1.736 sec/step, loss=0.60444, avg_loss=0.56551]\n",
            "Step 85018   [1.743 sec/step, loss=0.49239, avg_loss=0.56497]\n",
            "Step 85019   [1.743 sec/step, loss=0.57698, avg_loss=0.56498]\n",
            "Step 85020   [1.784 sec/step, loss=0.46005, avg_loss=0.56362]\n",
            "Step 85021   [1.785 sec/step, loss=0.55098, avg_loss=0.56356]\n",
            "Step 85022   [1.793 sec/step, loss=0.57264, avg_loss=0.56300]\n",
            "Step 85023   [1.797 sec/step, loss=0.57056, avg_loss=0.56313]\n",
            "Step 85024   [1.799 sec/step, loss=0.54865, avg_loss=0.56267]\n",
            "Step 85025   [1.800 sec/step, loss=0.58139, avg_loss=0.56264]\n",
            "Step 85026   [1.804 sec/step, loss=0.56209, avg_loss=0.56244]\n",
            "Step 85027   [1.810 sec/step, loss=0.56848, avg_loss=0.56221]\n",
            "Generated 16 batches of size 16 in 10.136 sec\n",
            "Step 85028   [1.812 sec/step, loss=0.57883, avg_loss=0.56209]\n",
            "Step 85029   [1.807 sec/step, loss=0.53438, avg_loss=0.56165]\n",
            "Step 85030   [1.800 sec/step, loss=0.59023, avg_loss=0.56182]\n",
            "Step 85031   [1.796 sec/step, loss=0.60382, avg_loss=0.56221]\n",
            "Step 85032   [1.789 sec/step, loss=0.57146, avg_loss=0.56222]\n",
            "Step 85033   [1.792 sec/step, loss=0.60248, avg_loss=0.56236]\n",
            "Step 85034   [1.817 sec/step, loss=0.50315, avg_loss=0.56206]\n",
            "Step 85035   [1.816 sec/step, loss=0.58522, avg_loss=0.56222]\n",
            "Step 85036   [1.779 sec/step, loss=0.54316, avg_loss=0.56308]\n",
            "Step 85037   [1.792 sec/step, loss=0.57830, avg_loss=0.56303]\n",
            "Step 85038   [1.797 sec/step, loss=0.55757, avg_loss=0.56279]\n",
            "Step 85039   [1.792 sec/step, loss=0.55342, avg_loss=0.56265]\n",
            "Step 85040   [1.800 sec/step, loss=0.60025, avg_loss=0.56277]\n",
            "Step 85041   [1.808 sec/step, loss=0.56890, avg_loss=0.56267]\n",
            "Step 85042   [1.808 sec/step, loss=0.61343, avg_loss=0.56325]\n",
            "Step 85043   [1.810 sec/step, loss=0.58933, avg_loss=0.56359]\n",
            "Generated 16 batches of size 16 in 10.222 sec\n",
            "Step 85044   [1.766 sec/step, loss=0.60505, avg_loss=0.56525]\n",
            "Step 85045   [1.744 sec/step, loss=0.59939, avg_loss=0.56610]\n",
            "Step 85046   [1.754 sec/step, loss=0.55534, avg_loss=0.56597]\n",
            "Step 85047   [1.748 sec/step, loss=0.56861, avg_loss=0.56606]\n",
            "Step 85048   [1.757 sec/step, loss=0.57139, avg_loss=0.56616]\n",
            "Step 85049   [1.758 sec/step, loss=0.57547, avg_loss=0.56627]\n",
            "Step 85050   [1.758 sec/step, loss=0.56305, avg_loss=0.56581]\n",
            "Step 85051   [1.761 sec/step, loss=0.54454, avg_loss=0.56582]\n",
            "Step 85052   [1.762 sec/step, loss=0.58690, avg_loss=0.56619]\n",
            "Step 85053   [1.773 sec/step, loss=0.56223, avg_loss=0.56587]\n",
            "Step 85054   [1.777 sec/step, loss=0.60202, avg_loss=0.56659]\n",
            "Step 85055   [1.779 sec/step, loss=0.57783, avg_loss=0.56661]\n",
            "Step 85056   [1.741 sec/step, loss=0.57460, avg_loss=0.56776]\n",
            "Step 85057   [1.731 sec/step, loss=0.56568, avg_loss=0.56837]\n",
            "Step 85058   [1.735 sec/step, loss=0.56921, avg_loss=0.56800]\n",
            "Generated 16 batches of size 16 in 10.328 sec\n",
            "Step 85059   [1.751 sec/step, loss=0.54695, avg_loss=0.56779]\n",
            "Step 85060   [1.735 sec/step, loss=0.57143, avg_loss=0.56807]\n",
            "Step 85061   [1.730 sec/step, loss=0.56147, avg_loss=0.56780]\n",
            "Step 85062   [1.762 sec/step, loss=0.44496, avg_loss=0.56636]\n",
            "Step 85063   [1.748 sec/step, loss=0.56156, avg_loss=0.56672]\n",
            "Step 85064   [1.762 sec/step, loss=0.54068, avg_loss=0.56667]\n",
            "Step 85065   [1.760 sec/step, loss=0.57282, avg_loss=0.56625]\n",
            "Step 85066   [1.757 sec/step, loss=0.56122, avg_loss=0.56588]\n",
            "Step 85067   [1.797 sec/step, loss=0.42371, avg_loss=0.56388]\n",
            "Step 85068   [1.798 sec/step, loss=0.57052, avg_loss=0.56408]\n",
            "Step 85069   [1.797 sec/step, loss=0.58745, avg_loss=0.56429]\n",
            "Step 85070   [1.799 sec/step, loss=0.57916, avg_loss=0.56424]\n",
            "Step 85071   [1.798 sec/step, loss=0.59301, avg_loss=0.56494]\n",
            "Step 85072   [1.789 sec/step, loss=0.60335, avg_loss=0.56562]\n",
            "Step 85073   [1.804 sec/step, loss=0.54806, avg_loss=0.56498]\n",
            "Step 85074   [1.807 sec/step, loss=0.58791, avg_loss=0.56491]\n",
            "Generated 16 batches of size 16 in 10.129 sec\n",
            "Step 85075   [1.805 sec/step, loss=0.56824, avg_loss=0.56483]\n",
            "Step 85076   [1.813 sec/step, loss=0.54825, avg_loss=0.56426]\n",
            "Step 85077   [1.811 sec/step, loss=0.55315, avg_loss=0.56411]\n",
            "Step 85078   [1.804 sec/step, loss=0.56926, avg_loss=0.56411]\n",
            "Step 85079   [1.830 sec/step, loss=0.51586, avg_loss=0.56343]\n",
            "Step 85080   [1.828 sec/step, loss=0.58073, avg_loss=0.56345]\n",
            "Step 85081   [1.837 sec/step, loss=0.47752, avg_loss=0.56283]\n",
            "Step 85082   [1.838 sec/step, loss=0.60970, avg_loss=0.56310]\n",
            "Step 85083   [1.793 sec/step, loss=0.55440, avg_loss=0.56440]\n",
            "Step 85084   [1.804 sec/step, loss=0.51212, avg_loss=0.56367]\n",
            "Step 85085   [1.802 sec/step, loss=0.57897, avg_loss=0.56357]\n",
            "Step 85086   [1.799 sec/step, loss=0.60186, avg_loss=0.56409]\n",
            "Step 85087   [1.798 sec/step, loss=0.55161, avg_loss=0.56393]\n",
            "Step 85088   [1.802 sec/step, loss=0.60933, avg_loss=0.56423]\n",
            "Step 85089   [1.801 sec/step, loss=0.60790, avg_loss=0.56423]\n",
            "Step 85090   [1.810 sec/step, loss=0.54153, avg_loss=0.56392]\n",
            "Step 85091   [1.804 sec/step, loss=0.51678, avg_loss=0.56345]\n",
            "Generated 16 batches of size 16 in 10.323 sec\n",
            "Step 85092   [1.800 sec/step, loss=0.61105, avg_loss=0.56355]\n",
            "Step 85093   [1.804 sec/step, loss=0.56462, avg_loss=0.56332]\n",
            "Step 85094   [1.800 sec/step, loss=0.61183, avg_loss=0.56357]\n",
            "Step 85095   [1.802 sec/step, loss=0.50764, avg_loss=0.56301]\n",
            "Step 85096   [1.795 sec/step, loss=0.54198, avg_loss=0.56292]\n",
            "Step 85097   [1.796 sec/step, loss=0.59720, avg_loss=0.56307]\n",
            "Step 85098   [1.785 sec/step, loss=0.59676, avg_loss=0.56374]\n",
            "Step 85099   [1.782 sec/step, loss=0.60528, avg_loss=0.56400]\n",
            "Step 85100   [1.750 sec/step, loss=0.57318, avg_loss=0.56465]\n",
            "Writing summary at step: 85100\n",
            "Step 85101   [1.753 sec/step, loss=0.58431, avg_loss=0.56489]\n",
            "Step 85102   [1.747 sec/step, loss=0.61494, avg_loss=0.56543]\n",
            "Step 85103   [1.754 sec/step, loss=0.52121, avg_loss=0.56524]\n",
            "Step 85104   [1.796 sec/step, loss=0.44269, avg_loss=0.56395]\n",
            "Generated 16 batches of size 16 in 10.481 sec\n",
            "Step 85105   [1.783 sec/step, loss=0.52575, avg_loss=0.56380]\n",
            "Step 85106   [1.794 sec/step, loss=0.55896, avg_loss=0.56345]\n",
            "Step 85107   [1.789 sec/step, loss=0.57457, avg_loss=0.56312]\n",
            "Step 85108   [1.783 sec/step, loss=0.56412, avg_loss=0.56303]\n",
            "Step 85109   [1.785 sec/step, loss=0.61919, avg_loss=0.56344]\n",
            "Step 85110   [1.782 sec/step, loss=0.56142, avg_loss=0.56320]\n",
            "Step 85111   [1.781 sec/step, loss=0.52771, avg_loss=0.56302]\n",
            "Step 85112   [1.788 sec/step, loss=0.55447, avg_loss=0.56311]\n",
            "Step 85113   [1.757 sec/step, loss=0.56175, avg_loss=0.56429]\n",
            "Step 85114   [1.754 sec/step, loss=0.56471, avg_loss=0.56402]\n",
            "Step 85115   [1.759 sec/step, loss=0.58030, avg_loss=0.56401]\n",
            "Step 85116   [1.767 sec/step, loss=0.57190, avg_loss=0.56373]\n",
            "Step 85117   [1.821 sec/step, loss=0.46180, avg_loss=0.56230]\n",
            "Step 85118   [1.806 sec/step, loss=0.58040, avg_loss=0.56318]\n",
            "Step 85119   [1.808 sec/step, loss=0.58188, avg_loss=0.56323]\n",
            "Generated 16 batches of size 16 in 10.138 sec\n",
            "Step 85120   [1.786 sec/step, loss=0.50430, avg_loss=0.56367]\n",
            "Step 85121   [1.786 sec/step, loss=0.58231, avg_loss=0.56399]\n",
            "Step 85122   [1.778 sec/step, loss=0.56369, avg_loss=0.56390]\n",
            "Step 85123   [1.773 sec/step, loss=0.55545, avg_loss=0.56375]\n",
            "Step 85124   [1.771 sec/step, loss=0.62370, avg_loss=0.56450]\n",
            "Step 85125   [1.772 sec/step, loss=0.59043, avg_loss=0.56459]\n",
            "Step 85126   [1.765 sec/step, loss=0.59616, avg_loss=0.56493]\n",
            "Step 85127   [1.764 sec/step, loss=0.60711, avg_loss=0.56531]\n",
            "Step 85128   [1.767 sec/step, loss=0.53873, avg_loss=0.56491]\n",
            "Step 85129   [1.771 sec/step, loss=0.55259, avg_loss=0.56510]\n",
            "Step 85130   [1.799 sec/step, loss=0.47280, avg_loss=0.56392]\n",
            "Step 85131   [1.798 sec/step, loss=0.55446, avg_loss=0.56343]\n",
            "Step 85132   [1.800 sec/step, loss=0.58755, avg_loss=0.56359]\n",
            "Step 85133   [1.795 sec/step, loss=0.57435, avg_loss=0.56331]\n",
            "Step 85134   [1.754 sec/step, loss=0.58945, avg_loss=0.56417]\n",
            "Step 85135   [1.762 sec/step, loss=0.51580, avg_loss=0.56348]\n",
            "Step 85136   [1.766 sec/step, loss=0.61853, avg_loss=0.56423]\n",
            "Step 85137   [1.765 sec/step, loss=0.55135, avg_loss=0.56396]\n",
            "Step 85138   [1.765 sec/step, loss=0.56210, avg_loss=0.56401]\n",
            "Generated 16 batches of size 16 in 10.124 sec\n",
            "Step 85139   [1.769 sec/step, loss=0.61829, avg_loss=0.56465]\n",
            "Step 85140   [1.769 sec/step, loss=0.57666, avg_loss=0.56442]\n",
            "Step 85141   [1.758 sec/step, loss=0.55533, avg_loss=0.56428]\n",
            "Step 85142   [1.755 sec/step, loss=0.51537, avg_loss=0.56330]\n",
            "Step 85143   [1.753 sec/step, loss=0.57539, avg_loss=0.56316]\n",
            "Step 85144   [1.755 sec/step, loss=0.57889, avg_loss=0.56290]\n",
            "Step 85145   [1.772 sec/step, loss=0.48121, avg_loss=0.56172]\n",
            "Step 85146   [1.757 sec/step, loss=0.60414, avg_loss=0.56221]\n",
            "Step 85147   [1.757 sec/step, loss=0.58987, avg_loss=0.56242]\n",
            "Step 85148   [1.751 sec/step, loss=0.57418, avg_loss=0.56245]\n",
            "Step 85149   [1.758 sec/step, loss=0.51558, avg_loss=0.56185]\n",
            "Step 85150   [1.757 sec/step, loss=0.57971, avg_loss=0.56202]\n",
            "Step 85151   [1.763 sec/step, loss=0.56322, avg_loss=0.56220]\n",
            "Step 85152   [1.758 sec/step, loss=0.61654, avg_loss=0.56250]\n",
            "Generated 16 batches of size 16 in 7.683 sec\n",
            "Step 85153   [1.789 sec/step, loss=0.45742, avg_loss=0.56145]\n",
            "Step 85154   [1.795 sec/step, loss=0.56241, avg_loss=0.56105]\n",
            "Step 85155   [1.794 sec/step, loss=0.61458, avg_loss=0.56142]\n",
            "Step 85156   [1.791 sec/step, loss=0.58325, avg_loss=0.56151]\n",
            "Step 85157   [1.782 sec/step, loss=0.56062, avg_loss=0.56146]\n",
            "Step 85158   [1.775 sec/step, loss=0.61566, avg_loss=0.56192]\n",
            "Step 85159   [1.756 sec/step, loss=0.58459, avg_loss=0.56230]\n",
            "Step 85160   [1.755 sec/step, loss=0.60088, avg_loss=0.56259]\n",
            "Step 85161   [1.762 sec/step, loss=0.56008, avg_loss=0.56258]\n",
            "Step 85162   [1.729 sec/step, loss=0.59398, avg_loss=0.56407]\n",
            "Step 85163   [1.738 sec/step, loss=0.56738, avg_loss=0.56413]\n",
            "Step 85164   [1.721 sec/step, loss=0.57449, avg_loss=0.56447]\n",
            "Step 85165   [1.722 sec/step, loss=0.60207, avg_loss=0.56476]\n",
            "Step 85166   [1.724 sec/step, loss=0.55213, avg_loss=0.56467]\n",
            "Step 85167   [1.687 sec/step, loss=0.57964, avg_loss=0.56623]\n",
            "Step 85168   [1.703 sec/step, loss=0.51334, avg_loss=0.56565]\n",
            "Generated 16 batches of size 16 in 7.462 sec\n",
            "Step 85169   [1.706 sec/step, loss=0.56446, avg_loss=0.56543]\n",
            "Step 85170   [1.712 sec/step, loss=0.56948, avg_loss=0.56533]\n",
            "Step 85171   [1.706 sec/step, loss=0.59392, avg_loss=0.56534]\n",
            "Step 85172   [1.742 sec/step, loss=0.37498, avg_loss=0.56305]\n",
            "Step 85173   [1.733 sec/step, loss=0.57163, avg_loss=0.56329]\n",
            "Step 85174   [1.726 sec/step, loss=0.57798, avg_loss=0.56319]\n",
            "Step 85175   [1.719 sec/step, loss=0.58880, avg_loss=0.56340]\n",
            "Step 85176   [1.748 sec/step, loss=0.44361, avg_loss=0.56235]\n",
            "Step 85177   [1.758 sec/step, loss=0.49797, avg_loss=0.56180]\n",
            "Step 85178   [1.766 sec/step, loss=0.53837, avg_loss=0.56149]\n",
            "Step 85179   [1.735 sec/step, loss=0.59317, avg_loss=0.56226]\n",
            "Step 85180   [1.739 sec/step, loss=0.58427, avg_loss=0.56230]\n",
            "Step 85181   [1.725 sec/step, loss=0.61962, avg_loss=0.56372]\n",
            "Step 85182   [1.727 sec/step, loss=0.58926, avg_loss=0.56351]\n",
            "Step 85183   [1.730 sec/step, loss=0.55665, avg_loss=0.56354]\n",
            "Step 85184   [1.727 sec/step, loss=0.57318, avg_loss=0.56415]\n",
            "Step 85185   [1.731 sec/step, loss=0.56290, avg_loss=0.56399]\n",
            "Generated 16 batches of size 16 in 8.033 sec\n",
            "Step 85186   [1.738 sec/step, loss=0.57441, avg_loss=0.56371]\n",
            "Step 85187   [1.735 sec/step, loss=0.55449, avg_loss=0.56374]\n",
            "Step 85188   [1.729 sec/step, loss=0.60085, avg_loss=0.56366]\n",
            "Step 85189   [1.731 sec/step, loss=0.54299, avg_loss=0.56301]\n",
            "Step 85190   [1.759 sec/step, loss=0.41995, avg_loss=0.56179]\n",
            "Step 85191   [1.759 sec/step, loss=0.57640, avg_loss=0.56239]\n",
            "Step 85192   [1.761 sec/step, loss=0.55253, avg_loss=0.56180]\n",
            "Step 85193   [1.770 sec/step, loss=0.55166, avg_loss=0.56167]\n",
            "Step 85194   [1.770 sec/step, loss=0.59342, avg_loss=0.56149]\n",
            "Step 85195   [1.771 sec/step, loss=0.54851, avg_loss=0.56190]\n",
            "Step 85196   [1.768 sec/step, loss=0.57098, avg_loss=0.56219]\n",
            "Step 85197   [1.773 sec/step, loss=0.61459, avg_loss=0.56236]\n",
            "Step 85198   [1.782 sec/step, loss=0.56377, avg_loss=0.56203]\n",
            "Step 85199   [1.793 sec/step, loss=0.55932, avg_loss=0.56157]\n",
            "Step 85200   [1.791 sec/step, loss=0.57642, avg_loss=0.56160]\n",
            "Writing summary at step: 85200\n",
            "Generated 16 batches of size 16 in 8.746 sec\n",
            "Step 85201   [1.788 sec/step, loss=0.58337, avg_loss=0.56159]\n",
            "Step 85202   [1.786 sec/step, loss=0.57316, avg_loss=0.56118]\n",
            "Step 85203   [1.778 sec/step, loss=0.51923, avg_loss=0.56116]\n",
            "Step 85204   [1.735 sec/step, loss=0.54451, avg_loss=0.56217]\n",
            "Step 85205   [1.734 sec/step, loss=0.58116, avg_loss=0.56273]\n",
            "Step 85206   [1.724 sec/step, loss=0.60984, avg_loss=0.56324]\n",
            "Step 85207   [1.732 sec/step, loss=0.56335, avg_loss=0.56313]\n",
            "Step 85208   [1.729 sec/step, loss=0.59064, avg_loss=0.56339]\n",
            "Step 85209   [1.743 sec/step, loss=0.53484, avg_loss=0.56255]\n",
            "Step 85210   [1.748 sec/step, loss=0.58735, avg_loss=0.56281]\n",
            "Step 85211   [1.741 sec/step, loss=0.60237, avg_loss=0.56355]\n",
            "Step 85212   [1.732 sec/step, loss=0.58239, avg_loss=0.56383]\n",
            "Step 85213   [1.735 sec/step, loss=0.55116, avg_loss=0.56373]\n",
            "Step 85214   [1.744 sec/step, loss=0.51568, avg_loss=0.56324]\n",
            "Generated 16 batches of size 16 in 9.064 sec\n",
            "Step 85215   [1.791 sec/step, loss=0.48594, avg_loss=0.56229]\n",
            "Step 85216   [1.794 sec/step, loss=0.53445, avg_loss=0.56192]\n",
            "Step 85217   [1.739 sec/step, loss=0.58779, avg_loss=0.56318]\n",
            "Step 85218   [1.739 sec/step, loss=0.60010, avg_loss=0.56337]\n",
            "Step 85219   [1.737 sec/step, loss=0.60674, avg_loss=0.56362]\n",
            "Step 85220   [1.724 sec/step, loss=0.55313, avg_loss=0.56411]\n",
            "Step 85221   [1.725 sec/step, loss=0.58762, avg_loss=0.56416]\n",
            "Step 85222   [1.735 sec/step, loss=0.58161, avg_loss=0.56434]\n",
            "Step 85223   [1.728 sec/step, loss=0.55193, avg_loss=0.56431]\n",
            "Step 85224   [1.729 sec/step, loss=0.56562, avg_loss=0.56373]\n",
            "Step 85225   [1.769 sec/step, loss=0.44075, avg_loss=0.56223]\n",
            "Step 85226   [1.770 sec/step, loss=0.62362, avg_loss=0.56251]\n",
            "Step 85227   [1.767 sec/step, loss=0.57312, avg_loss=0.56217]\n",
            "Step 85228   [1.764 sec/step, loss=0.58564, avg_loss=0.56264]\n",
            "Step 85229   [1.754 sec/step, loss=0.58907, avg_loss=0.56300]\n",
            "Step 85230   [1.731 sec/step, loss=0.57541, avg_loss=0.56403]\n",
            "Step 85231   [1.755 sec/step, loss=0.49566, avg_loss=0.56344]\n",
            "Generated 16 batches of size 16 in 10.217 sec\n",
            "Step 85232   [1.760 sec/step, loss=0.58016, avg_loss=0.56336]\n",
            "Step 85233   [1.766 sec/step, loss=0.54679, avg_loss=0.56309]\n",
            "Step 85234   [1.765 sec/step, loss=0.58954, avg_loss=0.56309]\n",
            "Step 85235   [1.758 sec/step, loss=0.57776, avg_loss=0.56371]\n",
            "Step 85236   [1.753 sec/step, loss=0.59679, avg_loss=0.56349]\n",
            "Step 85237   [1.745 sec/step, loss=0.57517, avg_loss=0.56373]\n",
            "Step 85238   [1.786 sec/step, loss=0.48926, avg_loss=0.56300]\n",
            "Step 85239   [1.782 sec/step, loss=0.59565, avg_loss=0.56278]\n",
            "Step 85240   [1.777 sec/step, loss=0.61919, avg_loss=0.56320]\n",
            "Step 85241   [1.784 sec/step, loss=0.55977, avg_loss=0.56324]\n",
            "Step 85242   [1.779 sec/step, loss=0.56221, avg_loss=0.56371]\n",
            "Step 85243   [1.778 sec/step, loss=0.60628, avg_loss=0.56402]\n",
            "Step 85244   [1.783 sec/step, loss=0.54648, avg_loss=0.56370]\n",
            "Step 85245   [1.770 sec/step, loss=0.56109, avg_loss=0.56450]\n",
            "Step 85246   [1.776 sec/step, loss=0.55552, avg_loss=0.56401]\n",
            "Step 85247   [1.776 sec/step, loss=0.59393, avg_loss=0.56405]\n",
            "Step 85248   [1.788 sec/step, loss=0.60395, avg_loss=0.56435]\n",
            "Generated 16 batches of size 16 in 9.838 sec\n",
            "Step 85249   [1.803 sec/step, loss=0.53084, avg_loss=0.56450]\n",
            "Step 85250   [1.812 sec/step, loss=0.55784, avg_loss=0.56428]\n",
            "Step 85251   [1.806 sec/step, loss=0.58046, avg_loss=0.56446]\n",
            "Step 85252   [1.806 sec/step, loss=0.58951, avg_loss=0.56418]\n",
            "Step 85253   [1.772 sec/step, loss=0.54306, avg_loss=0.56504]\n",
            "Step 85254   [1.764 sec/step, loss=0.58303, avg_loss=0.56525]\n",
            "Step 85255   [1.765 sec/step, loss=0.58148, avg_loss=0.56492]\n",
            "Step 85256   [1.777 sec/step, loss=0.51468, avg_loss=0.56423]\n",
            "Step 85257   [1.778 sec/step, loss=0.55845, avg_loss=0.56421]\n",
            "Step 85258   [1.778 sec/step, loss=0.55033, avg_loss=0.56356]\n",
            "Step 85259   [1.779 sec/step, loss=0.61865, avg_loss=0.56390]\n",
            "Step 85260   [1.789 sec/step, loss=0.55637, avg_loss=0.56345]\n",
            "Step 85261   [1.790 sec/step, loss=0.59834, avg_loss=0.56383]\n",
            "Step 85262   [1.801 sec/step, loss=0.52999, avg_loss=0.56319]\n",
            "Step 85263   [1.799 sec/step, loss=0.55578, avg_loss=0.56308]\n",
            "Step 85264   [1.801 sec/step, loss=0.60473, avg_loss=0.56338]\n",
            "Generated 16 batches of size 16 in 10.114 sec\n",
            "Step 85265   [1.802 sec/step, loss=0.57618, avg_loss=0.56312]\n",
            "Step 85266   [1.808 sec/step, loss=0.60336, avg_loss=0.56363]\n",
            "Step 85267   [1.841 sec/step, loss=0.43873, avg_loss=0.56222]\n",
            "Step 85268   [1.820 sec/step, loss=0.57774, avg_loss=0.56287]\n",
            "Step 85269   [1.815 sec/step, loss=0.59322, avg_loss=0.56316]\n",
            "Step 85270   [1.844 sec/step, loss=0.44849, avg_loss=0.56195]\n",
            "Step 85271   [1.846 sec/step, loss=0.59052, avg_loss=0.56191]\n",
            "Step 85272   [1.810 sec/step, loss=0.56085, avg_loss=0.56377]\n",
            "Step 85273   [1.806 sec/step, loss=0.61343, avg_loss=0.56419]\n",
            "Step 85274   [1.822 sec/step, loss=0.55484, avg_loss=0.56396]\n",
            "Step 85275   [1.821 sec/step, loss=0.56160, avg_loss=0.56369]\n",
            "Step 85276   [1.785 sec/step, loss=0.59454, avg_loss=0.56520]\n",
            "Step 85277   [1.772 sec/step, loss=0.58217, avg_loss=0.56604]\n",
            "Step 85278   [1.768 sec/step, loss=0.56939, avg_loss=0.56635]\n",
            "Step 85279   [1.777 sec/step, loss=0.56295, avg_loss=0.56604]\n",
            "Step 85280   [1.772 sec/step, loss=0.59357, avg_loss=0.56614]\n",
            "Step 85281   [1.779 sec/step, loss=0.55533, avg_loss=0.56549]\n",
            "Generated 16 batches of size 16 in 9.924 sec\n",
            "Step 85282   [1.775 sec/step, loss=0.57867, avg_loss=0.56539]\n",
            "Step 85283   [1.784 sec/step, loss=0.54052, avg_loss=0.56523]\n",
            "Step 85284   [1.783 sec/step, loss=0.54717, avg_loss=0.56497]\n",
            "Step 85285   [1.778 sec/step, loss=0.56147, avg_loss=0.56495]\n",
            "Step 85286   [1.797 sec/step, loss=0.52296, avg_loss=0.56444]\n",
            "Step 85287   [1.797 sec/step, loss=0.60723, avg_loss=0.56497]\n",
            "Step 85288   [1.805 sec/step, loss=0.58168, avg_loss=0.56477]\n",
            "Step 85289   [1.803 sec/step, loss=0.59455, avg_loss=0.56529]\n",
            "Step 85290   [1.769 sec/step, loss=0.55488, avg_loss=0.56664]\n",
            "Step 85291   [1.765 sec/step, loss=0.57461, avg_loss=0.56662]\n",
            "Step 85292   [1.778 sec/step, loss=0.51648, avg_loss=0.56626]\n",
            "Step 85293   [1.773 sec/step, loss=0.56669, avg_loss=0.56641]\n",
            "Step 85294   [1.781 sec/step, loss=0.56061, avg_loss=0.56608]\n",
            "Step 85295   [1.781 sec/step, loss=0.57547, avg_loss=0.56635]\n",
            "Step 85296   [1.787 sec/step, loss=0.56811, avg_loss=0.56632]\n",
            "Generated 16 batches of size 16 in 10.320 sec\n",
            "Step 85297   [1.782 sec/step, loss=0.59213, avg_loss=0.56610]\n",
            "Step 85298   [1.771 sec/step, loss=0.54962, avg_loss=0.56596]\n",
            "Step 85299   [1.760 sec/step, loss=0.57075, avg_loss=0.56607]\n",
            "Step 85300   [1.756 sec/step, loss=0.56541, avg_loss=0.56596]\n",
            "Writing summary at step: 85300\n",
            "Step 85301   [1.755 sec/step, loss=0.56009, avg_loss=0.56573]\n",
            "Step 85302   [1.761 sec/step, loss=0.59612, avg_loss=0.56596]\n",
            "Step 85303   [1.768 sec/step, loss=0.52661, avg_loss=0.56603]\n",
            "Step 85304   [1.773 sec/step, loss=0.57551, avg_loss=0.56634]\n",
            "Step 85305   [1.817 sec/step, loss=0.43170, avg_loss=0.56485]\n",
            "Step 85306   [1.814 sec/step, loss=0.60215, avg_loss=0.56477]\n",
            "Step 85307   [1.813 sec/step, loss=0.56680, avg_loss=0.56481]\n",
            "Step 85308   [1.812 sec/step, loss=0.52613, avg_loss=0.56416]\n",
            "Step 85309   [1.803 sec/step, loss=0.53850, avg_loss=0.56420]\n",
            "Step 85310   [1.803 sec/step, loss=0.59361, avg_loss=0.56426]\n",
            "Step 85311   [1.802 sec/step, loss=0.58576, avg_loss=0.56409]\n",
            "Step 85312   [1.809 sec/step, loss=0.54935, avg_loss=0.56376]\n",
            "Generated 16 batches of size 16 in 10.226 sec\n",
            "Step 85313   [1.813 sec/step, loss=0.60152, avg_loss=0.56427]\n",
            "Step 85314   [1.804 sec/step, loss=0.56658, avg_loss=0.56478]\n",
            "Step 85315   [1.769 sec/step, loss=0.56916, avg_loss=0.56561]\n",
            "Step 85316   [1.758 sec/step, loss=0.57233, avg_loss=0.56599]\n",
            "Step 85317   [1.758 sec/step, loss=0.58207, avg_loss=0.56593]\n",
            "Step 85318   [1.760 sec/step, loss=0.60251, avg_loss=0.56595]\n",
            "Step 85319   [1.763 sec/step, loss=0.57083, avg_loss=0.56559]\n",
            "Step 85320   [1.757 sec/step, loss=0.60377, avg_loss=0.56610]\n",
            "Step 85321   [1.763 sec/step, loss=0.56001, avg_loss=0.56582]\n",
            "Step 85322   [1.754 sec/step, loss=0.60425, avg_loss=0.56605]\n",
            "Step 85323   [1.764 sec/step, loss=0.58778, avg_loss=0.56641]\n",
            "Step 85324   [1.768 sec/step, loss=0.54767, avg_loss=0.56623]\n",
            "Step 85325   [1.748 sec/step, loss=0.51916, avg_loss=0.56701]\n",
            "Step 85326   [1.755 sec/step, loss=0.58112, avg_loss=0.56659]\n",
            "Generated 16 batches of size 16 in 10.029 sec\n",
            "Step 85327   [1.765 sec/step, loss=0.55669, avg_loss=0.56643]\n",
            "Step 85328   [1.759 sec/step, loss=0.57158, avg_loss=0.56628]\n",
            "Step 85329   [1.761 sec/step, loss=0.60722, avg_loss=0.56647]\n",
            "Step 85330   [1.793 sec/step, loss=0.45495, avg_loss=0.56526]\n",
            "Step 85331   [1.767 sec/step, loss=0.59005, avg_loss=0.56621]\n",
            "Step 85332   [1.761 sec/step, loss=0.57715, avg_loss=0.56618]\n",
            "Step 85333   [1.765 sec/step, loss=0.56235, avg_loss=0.56633]\n",
            "Step 85334   [1.770 sec/step, loss=0.55997, avg_loss=0.56604]\n",
            "Step 85335   [1.780 sec/step, loss=0.56405, avg_loss=0.56590]\n",
            "Step 85336   [1.783 sec/step, loss=0.58818, avg_loss=0.56581]\n",
            "Step 85337   [1.782 sec/step, loss=0.58771, avg_loss=0.56594]\n",
            "Step 85338   [1.737 sec/step, loss=0.57170, avg_loss=0.56676]\n",
            "Step 85339   [1.746 sec/step, loss=0.58001, avg_loss=0.56661]\n",
            "Step 85340   [1.754 sec/step, loss=0.59231, avg_loss=0.56634]\n",
            "Step 85341   [1.760 sec/step, loss=0.54333, avg_loss=0.56617]\n",
            "Step 85342   [1.760 sec/step, loss=0.58122, avg_loss=0.56636]\n",
            "Step 85343   [1.761 sec/step, loss=0.57718, avg_loss=0.56607]\n",
            "Generated 16 batches of size 16 in 9.693 sec\n",
            "Step 85344   [1.750 sec/step, loss=0.57887, avg_loss=0.56640]\n",
            "Step 85345   [1.791 sec/step, loss=0.37463, avg_loss=0.56453]\n",
            "Step 85346   [1.789 sec/step, loss=0.56501, avg_loss=0.56463]\n",
            "Step 85347   [1.788 sec/step, loss=0.59724, avg_loss=0.56466]\n",
            "Step 85348   [1.774 sec/step, loss=0.58755, avg_loss=0.56449]\n",
            "Step 85349   [1.790 sec/step, loss=0.43010, avg_loss=0.56349]\n",
            "Step 85350   [1.788 sec/step, loss=0.58970, avg_loss=0.56381]\n",
            "Step 85351   [1.792 sec/step, loss=0.60500, avg_loss=0.56405]\n",
            "Step 85352   [1.798 sec/step, loss=0.53779, avg_loss=0.56353]\n",
            "Step 85353   [1.792 sec/step, loss=0.55941, avg_loss=0.56370]\n",
            "Step 85354   [1.791 sec/step, loss=0.59787, avg_loss=0.56385]\n",
            "Step 85355   [1.793 sec/step, loss=0.58307, avg_loss=0.56386]\n",
            "Step 85356   [1.784 sec/step, loss=0.52858, avg_loss=0.56400]\n",
            "Step 85357   [1.789 sec/step, loss=0.59192, avg_loss=0.56434]\n",
            "Step 85358   [1.794 sec/step, loss=0.57180, avg_loss=0.56455]\n",
            "Generated 16 batches of size 16 in 7.270 sec\n",
            "Step 85359   [1.801 sec/step, loss=0.57975, avg_loss=0.56416]\n",
            "Step 85360   [1.796 sec/step, loss=0.62188, avg_loss=0.56482]\n",
            "Step 85361   [1.793 sec/step, loss=0.56783, avg_loss=0.56451]\n",
            "Step 85362   [1.796 sec/step, loss=0.53723, avg_loss=0.56458]\n",
            "Step 85363   [1.788 sec/step, loss=0.59406, avg_loss=0.56497]\n",
            "Step 85364   [1.783 sec/step, loss=0.56596, avg_loss=0.56458]\n",
            "Step 85365   [1.784 sec/step, loss=0.57185, avg_loss=0.56454]\n",
            "Step 85366   [1.778 sec/step, loss=0.60108, avg_loss=0.56451]\n",
            "Step 85367   [1.741 sec/step, loss=0.56333, avg_loss=0.56576]\n",
            "Step 85368   [1.745 sec/step, loss=0.52985, avg_loss=0.56528]\n",
            "Step 85369   [1.747 sec/step, loss=0.61862, avg_loss=0.56553]\n",
            "Step 85370   [1.714 sec/step, loss=0.55475, avg_loss=0.56660]\n",
            "Step 85371   [1.713 sec/step, loss=0.56594, avg_loss=0.56635]\n",
            "Step 85372   [1.731 sec/step, loss=0.49540, avg_loss=0.56570]\n",
            "Generated 16 batches of size 16 in 6.770 sec\n",
            "Step 85373   [1.748 sec/step, loss=0.58652, avg_loss=0.56543]\n",
            "Step 85374   [1.739 sec/step, loss=0.57570, avg_loss=0.56564]\n",
            "Step 85375   [1.746 sec/step, loss=0.55561, avg_loss=0.56558]\n",
            "Step 85376   [1.741 sec/step, loss=0.54327, avg_loss=0.56506]\n",
            "Step 85377   [1.781 sec/step, loss=0.42935, avg_loss=0.56353]\n",
            "Step 85378   [1.777 sec/step, loss=0.59567, avg_loss=0.56380]\n",
            "Step 85379   [1.767 sec/step, loss=0.58791, avg_loss=0.56405]\n",
            "Step 85380   [1.768 sec/step, loss=0.60900, avg_loss=0.56420]\n",
            "Step 85381   [1.762 sec/step, loss=0.57632, avg_loss=0.56441]\n",
            "Step 85382   [1.801 sec/step, loss=0.39246, avg_loss=0.56255]\n",
            "Step 85383   [1.791 sec/step, loss=0.54853, avg_loss=0.56263]\n",
            "Step 85384   [1.793 sec/step, loss=0.56828, avg_loss=0.56284]\n",
            "Step 85385   [1.798 sec/step, loss=0.57786, avg_loss=0.56300]\n",
            "Step 85386   [1.773 sec/step, loss=0.54992, avg_loss=0.56327]\n",
            "Step 85387   [1.770 sec/step, loss=0.59441, avg_loss=0.56315]\n",
            "Step 85388   [1.764 sec/step, loss=0.56208, avg_loss=0.56295]\n",
            "Step 85389   [1.781 sec/step, loss=0.55225, avg_loss=0.56253]\n",
            "Step 85390   [1.783 sec/step, loss=0.58866, avg_loss=0.56286]\n",
            "Generated 16 batches of size 16 in 7.714 sec\n",
            "Step 85391   [1.789 sec/step, loss=0.57674, avg_loss=0.56289]\n",
            "Step 85392   [1.770 sec/step, loss=0.57751, avg_loss=0.56350]\n",
            "Step 85393   [1.757 sec/step, loss=0.58642, avg_loss=0.56369]\n",
            "Step 85394   [1.757 sec/step, loss=0.61240, avg_loss=0.56421]\n",
            "Step 85395   [1.756 sec/step, loss=0.53430, avg_loss=0.56380]\n",
            "Step 85396   [1.754 sec/step, loss=0.60143, avg_loss=0.56413]\n",
            "Step 85397   [1.753 sec/step, loss=0.59551, avg_loss=0.56417]\n",
            "Step 85398   [1.751 sec/step, loss=0.58803, avg_loss=0.56455]\n",
            "Step 85399   [1.755 sec/step, loss=0.53449, avg_loss=0.56419]\n",
            "Step 85400   [1.760 sec/step, loss=0.56695, avg_loss=0.56420]\n",
            "Writing summary at step: 85400\n",
            "Step 85401   [1.768 sec/step, loss=0.53102, avg_loss=0.56391]\n",
            "Step 85402   [1.763 sec/step, loss=0.59730, avg_loss=0.56392]\n",
            "Step 85403   [1.765 sec/step, loss=0.56177, avg_loss=0.56428]\n",
            "Step 85404   [1.765 sec/step, loss=0.59340, avg_loss=0.56446]\n",
            "Step 85405   [1.725 sec/step, loss=0.60937, avg_loss=0.56623]\n",
            "Generated 16 batches of size 16 in 8.626 sec\n",
            "Step 85406   [1.758 sec/step, loss=0.54094, avg_loss=0.56562]\n",
            "Step 85407   [1.752 sec/step, loss=0.55823, avg_loss=0.56553]\n",
            "Step 85408   [1.754 sec/step, loss=0.60712, avg_loss=0.56634]\n",
            "Step 85409   [1.745 sec/step, loss=0.56991, avg_loss=0.56666]\n",
            "Step 85410   [1.744 sec/step, loss=0.56400, avg_loss=0.56636]\n",
            "Step 85411   [1.748 sec/step, loss=0.56066, avg_loss=0.56611]\n",
            "Step 85412   [1.744 sec/step, loss=0.57130, avg_loss=0.56633]\n",
            "Step 85413   [1.732 sec/step, loss=0.59638, avg_loss=0.56628]\n",
            "Step 85414   [1.732 sec/step, loss=0.60362, avg_loss=0.56665]\n",
            "Step 85415   [1.722 sec/step, loss=0.56375, avg_loss=0.56660]\n",
            "Step 85416   [1.729 sec/step, loss=0.53449, avg_loss=0.56622]\n",
            "Step 85417   [1.729 sec/step, loss=0.60849, avg_loss=0.56648]\n",
            "Step 85418   [1.731 sec/step, loss=0.53686, avg_loss=0.56582]\n",
            "Step 85419   [1.741 sec/step, loss=0.55205, avg_loss=0.56564]\n",
            "Step 85420   [1.761 sec/step, loss=0.50886, avg_loss=0.56469]\n",
            "Step 85421   [1.753 sec/step, loss=0.57331, avg_loss=0.56482]\n",
            "Generated 16 batches of size 16 in 8.643 sec\n",
            "Step 85422   [1.789 sec/step, loss=0.43220, avg_loss=0.56310]\n",
            "Step 85423   [1.783 sec/step, loss=0.57850, avg_loss=0.56301]\n",
            "Step 85424   [1.777 sec/step, loss=0.55910, avg_loss=0.56312]\n",
            "Step 85425   [1.760 sec/step, loss=0.62086, avg_loss=0.56414]\n",
            "Step 85426   [1.761 sec/step, loss=0.54845, avg_loss=0.56381]\n",
            "Step 85427   [1.759 sec/step, loss=0.53623, avg_loss=0.56361]\n",
            "Step 85428   [1.761 sec/step, loss=0.59616, avg_loss=0.56385]\n",
            "Step 85429   [1.760 sec/step, loss=0.59348, avg_loss=0.56372]\n",
            "Step 85430   [1.724 sec/step, loss=0.56747, avg_loss=0.56484]\n",
            "Step 85431   [1.726 sec/step, loss=0.55970, avg_loss=0.56454]\n",
            "Step 85432   [1.728 sec/step, loss=0.57993, avg_loss=0.56457]\n",
            "Step 85433   [1.715 sec/step, loss=0.59349, avg_loss=0.56488]\n",
            "Step 85434   [1.716 sec/step, loss=0.57177, avg_loss=0.56500]\n",
            "Step 85435   [1.759 sec/step, loss=0.44039, avg_loss=0.56376]\n",
            "Generated 16 batches of size 16 in 10.027 sec\n",
            "Step 85436   [1.767 sec/step, loss=0.56270, avg_loss=0.56350]\n",
            "Step 85437   [1.767 sec/step, loss=0.56705, avg_loss=0.56330]\n",
            "Step 85438   [1.764 sec/step, loss=0.54624, avg_loss=0.56304]\n",
            "Step 85439   [1.757 sec/step, loss=0.59835, avg_loss=0.56323]\n",
            "Step 85440   [1.762 sec/step, loss=0.55668, avg_loss=0.56287]\n",
            "Step 85441   [1.751 sec/step, loss=0.60378, avg_loss=0.56347]\n",
            "Step 85442   [1.749 sec/step, loss=0.56395, avg_loss=0.56330]\n",
            "Step 85443   [1.790 sec/step, loss=0.42494, avg_loss=0.56178]\n",
            "Step 85444   [1.790 sec/step, loss=0.55563, avg_loss=0.56155]\n",
            "Step 85445   [1.750 sec/step, loss=0.57226, avg_loss=0.56352]\n",
            "Step 85446   [1.744 sec/step, loss=0.57888, avg_loss=0.56366]\n",
            "Step 85447   [1.753 sec/step, loss=0.57793, avg_loss=0.56347]\n",
            "Step 85448   [1.770 sec/step, loss=0.53513, avg_loss=0.56294]\n",
            "Step 85449   [1.744 sec/step, loss=0.56505, avg_loss=0.56429]\n",
            "Step 85450   [1.736 sec/step, loss=0.61085, avg_loss=0.56450]\n",
            "Step 85451   [1.740 sec/step, loss=0.58987, avg_loss=0.56435]\n",
            "Step 85452   [1.733 sec/step, loss=0.58018, avg_loss=0.56478]\n",
            "Step 85453   [1.738 sec/step, loss=0.55463, avg_loss=0.56473]\n",
            "Step 85454   [1.747 sec/step, loss=0.54859, avg_loss=0.56424]\n",
            "Step 85455   [1.747 sec/step, loss=0.57712, avg_loss=0.56418]\n",
            "Generated 16 batches of size 16 in 10.046 sec\n",
            "Step 85456   [1.748 sec/step, loss=0.56794, avg_loss=0.56457]\n",
            "Step 85457   [1.744 sec/step, loss=0.58165, avg_loss=0.56447]\n",
            "Step 85458   [1.739 sec/step, loss=0.59135, avg_loss=0.56466]\n",
            "Step 85459   [1.732 sec/step, loss=0.58073, avg_loss=0.56467]\n",
            "Step 85460   [1.733 sec/step, loss=0.57037, avg_loss=0.56416]\n",
            "Step 85461   [1.730 sec/step, loss=0.58367, avg_loss=0.56432]\n",
            "Step 85462   [1.713 sec/step, loss=0.61087, avg_loss=0.56505]\n",
            "Step 85463   [1.716 sec/step, loss=0.59058, avg_loss=0.56502]\n",
            "Step 85464   [1.718 sec/step, loss=0.57010, avg_loss=0.56506]\n",
            "Step 85465   [1.719 sec/step, loss=0.55159, avg_loss=0.56486]\n",
            "Step 85466   [1.721 sec/step, loss=0.55243, avg_loss=0.56437]\n",
            "Step 85467   [1.735 sec/step, loss=0.56281, avg_loss=0.56437]\n",
            "Step 85468   [1.733 sec/step, loss=0.60285, avg_loss=0.56510]\n",
            "Step 85469   [1.741 sec/step, loss=0.56536, avg_loss=0.56456]\n",
            "Step 85470   [1.738 sec/step, loss=0.59313, avg_loss=0.56495]\n",
            "Generated 16 batches of size 16 in 10.437 sec\n",
            "Step 85471   [1.755 sec/step, loss=0.51644, avg_loss=0.56445]\n",
            "Step 85472   [1.766 sec/step, loss=0.50660, avg_loss=0.56456]\n",
            "Step 85473   [1.749 sec/step, loss=0.59115, avg_loss=0.56461]\n",
            "Step 85474   [1.750 sec/step, loss=0.56070, avg_loss=0.56446]\n",
            "Step 85475   [1.746 sec/step, loss=0.60507, avg_loss=0.56495]\n",
            "Step 85476   [1.745 sec/step, loss=0.57138, avg_loss=0.56524]\n",
            "Step 85477   [1.715 sec/step, loss=0.58052, avg_loss=0.56675]\n",
            "Step 85478   [1.712 sec/step, loss=0.60762, avg_loss=0.56687]\n",
            "Step 85479   [1.717 sec/step, loss=0.55213, avg_loss=0.56651]\n",
            "Step 85480   [1.755 sec/step, loss=0.47885, avg_loss=0.56521]\n",
            "Step 85481   [1.754 sec/step, loss=0.61961, avg_loss=0.56564]\n",
            "Step 85482   [1.721 sec/step, loss=0.50690, avg_loss=0.56679]\n",
            "Step 85483   [1.744 sec/step, loss=0.52410, avg_loss=0.56654]\n",
            "Step 85484   [1.740 sec/step, loss=0.59919, avg_loss=0.56685]\n",
            "Step 85485   [1.738 sec/step, loss=0.58148, avg_loss=0.56689]\n",
            "Step 85486   [1.735 sec/step, loss=0.55115, avg_loss=0.56690]\n",
            "Generated 16 batches of size 16 in 10.425 sec\n",
            "Step 85487   [1.737 sec/step, loss=0.62280, avg_loss=0.56718]\n",
            "Step 85488   [1.741 sec/step, loss=0.58823, avg_loss=0.56744]\n",
            "Step 85489   [1.721 sec/step, loss=0.58771, avg_loss=0.56780]\n",
            "Step 85490   [1.725 sec/step, loss=0.56610, avg_loss=0.56757]\n",
            "Step 85491   [1.720 sec/step, loss=0.59173, avg_loss=0.56772]\n",
            "Step 85492   [1.730 sec/step, loss=0.59111, avg_loss=0.56786]\n",
            "Step 85493   [1.735 sec/step, loss=0.61066, avg_loss=0.56810]\n",
            "Step 85494   [1.728 sec/step, loss=0.55464, avg_loss=0.56752]\n",
            "Step 85495   [1.723 sec/step, loss=0.57798, avg_loss=0.56796]\n",
            "Step 85496   [1.727 sec/step, loss=0.51444, avg_loss=0.56709]\n",
            "Step 85497   [1.726 sec/step, loss=0.58950, avg_loss=0.56703]\n",
            "Step 85498   [1.730 sec/step, loss=0.59373, avg_loss=0.56709]\n",
            "Step 85499   [1.730 sec/step, loss=0.55959, avg_loss=0.56734]\n",
            "Step 85500   [1.748 sec/step, loss=0.50462, avg_loss=0.56671]\n",
            "Writing summary at step: 85500\n",
            "Step 85501   [1.742 sec/step, loss=0.56256, avg_loss=0.56703]\n",
            "Generated 16 batches of size 16 in 10.112 sec\n",
            "Step 85502   [1.748 sec/step, loss=0.55952, avg_loss=0.56665]\n",
            "Step 85503   [1.774 sec/step, loss=0.51397, avg_loss=0.56617]\n",
            "Step 85504   [1.771 sec/step, loss=0.56955, avg_loss=0.56594]\n",
            "Step 85505   [1.773 sec/step, loss=0.54790, avg_loss=0.56532]\n",
            "Step 85506   [1.739 sec/step, loss=0.59158, avg_loss=0.56583]\n",
            "Step 85507   [1.740 sec/step, loss=0.60987, avg_loss=0.56634]\n",
            "Step 85508   [1.745 sec/step, loss=0.54597, avg_loss=0.56573]\n",
            "Step 85509   [1.787 sec/step, loss=0.42630, avg_loss=0.56430]\n",
            "Step 85510   [1.789 sec/step, loss=0.54017, avg_loss=0.56406]\n",
            "Step 85511   [1.792 sec/step, loss=0.51613, avg_loss=0.56361]\n",
            "Step 85512   [1.788 sec/step, loss=0.57259, avg_loss=0.56363]\n",
            "Step 85513   [1.796 sec/step, loss=0.55704, avg_loss=0.56323]\n",
            "Step 85514   [1.802 sec/step, loss=0.59661, avg_loss=0.56316]\n",
            "Step 85515   [1.803 sec/step, loss=0.55126, avg_loss=0.56304]\n",
            "Step 85516   [1.797 sec/step, loss=0.53500, avg_loss=0.56304]\n",
            "Step 85517   [1.808 sec/step, loss=0.56393, avg_loss=0.56260]\n",
            "Generated 16 batches of size 16 in 10.710 sec\n",
            "Step 85518   [1.821 sec/step, loss=0.56134, avg_loss=0.56284]\n",
            "Step 85519   [1.809 sec/step, loss=0.58946, avg_loss=0.56322]\n",
            "Step 85520   [1.788 sec/step, loss=0.60285, avg_loss=0.56416]\n",
            "Step 85521   [1.786 sec/step, loss=0.58447, avg_loss=0.56427]\n",
            "Step 85522   [1.750 sec/step, loss=0.59369, avg_loss=0.56588]\n",
            "Step 85523   [1.746 sec/step, loss=0.59335, avg_loss=0.56603]\n",
            "Step 85524   [1.747 sec/step, loss=0.60689, avg_loss=0.56651]\n",
            "Step 85525   [1.748 sec/step, loss=0.52342, avg_loss=0.56553]\n",
            "Step 85526   [1.738 sec/step, loss=0.56922, avg_loss=0.56574]\n",
            "Step 85527   [1.736 sec/step, loss=0.56250, avg_loss=0.56600]\n",
            "Step 85528   [1.735 sec/step, loss=0.60494, avg_loss=0.56609]\n",
            "Step 85529   [1.747 sec/step, loss=0.57825, avg_loss=0.56594]\n",
            "Step 85530   [1.795 sec/step, loss=0.43271, avg_loss=0.56459]\n",
            "Generated 16 batches of size 16 in 10.127 sec\n",
            "Step 85531   [1.796 sec/step, loss=0.58919, avg_loss=0.56489]\n",
            "Step 85532   [1.799 sec/step, loss=0.60241, avg_loss=0.56511]\n",
            "Step 85533   [1.804 sec/step, loss=0.58806, avg_loss=0.56506]\n",
            "Step 85534   [1.815 sec/step, loss=0.51730, avg_loss=0.56451]\n",
            "Step 85535   [1.764 sec/step, loss=0.59361, avg_loss=0.56605]\n",
            "Step 85536   [1.756 sec/step, loss=0.53216, avg_loss=0.56574]\n",
            "Step 85537   [1.762 sec/step, loss=0.55250, avg_loss=0.56559]\n",
            "Step 85538   [1.778 sec/step, loss=0.52093, avg_loss=0.56534]\n",
            "Step 85539   [1.783 sec/step, loss=0.55247, avg_loss=0.56488]\n",
            "Step 85540   [1.770 sec/step, loss=0.58524, avg_loss=0.56517]\n",
            "Step 85541   [1.768 sec/step, loss=0.58516, avg_loss=0.56498]\n",
            "Step 85542   [1.769 sec/step, loss=0.59703, avg_loss=0.56531]\n",
            "Step 85543   [1.727 sec/step, loss=0.55755, avg_loss=0.56664]\n",
            "Step 85544   [1.728 sec/step, loss=0.54573, avg_loss=0.56654]\n",
            "Step 85545   [1.775 sec/step, loss=0.44897, avg_loss=0.56531]\n",
            "Step 85546   [1.780 sec/step, loss=0.58502, avg_loss=0.56537]\n",
            "Generated 16 batches of size 16 in 8.003 sec\n",
            "Step 85547   [1.780 sec/step, loss=0.59156, avg_loss=0.56551]\n",
            "Step 85548   [1.768 sec/step, loss=0.53742, avg_loss=0.56553]\n",
            "Step 85549   [1.765 sec/step, loss=0.54234, avg_loss=0.56530]\n",
            "Step 85550   [1.762 sec/step, loss=0.59653, avg_loss=0.56516]\n",
            "Step 85551   [1.756 sec/step, loss=0.53684, avg_loss=0.56463]\n",
            "Step 85552   [1.755 sec/step, loss=0.60100, avg_loss=0.56484]\n",
            "Step 85553   [1.749 sec/step, loss=0.59890, avg_loss=0.56528]\n",
            "Step 85554   [1.756 sec/step, loss=0.53193, avg_loss=0.56511]\n",
            "Step 85555   [1.755 sec/step, loss=0.62283, avg_loss=0.56557]\n",
            "Step 85556   [1.749 sec/step, loss=0.53660, avg_loss=0.56526]\n",
            "Step 85557   [1.756 sec/step, loss=0.56976, avg_loss=0.56514]\n",
            "Step 85558   [1.758 sec/step, loss=0.61823, avg_loss=0.56541]\n",
            "Step 85559   [1.760 sec/step, loss=0.56359, avg_loss=0.56523]\n",
            "Step 85560   [1.758 sec/step, loss=0.59819, avg_loss=0.56551]\n",
            "Step 85561   [1.758 sec/step, loss=0.59764, avg_loss=0.56565]\n",
            "Step 85562   [1.759 sec/step, loss=0.55001, avg_loss=0.56504]\n",
            "Step 85563   [1.760 sec/step, loss=0.55119, avg_loss=0.56465]\n",
            "Step 85564   [1.777 sec/step, loss=0.56892, avg_loss=0.56464]\n",
            "Generated 16 batches of size 16 in 7.332 sec\n",
            "Step 85565   [1.778 sec/step, loss=0.58234, avg_loss=0.56495]\n",
            "Step 85566   [1.817 sec/step, loss=0.47013, avg_loss=0.56412]\n",
            "Step 85567   [1.800 sec/step, loss=0.54355, avg_loss=0.56393]\n",
            "Step 85568   [1.801 sec/step, loss=0.59509, avg_loss=0.56385]\n",
            "Step 85569   [1.800 sec/step, loss=0.48514, avg_loss=0.56305]\n",
            "Step 85570   [1.814 sec/step, loss=0.52220, avg_loss=0.56234]\n",
            "Step 85571   [1.801 sec/step, loss=0.56616, avg_loss=0.56284]\n",
            "Step 85572   [1.778 sec/step, loss=0.51703, avg_loss=0.56294]\n",
            "Step 85573   [1.776 sec/step, loss=0.59831, avg_loss=0.56301]\n",
            "Step 85574   [1.769 sec/step, loss=0.56220, avg_loss=0.56303]\n",
            "Step 85575   [1.772 sec/step, loss=0.57307, avg_loss=0.56271]\n",
            "Step 85576   [1.776 sec/step, loss=0.56898, avg_loss=0.56268]\n",
            "Step 85577   [1.773 sec/step, loss=0.58616, avg_loss=0.56274]\n",
            "Step 85578   [1.778 sec/step, loss=0.59032, avg_loss=0.56257]\n",
            "Step 85579   [1.779 sec/step, loss=0.58543, avg_loss=0.56290]\n",
            "Step 85580   [1.741 sec/step, loss=0.58543, avg_loss=0.56397]\n",
            "Step 85581   [1.743 sec/step, loss=0.56593, avg_loss=0.56343]\n",
            "Generated 16 batches of size 16 in 8.031 sec\n",
            "Step 85582   [1.743 sec/step, loss=0.56340, avg_loss=0.56399]\n",
            "Step 85583   [1.720 sec/step, loss=0.62579, avg_loss=0.56501]\n",
            "Step 85584   [1.759 sec/step, loss=0.40178, avg_loss=0.56304]\n",
            "Step 85585   [1.758 sec/step, loss=0.58058, avg_loss=0.56303]\n",
            "Step 85586   [1.758 sec/step, loss=0.56952, avg_loss=0.56321]\n",
            "Step 85587   [1.760 sec/step, loss=0.57723, avg_loss=0.56276]\n",
            "Step 85588   [1.761 sec/step, loss=0.54907, avg_loss=0.56237]\n",
            "Step 85589   [1.770 sec/step, loss=0.51878, avg_loss=0.56168]\n",
            "Step 85590   [1.771 sec/step, loss=0.59264, avg_loss=0.56194]\n",
            "Step 85591   [1.772 sec/step, loss=0.56946, avg_loss=0.56172]\n",
            "Step 85592   [1.778 sec/step, loss=0.50140, avg_loss=0.56082]\n",
            "Step 85593   [1.825 sec/step, loss=0.45442, avg_loss=0.55926]\n",
            "Step 85594   [1.831 sec/step, loss=0.55665, avg_loss=0.55928]\n",
            "Generated 16 batches of size 16 in 8.356 sec\n",
            "Step 85595   [1.834 sec/step, loss=0.58628, avg_loss=0.55936]\n",
            "Step 85596   [1.828 sec/step, loss=0.58314, avg_loss=0.56005]\n",
            "Step 85597   [1.834 sec/step, loss=0.51832, avg_loss=0.55934]\n",
            "Step 85598   [1.829 sec/step, loss=0.61539, avg_loss=0.55955]\n",
            "Step 85599   [1.822 sec/step, loss=0.57797, avg_loss=0.55974]\n",
            "Step 85600   [1.799 sec/step, loss=0.61013, avg_loss=0.56079]\n",
            "Writing summary at step: 85600\n",
            "Step 85601   [1.801 sec/step, loss=0.52014, avg_loss=0.56037]\n",
            "Step 85602   [1.837 sec/step, loss=0.43391, avg_loss=0.55911]\n",
            "Step 85603   [1.799 sec/step, loss=0.59685, avg_loss=0.55994]\n",
            "Step 85604   [1.799 sec/step, loss=0.56110, avg_loss=0.55986]\n",
            "Step 85605   [1.803 sec/step, loss=0.58301, avg_loss=0.56021]\n",
            "Step 85606   [1.802 sec/step, loss=0.55080, avg_loss=0.55980]\n",
            "Step 85607   [1.805 sec/step, loss=0.60173, avg_loss=0.55972]\n",
            "Step 85608   [1.811 sec/step, loss=0.48974, avg_loss=0.55916]\n",
            "Step 85609   [1.782 sec/step, loss=0.58639, avg_loss=0.56076]\n",
            "Step 85610   [1.782 sec/step, loss=0.60367, avg_loss=0.56139]\n",
            "Step 85611   [1.785 sec/step, loss=0.54963, avg_loss=0.56173]\n",
            "Generated 16 batches of size 16 in 9.226 sec\n",
            "Step 85612   [1.801 sec/step, loss=0.55484, avg_loss=0.56155]\n",
            "Step 85613   [1.796 sec/step, loss=0.61990, avg_loss=0.56218]\n",
            "Step 85614   [1.790 sec/step, loss=0.59427, avg_loss=0.56216]\n",
            "Step 85615   [1.785 sec/step, loss=0.58148, avg_loss=0.56246]\n",
            "Step 85616   [1.786 sec/step, loss=0.62072, avg_loss=0.56331]\n",
            "Step 85617   [1.791 sec/step, loss=0.53069, avg_loss=0.56298]\n",
            "Step 85618   [1.814 sec/step, loss=0.48609, avg_loss=0.56223]\n",
            "Step 85619   [1.814 sec/step, loss=0.62129, avg_loss=0.56255]\n",
            "Step 85620   [1.824 sec/step, loss=0.53537, avg_loss=0.56187]\n",
            "Step 85621   [1.823 sec/step, loss=0.61521, avg_loss=0.56218]\n",
            "Step 85622   [1.820 sec/step, loss=0.53169, avg_loss=0.56156]\n",
            "Step 85623   [1.821 sec/step, loss=0.59570, avg_loss=0.56158]\n",
            "Step 85624   [1.825 sec/step, loss=0.55902, avg_loss=0.56111]\n",
            "Step 85625   [1.837 sec/step, loss=0.54679, avg_loss=0.56134]\n",
            "Step 85626   [1.846 sec/step, loss=0.61326, avg_loss=0.56178]\n",
            "Step 85627   [1.840 sec/step, loss=0.59283, avg_loss=0.56208]\n",
            "Generated 16 batches of size 16 in 8.724 sec\n",
            "Step 85628   [1.846 sec/step, loss=0.56953, avg_loss=0.56173]\n",
            "Step 85629   [1.828 sec/step, loss=0.57662, avg_loss=0.56171]\n",
            "Step 85630   [1.775 sec/step, loss=0.58764, avg_loss=0.56326]\n",
            "Step 85631   [1.781 sec/step, loss=0.55297, avg_loss=0.56290]\n",
            "Step 85632   [1.779 sec/step, loss=0.58062, avg_loss=0.56268]\n",
            "Step 85633   [1.775 sec/step, loss=0.54334, avg_loss=0.56223]\n",
            "Step 85634   [1.765 sec/step, loss=0.55180, avg_loss=0.56258]\n",
            "Step 85635   [1.761 sec/step, loss=0.59155, avg_loss=0.56256]\n",
            "Step 85636   [1.762 sec/step, loss=0.56779, avg_loss=0.56292]\n",
            "Step 85637   [1.757 sec/step, loss=0.60370, avg_loss=0.56343]\n",
            "Step 85638   [1.742 sec/step, loss=0.57434, avg_loss=0.56396]\n",
            "Step 85639   [1.742 sec/step, loss=0.59700, avg_loss=0.56441]\n",
            "Step 85640   [1.757 sec/step, loss=0.53617, avg_loss=0.56392]\n",
            "Step 85641   [1.764 sec/step, loss=0.58525, avg_loss=0.56392]\n",
            "Step 85642   [1.766 sec/step, loss=0.58553, avg_loss=0.56380]\n",
            "Step 85643   [1.770 sec/step, loss=0.56219, avg_loss=0.56385]\n",
            "Step 85644   [1.767 sec/step, loss=0.58139, avg_loss=0.56420]\n",
            "Generated 16 batches of size 16 in 9.843 sec\n",
            "Step 85645   [1.762 sec/step, loss=0.43764, avg_loss=0.56409]\n",
            "Step 85646   [1.761 sec/step, loss=0.60311, avg_loss=0.56427]\n",
            "Step 85647   [1.770 sec/step, loss=0.53374, avg_loss=0.56369]\n",
            "Step 85648   [1.770 sec/step, loss=0.56030, avg_loss=0.56392]\n",
            "Step 85649   [1.762 sec/step, loss=0.59227, avg_loss=0.56442]\n",
            "Step 85650   [1.762 sec/step, loss=0.58978, avg_loss=0.56435]\n",
            "Step 85651   [1.761 sec/step, loss=0.53459, avg_loss=0.56433]\n",
            "Step 85652   [1.761 sec/step, loss=0.58806, avg_loss=0.56420]\n",
            "Step 85653   [1.771 sec/step, loss=0.59156, avg_loss=0.56413]\n",
            "Step 85654   [1.762 sec/step, loss=0.55462, avg_loss=0.56436]\n",
            "Step 85655   [1.758 sec/step, loss=0.58821, avg_loss=0.56401]\n",
            "Step 85656   [1.766 sec/step, loss=0.58390, avg_loss=0.56448]\n",
            "Step 85657   [1.770 sec/step, loss=0.54613, avg_loss=0.56425]\n",
            "Step 85658   [1.787 sec/step, loss=0.51420, avg_loss=0.56321]\n",
            "Step 85659   [1.786 sec/step, loss=0.56160, avg_loss=0.56319]\n",
            "Step 85660   [1.786 sec/step, loss=0.59346, avg_loss=0.56314]\n",
            "Generated 16 batches of size 16 in 10.435 sec\n",
            "Step 85661   [1.821 sec/step, loss=0.43617, avg_loss=0.56152]\n",
            "Step 85662   [1.819 sec/step, loss=0.57175, avg_loss=0.56174]\n",
            "Step 85663   [1.816 sec/step, loss=0.59658, avg_loss=0.56220]\n",
            "Step 85664   [1.802 sec/step, loss=0.59590, avg_loss=0.56247]\n",
            "Step 85665   [1.800 sec/step, loss=0.59616, avg_loss=0.56260]\n",
            "Step 85666   [1.760 sec/step, loss=0.58605, avg_loss=0.56376]\n",
            "Step 85667   [1.760 sec/step, loss=0.57958, avg_loss=0.56412]\n",
            "Step 85668   [1.759 sec/step, loss=0.54870, avg_loss=0.56366]\n",
            "Step 85669   [1.753 sec/step, loss=0.60710, avg_loss=0.56488]\n",
            "Step 85670   [1.738 sec/step, loss=0.57943, avg_loss=0.56545]\n",
            "Step 85671   [1.732 sec/step, loss=0.61486, avg_loss=0.56594]\n",
            "Step 85672   [1.729 sec/step, loss=0.57026, avg_loss=0.56647]\n",
            "Step 85673   [1.740 sec/step, loss=0.54783, avg_loss=0.56597]\n",
            "Step 85674   [1.748 sec/step, loss=0.58332, avg_loss=0.56618]\n",
            "Step 85675   [1.762 sec/step, loss=0.53603, avg_loss=0.56581]\n",
            "Generated 16 batches of size 16 in 9.727 sec\n",
            "Step 85676   [1.805 sec/step, loss=0.45665, avg_loss=0.56468]\n",
            "Step 85677   [1.803 sec/step, loss=0.59559, avg_loss=0.56478]\n",
            "Step 85678   [1.799 sec/step, loss=0.60537, avg_loss=0.56493]\n",
            "Step 85679   [1.800 sec/step, loss=0.58110, avg_loss=0.56488]\n",
            "Step 85680   [1.799 sec/step, loss=0.55866, avg_loss=0.56462]\n",
            "Step 85681   [1.797 sec/step, loss=0.58888, avg_loss=0.56485]\n",
            "Step 85682   [1.800 sec/step, loss=0.56981, avg_loss=0.56491]\n",
            "Step 85683   [1.798 sec/step, loss=0.57373, avg_loss=0.56439]\n",
            "Step 85684   [1.762 sec/step, loss=0.59501, avg_loss=0.56632]\n",
            "Step 85685   [1.769 sec/step, loss=0.56694, avg_loss=0.56619]\n",
            "Step 85686   [1.770 sec/step, loss=0.55555, avg_loss=0.56605]\n",
            "Step 85687   [1.767 sec/step, loss=0.58186, avg_loss=0.56609]\n",
            "Step 85688   [1.764 sec/step, loss=0.55069, avg_loss=0.56611]\n",
            "Step 85689   [1.787 sec/step, loss=0.49485, avg_loss=0.56587]\n",
            "Step 85690   [1.779 sec/step, loss=0.59091, avg_loss=0.56585]\n",
            "Step 85691   [1.779 sec/step, loss=0.56080, avg_loss=0.56577]\n",
            "Step 85692   [1.765 sec/step, loss=0.56028, avg_loss=0.56635]\n",
            "Generated 16 batches of size 16 in 9.918 sec\n",
            "Step 85693   [1.718 sec/step, loss=0.57512, avg_loss=0.56756]\n",
            "Step 85694   [1.716 sec/step, loss=0.54192, avg_loss=0.56741]\n",
            "Step 85695   [1.717 sec/step, loss=0.60070, avg_loss=0.56756]\n",
            "Step 85696   [1.719 sec/step, loss=0.52556, avg_loss=0.56698]\n",
            "Step 85697   [1.717 sec/step, loss=0.58903, avg_loss=0.56769]\n",
            "Step 85698   [1.719 sec/step, loss=0.58613, avg_loss=0.56740]\n",
            "Step 85699   [1.721 sec/step, loss=0.57558, avg_loss=0.56737]\n",
            "Step 85700   [1.724 sec/step, loss=0.59267, avg_loss=0.56720]\n",
            "Writing summary at step: 85700\n",
            "Step 85701   [1.720 sec/step, loss=0.57675, avg_loss=0.56776]\n",
            "Step 85702   [1.689 sec/step, loss=0.55047, avg_loss=0.56893]\n",
            "Step 85703   [1.696 sec/step, loss=0.57363, avg_loss=0.56870]\n",
            "Step 85704   [1.696 sec/step, loss=0.54073, avg_loss=0.56849]\n",
            "Step 85705   [1.698 sec/step, loss=0.55542, avg_loss=0.56822]\n",
            "Step 85706   [1.701 sec/step, loss=0.57910, avg_loss=0.56850]\n",
            "Step 85707   [1.710 sec/step, loss=0.52909, avg_loss=0.56777]\n",
            "Step 85708   [1.695 sec/step, loss=0.57115, avg_loss=0.56859]\n",
            "Generated 16 batches of size 16 in 10.230 sec\n",
            "Step 85709   [1.686 sec/step, loss=0.59180, avg_loss=0.56864]\n",
            "Step 85710   [1.686 sec/step, loss=0.60453, avg_loss=0.56865]\n",
            "Step 85711   [1.679 sec/step, loss=0.58084, avg_loss=0.56896]\n",
            "Step 85712   [1.660 sec/step, loss=0.54360, avg_loss=0.56885]\n",
            "Step 85713   [1.701 sec/step, loss=0.41774, avg_loss=0.56683]\n",
            "Step 85714   [1.700 sec/step, loss=0.59370, avg_loss=0.56682]\n",
            "Step 85715   [1.707 sec/step, loss=0.57441, avg_loss=0.56675]\n",
            "Step 85716   [1.716 sec/step, loss=0.56189, avg_loss=0.56617]\n",
            "Step 85717   [1.700 sec/step, loss=0.57738, avg_loss=0.56663]\n",
            "Step 85718   [1.666 sec/step, loss=0.52784, avg_loss=0.56705]\n",
            "Step 85719   [1.668 sec/step, loss=0.60897, avg_loss=0.56693]\n",
            "Step 85720   [1.663 sec/step, loss=0.61880, avg_loss=0.56776]\n",
            "Step 85721   [1.667 sec/step, loss=0.58082, avg_loss=0.56742]\n",
            "Step 85722   [1.674 sec/step, loss=0.58141, avg_loss=0.56791]\n",
            "Step 85723   [1.678 sec/step, loss=0.56828, avg_loss=0.56764]\n",
            "Step 85724   [1.672 sec/step, loss=0.60128, avg_loss=0.56806]\n",
            "Generated 16 batches of size 16 in 10.014 sec\n",
            "Step 85725   [1.665 sec/step, loss=0.53569, avg_loss=0.56795]\n",
            "Step 85726   [1.661 sec/step, loss=0.56313, avg_loss=0.56745]\n",
            "Step 85727   [1.675 sec/step, loss=0.51923, avg_loss=0.56671]\n",
            "Step 85728   [1.681 sec/step, loss=0.54361, avg_loss=0.56645]\n",
            "Step 85729   [1.687 sec/step, loss=0.58802, avg_loss=0.56657]\n",
            "Step 85730   [1.694 sec/step, loss=0.53420, avg_loss=0.56603]\n",
            "Step 85731   [1.688 sec/step, loss=0.58403, avg_loss=0.56634]\n",
            "Step 85732   [1.682 sec/step, loss=0.60887, avg_loss=0.56663]\n",
            "Step 85733   [1.679 sec/step, loss=0.56488, avg_loss=0.56684]\n",
            "Step 85734   [1.672 sec/step, loss=0.57832, avg_loss=0.56711]\n",
            "Step 85735   [1.718 sec/step, loss=0.49147, avg_loss=0.56611]\n",
            "Step 85736   [1.724 sec/step, loss=0.53130, avg_loss=0.56574]\n",
            "Step 85737   [1.726 sec/step, loss=0.61898, avg_loss=0.56590]\n",
            "Generated 16 batches of size 16 in 10.101 sec\n",
            "Step 85738   [1.729 sec/step, loss=0.56102, avg_loss=0.56576]\n",
            "Step 85739   [1.728 sec/step, loss=0.58374, avg_loss=0.56563]\n",
            "Step 85740   [1.715 sec/step, loss=0.49438, avg_loss=0.56521]\n",
            "Step 85741   [1.712 sec/step, loss=0.54643, avg_loss=0.56482]\n",
            "Step 85742   [1.712 sec/step, loss=0.58120, avg_loss=0.56478]\n",
            "Step 85743   [1.707 sec/step, loss=0.59782, avg_loss=0.56514]\n",
            "Step 85744   [1.718 sec/step, loss=0.57033, avg_loss=0.56503]\n",
            "Step 85745   [1.682 sec/step, loss=0.57537, avg_loss=0.56640]\n",
            "Step 85746   [1.678 sec/step, loss=0.57932, avg_loss=0.56617]\n",
            "Step 85747   [1.677 sec/step, loss=0.54710, avg_loss=0.56630]\n",
            "Step 85748   [1.672 sec/step, loss=0.55887, avg_loss=0.56628]\n",
            "Step 85749   [1.672 sec/step, loss=0.57505, avg_loss=0.56611]\n",
            "Step 85750   [1.675 sec/step, loss=0.59985, avg_loss=0.56621]\n",
            "Step 85751   [1.674 sec/step, loss=0.57803, avg_loss=0.56665]\n",
            "Step 85752   [1.677 sec/step, loss=0.57736, avg_loss=0.56654]\n",
            "Step 85753   [1.676 sec/step, loss=0.55791, avg_loss=0.56620]\n",
            "Step 85754   [1.680 sec/step, loss=0.57877, avg_loss=0.56645]\n",
            "Generated 16 batches of size 16 in 6.763 sec\n",
            "Step 85755   [1.688 sec/step, loss=0.54574, avg_loss=0.56602]\n",
            "Step 85756   [1.679 sec/step, loss=0.58352, avg_loss=0.56602]\n",
            "Step 85757   [1.667 sec/step, loss=0.57106, avg_loss=0.56627]\n",
            "Step 85758   [1.655 sec/step, loss=0.56770, avg_loss=0.56680]\n",
            "Step 85759   [1.691 sec/step, loss=0.45402, avg_loss=0.56573]\n",
            "Step 85760   [1.687 sec/step, loss=0.56999, avg_loss=0.56549]\n",
            "Step 85761   [1.651 sec/step, loss=0.57088, avg_loss=0.56684]\n",
            "Step 85762   [1.655 sec/step, loss=0.57000, avg_loss=0.56682]\n",
            "Step 85763   [1.654 sec/step, loss=0.57213, avg_loss=0.56658]\n",
            "Step 85764   [1.657 sec/step, loss=0.59589, avg_loss=0.56658]\n",
            "Step 85765   [1.663 sec/step, loss=0.53067, avg_loss=0.56592]\n",
            "Step 85766   [1.664 sec/step, loss=0.57379, avg_loss=0.56580]\n",
            "Step 85767   [1.674 sec/step, loss=0.57179, avg_loss=0.56572]\n",
            "Step 85768   [1.691 sec/step, loss=0.55636, avg_loss=0.56580]\n",
            "Step 85769   [1.693 sec/step, loss=0.54425, avg_loss=0.56517]\n",
            "Generated 16 batches of size 16 in 8.118 sec\n",
            "Step 85770   [1.701 sec/step, loss=0.53732, avg_loss=0.56475]\n",
            "Step 85771   [1.704 sec/step, loss=0.63172, avg_loss=0.56492]\n",
            "Step 85772   [1.733 sec/step, loss=0.41679, avg_loss=0.56338]\n",
            "Step 85773   [1.725 sec/step, loss=0.59284, avg_loss=0.56383]\n",
            "Step 85774   [1.724 sec/step, loss=0.55531, avg_loss=0.56355]\n",
            "Step 85775   [1.705 sec/step, loss=0.55455, avg_loss=0.56374]\n",
            "Step 85776   [1.666 sec/step, loss=0.58464, avg_loss=0.56502]\n",
            "Step 85777   [1.659 sec/step, loss=0.54605, avg_loss=0.56452]\n",
            "Step 85778   [1.663 sec/step, loss=0.52983, avg_loss=0.56377]\n",
            "Step 85779   [1.654 sec/step, loss=0.58305, avg_loss=0.56378]\n",
            "Step 85780   [1.653 sec/step, loss=0.62362, avg_loss=0.56443]\n",
            "Step 85781   [1.650 sec/step, loss=0.54865, avg_loss=0.56403]\n",
            "Step 85782   [1.649 sec/step, loss=0.53870, avg_loss=0.56372]\n",
            "Step 85783   [1.654 sec/step, loss=0.56246, avg_loss=0.56361]\n",
            "Step 85784   [1.649 sec/step, loss=0.58020, avg_loss=0.56346]\n",
            "Step 85785   [1.647 sec/step, loss=0.62047, avg_loss=0.56400]\n",
            "Generated 16 batches of size 16 in 7.740 sec\n",
            "Step 85786   [1.697 sec/step, loss=0.42757, avg_loss=0.56272]\n",
            "Step 85787   [1.697 sec/step, loss=0.60723, avg_loss=0.56297]\n",
            "Step 85788   [1.711 sec/step, loss=0.52782, avg_loss=0.56274]\n",
            "Step 85789   [1.685 sec/step, loss=0.56117, avg_loss=0.56340]\n",
            "Step 85790   [1.684 sec/step, loss=0.58812, avg_loss=0.56338]\n",
            "Step 85791   [1.692 sec/step, loss=0.55274, avg_loss=0.56330]\n",
            "Step 85792   [1.733 sec/step, loss=0.43740, avg_loss=0.56207]\n",
            "Step 85793   [1.729 sec/step, loss=0.60013, avg_loss=0.56232]\n",
            "Step 85794   [1.733 sec/step, loss=0.51225, avg_loss=0.56202]\n",
            "Step 85795   [1.738 sec/step, loss=0.54732, avg_loss=0.56149]\n",
            "Step 85796   [1.740 sec/step, loss=0.60215, avg_loss=0.56225]\n",
            "Step 85797   [1.739 sec/step, loss=0.59652, avg_loss=0.56233]\n",
            "Step 85798   [1.739 sec/step, loss=0.56989, avg_loss=0.56216]\n",
            "Step 85799   [1.746 sec/step, loss=0.54016, avg_loss=0.56181]\n",
            "Step 85800   [1.748 sec/step, loss=0.50277, avg_loss=0.56091]\n",
            "Writing summary at step: 85800\n",
            "Step 85801   [1.754 sec/step, loss=0.58195, avg_loss=0.56096]\n",
            "Generated 16 batches of size 16 in 8.120 sec\n",
            "Step 85802   [1.751 sec/step, loss=0.52038, avg_loss=0.56066]\n",
            "Step 85803   [1.743 sec/step, loss=0.60463, avg_loss=0.56097]\n",
            "Step 85804   [1.742 sec/step, loss=0.57713, avg_loss=0.56134]\n",
            "Step 85805   [1.745 sec/step, loss=0.58103, avg_loss=0.56159]\n",
            "Step 85806   [1.742 sec/step, loss=0.58535, avg_loss=0.56166]\n",
            "Step 85807   [1.737 sec/step, loss=0.53356, avg_loss=0.56170]\n",
            "Step 85808   [1.737 sec/step, loss=0.58482, avg_loss=0.56184]\n",
            "Step 85809   [1.739 sec/step, loss=0.53265, avg_loss=0.56125]\n",
            "Step 85810   [1.739 sec/step, loss=0.58671, avg_loss=0.56107]\n",
            "Step 85811   [1.740 sec/step, loss=0.57874, avg_loss=0.56105]\n",
            "Step 85812   [1.738 sec/step, loss=0.56009, avg_loss=0.56121]\n",
            "Step 85813   [1.696 sec/step, loss=0.60643, avg_loss=0.56310]\n",
            "Step 85814   [1.712 sec/step, loss=0.53307, avg_loss=0.56249]\n",
            "Step 85815   [1.727 sec/step, loss=0.54340, avg_loss=0.56218]\n",
            "Generated 16 batches of size 16 in 8.415 sec\n",
            "Step 85816   [1.759 sec/step, loss=0.44115, avg_loss=0.56097]\n",
            "Step 85817   [1.760 sec/step, loss=0.59707, avg_loss=0.56117]\n",
            "Step 85818   [1.752 sec/step, loss=0.58523, avg_loss=0.56174]\n",
            "Step 85819   [1.748 sec/step, loss=0.54622, avg_loss=0.56112]\n",
            "Step 85820   [1.751 sec/step, loss=0.49483, avg_loss=0.55988]\n",
            "Step 85821   [1.749 sec/step, loss=0.62360, avg_loss=0.56031]\n",
            "Step 85822   [1.744 sec/step, loss=0.58746, avg_loss=0.56037]\n",
            "Step 85823   [1.738 sec/step, loss=0.59450, avg_loss=0.56063]\n",
            "Step 85824   [1.738 sec/step, loss=0.60088, avg_loss=0.56062]\n",
            "Step 85825   [1.731 sec/step, loss=0.58660, avg_loss=0.56113]\n",
            "Step 85826   [1.733 sec/step, loss=0.56947, avg_loss=0.56120]\n",
            "Step 85827   [1.715 sec/step, loss=0.56699, avg_loss=0.56167]\n",
            "Step 85828   [1.714 sec/step, loss=0.56760, avg_loss=0.56191]\n",
            "Step 85829   [1.709 sec/step, loss=0.61515, avg_loss=0.56219]\n",
            "Step 85830   [1.707 sec/step, loss=0.57240, avg_loss=0.56257]\n",
            "Step 85831   [1.718 sec/step, loss=0.54110, avg_loss=0.56214]\n",
            "Step 85832   [1.723 sec/step, loss=0.58207, avg_loss=0.56187]\n",
            "Step 85833   [1.738 sec/step, loss=0.54327, avg_loss=0.56165]\n",
            "Step 85834   [1.742 sec/step, loss=0.60593, avg_loss=0.56193]\n",
            "Generated 16 batches of size 16 in 9.925 sec\n",
            "Step 85835   [1.707 sec/step, loss=0.58353, avg_loss=0.56285]\n",
            "Step 85836   [1.700 sec/step, loss=0.58048, avg_loss=0.56334]\n",
            "Step 85837   [1.701 sec/step, loss=0.54589, avg_loss=0.56261]\n",
            "Step 85838   [1.731 sec/step, loss=0.44266, avg_loss=0.56143]\n",
            "Step 85839   [1.725 sec/step, loss=0.60175, avg_loss=0.56161]\n",
            "Step 85840   [1.723 sec/step, loss=0.55791, avg_loss=0.56224]\n",
            "Step 85841   [1.727 sec/step, loss=0.56225, avg_loss=0.56240]\n",
            "Step 85842   [1.724 sec/step, loss=0.57588, avg_loss=0.56235]\n",
            "Step 85843   [1.721 sec/step, loss=0.58690, avg_loss=0.56224]\n",
            "Step 85844   [1.715 sec/step, loss=0.56575, avg_loss=0.56219]\n",
            "Step 85845   [1.707 sec/step, loss=0.59149, avg_loss=0.56235]\n",
            "Step 85846   [1.713 sec/step, loss=0.58445, avg_loss=0.56241]\n",
            "Step 85847   [1.704 sec/step, loss=0.60707, avg_loss=0.56301]\n",
            "Step 85848   [1.726 sec/step, loss=0.50887, avg_loss=0.56251]\n",
            "Step 85849   [1.729 sec/step, loss=0.60209, avg_loss=0.56278]\n",
            "Generated 16 batches of size 16 in 10.513 sec\n",
            "Step 85850   [1.741 sec/step, loss=0.57890, avg_loss=0.56257]\n",
            "Step 85851   [1.747 sec/step, loss=0.58546, avg_loss=0.56264]\n",
            "Step 85852   [1.781 sec/step, loss=0.50677, avg_loss=0.56193]\n",
            "Step 85853   [1.776 sec/step, loss=0.56230, avg_loss=0.56198]\n",
            "Step 85854   [1.768 sec/step, loss=0.61568, avg_loss=0.56235]\n",
            "Step 85855   [1.766 sec/step, loss=0.56623, avg_loss=0.56255]\n",
            "Step 85856   [1.773 sec/step, loss=0.53352, avg_loss=0.56205]\n",
            "Step 85857   [1.778 sec/step, loss=0.57014, avg_loss=0.56204]\n",
            "Step 85858   [1.773 sec/step, loss=0.60146, avg_loss=0.56238]\n",
            "Step 85859   [1.735 sec/step, loss=0.55616, avg_loss=0.56340]\n",
            "Step 85860   [1.779 sec/step, loss=0.48001, avg_loss=0.56250]\n",
            "Step 85861   [1.779 sec/step, loss=0.60115, avg_loss=0.56281]\n",
            "Step 85862   [1.779 sec/step, loss=0.58428, avg_loss=0.56295]\n",
            "Step 85863   [1.787 sec/step, loss=0.57810, avg_loss=0.56301]\n",
            "Step 85864   [1.790 sec/step, loss=0.58598, avg_loss=0.56291]\n",
            "Step 85865   [1.780 sec/step, loss=0.57741, avg_loss=0.56338]\n",
            "Step 85866   [1.783 sec/step, loss=0.59040, avg_loss=0.56354]\n",
            "Step 85867   [1.774 sec/step, loss=0.58475, avg_loss=0.56367]\n",
            "Generated 16 batches of size 16 in 10.527 sec\n",
            "Step 85868   [1.775 sec/step, loss=0.52463, avg_loss=0.56335]\n",
            "Step 85869   [1.783 sec/step, loss=0.48959, avg_loss=0.56281]\n",
            "Step 85870   [1.775 sec/step, loss=0.54352, avg_loss=0.56287]\n",
            "Step 85871   [1.777 sec/step, loss=0.57323, avg_loss=0.56228]\n",
            "Step 85872   [1.740 sec/step, loss=0.57991, avg_loss=0.56392]\n",
            "Step 85873   [1.739 sec/step, loss=0.58361, avg_loss=0.56382]\n",
            "Step 85874   [1.734 sec/step, loss=0.56975, avg_loss=0.56397]\n",
            "Step 85875   [1.742 sec/step, loss=0.57975, avg_loss=0.56422]\n",
            "Step 85876   [1.738 sec/step, loss=0.58212, avg_loss=0.56419]\n",
            "Step 85877   [1.738 sec/step, loss=0.58406, avg_loss=0.56458]\n",
            "Step 85878   [1.741 sec/step, loss=0.53593, avg_loss=0.56464]\n",
            "Step 85879   [1.750 sec/step, loss=0.59195, avg_loss=0.56473]\n",
            "Step 85880   [1.754 sec/step, loss=0.54002, avg_loss=0.56389]\n",
            "Step 85881   [1.768 sec/step, loss=0.56283, avg_loss=0.56403]\n",
            "Step 85882   [1.758 sec/step, loss=0.56385, avg_loss=0.56428]\n",
            "Step 85883   [1.755 sec/step, loss=0.59608, avg_loss=0.56462]\n",
            "Generated 16 batches of size 16 in 10.359 sec\n",
            "Step 85884   [1.798 sec/step, loss=0.44729, avg_loss=0.56329]\n",
            "Step 85885   [1.809 sec/step, loss=0.53449, avg_loss=0.56243]\n",
            "Step 85886   [1.767 sec/step, loss=0.54178, avg_loss=0.56357]\n",
            "Step 85887   [1.766 sec/step, loss=0.59220, avg_loss=0.56342]\n",
            "Step 85888   [1.751 sec/step, loss=0.58703, avg_loss=0.56401]\n",
            "Step 85889   [1.752 sec/step, loss=0.58934, avg_loss=0.56430]\n",
            "Step 85890   [1.754 sec/step, loss=0.58959, avg_loss=0.56431]\n",
            "Step 85891   [1.747 sec/step, loss=0.57131, avg_loss=0.56450]\n",
            "Step 85892   [1.747 sec/step, loss=0.43265, avg_loss=0.56445]\n",
            "Step 85893   [1.748 sec/step, loss=0.51959, avg_loss=0.56364]\n",
            "Step 85894   [1.754 sec/step, loss=0.56988, avg_loss=0.56422]\n",
            "Step 85895   [1.758 sec/step, loss=0.58095, avg_loss=0.56456]\n",
            "Step 85896   [1.758 sec/step, loss=0.56558, avg_loss=0.56419]\n",
            "Step 85897   [1.776 sec/step, loss=0.55131, avg_loss=0.56374]\n",
            "Generated 16 batches of size 16 in 10.426 sec\n",
            "Step 85898   [1.776 sec/step, loss=0.56056, avg_loss=0.56364]\n",
            "Step 85899   [1.770 sec/step, loss=0.58222, avg_loss=0.56407]\n",
            "Step 85900   [1.764 sec/step, loss=0.59430, avg_loss=0.56498]\n",
            "Writing summary at step: 85900\n",
            "Step 85901   [1.765 sec/step, loss=0.59177, avg_loss=0.56508]\n",
            "Step 85902   [1.754 sec/step, loss=0.56221, avg_loss=0.56550]\n",
            "Step 85903   [1.754 sec/step, loss=0.58907, avg_loss=0.56534]\n",
            "Step 85904   [1.772 sec/step, loss=0.53836, avg_loss=0.56495]\n",
            "Step 85905   [1.758 sec/step, loss=0.59191, avg_loss=0.56506]\n",
            "Step 85906   [1.766 sec/step, loss=0.55584, avg_loss=0.56477]\n",
            "Step 85907   [1.759 sec/step, loss=0.61787, avg_loss=0.56561]\n",
            "Step 85908   [1.764 sec/step, loss=0.55179, avg_loss=0.56528]\n",
            "Step 85909   [1.766 sec/step, loss=0.61376, avg_loss=0.56609]\n",
            "Step 85910   [1.809 sec/step, loss=0.44907, avg_loss=0.56471]\n",
            "Step 85911   [1.807 sec/step, loss=0.57122, avg_loss=0.56464]\n",
            "Generated 16 batches of size 16 in 9.709 sec\n",
            "Step 85912   [1.817 sec/step, loss=0.55012, avg_loss=0.56454]\n",
            "Step 85913   [1.825 sec/step, loss=0.54355, avg_loss=0.56391]\n",
            "Step 85914   [1.810 sec/step, loss=0.60240, avg_loss=0.56460]\n",
            "Step 85915   [1.789 sec/step, loss=0.59872, avg_loss=0.56516]\n",
            "Step 85916   [1.758 sec/step, loss=0.59712, avg_loss=0.56672]\n",
            "Step 85917   [1.760 sec/step, loss=0.60954, avg_loss=0.56684]\n",
            "Step 85918   [1.766 sec/step, loss=0.54664, avg_loss=0.56646]\n",
            "Step 85919   [1.772 sec/step, loss=0.56826, avg_loss=0.56668]\n",
            "Step 85920   [1.780 sec/step, loss=0.53791, avg_loss=0.56711]\n",
            "Step 85921   [1.776 sec/step, loss=0.57123, avg_loss=0.56658]\n",
            "Step 85922   [1.777 sec/step, loss=0.57279, avg_loss=0.56644]\n",
            "Step 85923   [1.779 sec/step, loss=0.60009, avg_loss=0.56649]\n",
            "Step 85924   [1.779 sec/step, loss=0.60859, avg_loss=0.56657]\n",
            "Step 85925   [1.785 sec/step, loss=0.54888, avg_loss=0.56619]\n",
            "Step 85926   [1.785 sec/step, loss=0.57147, avg_loss=0.56621]\n",
            "Step 85927   [1.793 sec/step, loss=0.53826, avg_loss=0.56593]\n",
            "Step 85928   [1.783 sec/step, loss=0.59020, avg_loss=0.56615]\n",
            "Step 85929   [1.788 sec/step, loss=0.57032, avg_loss=0.56570]\n",
            "Step 85930   [1.786 sec/step, loss=0.59870, avg_loss=0.56597]\n",
            "Generated 16 batches of size 16 in 10.109 sec\n",
            "Step 85931   [1.774 sec/step, loss=0.55569, avg_loss=0.56611]\n",
            "Step 85932   [1.779 sec/step, loss=0.52762, avg_loss=0.56557]\n",
            "Step 85933   [1.799 sec/step, loss=0.42579, avg_loss=0.56439]\n",
            "Step 85934   [1.800 sec/step, loss=0.55743, avg_loss=0.56391]\n",
            "Step 85935   [1.795 sec/step, loss=0.57819, avg_loss=0.56385]\n",
            "Step 85936   [1.790 sec/step, loss=0.55727, avg_loss=0.56362]\n",
            "Step 85937   [1.790 sec/step, loss=0.61053, avg_loss=0.56427]\n",
            "Step 85938   [1.764 sec/step, loss=0.60252, avg_loss=0.56587]\n",
            "Step 85939   [1.773 sec/step, loss=0.54991, avg_loss=0.56535]\n",
            "Step 85940   [1.772 sec/step, loss=0.58989, avg_loss=0.56567]\n",
            "Step 85941   [1.785 sec/step, loss=0.51846, avg_loss=0.56523]\n",
            "Step 85942   [1.792 sec/step, loss=0.57621, avg_loss=0.56523]\n",
            "Step 85943   [1.798 sec/step, loss=0.59850, avg_loss=0.56535]\n",
            "Step 85944   [1.796 sec/step, loss=0.58181, avg_loss=0.56551]\n",
            "Generated 16 batches of size 16 in 8.202 sec\n",
            "Step 85945   [1.801 sec/step, loss=0.55220, avg_loss=0.56512]\n",
            "Step 85946   [1.838 sec/step, loss=0.39323, avg_loss=0.56321]\n",
            "Step 85947   [1.829 sec/step, loss=0.56348, avg_loss=0.56277]\n",
            "Step 85948   [1.812 sec/step, loss=0.55458, avg_loss=0.56323]\n",
            "Step 85949   [1.808 sec/step, loss=0.56890, avg_loss=0.56290]\n",
            "Step 85950   [1.792 sec/step, loss=0.61542, avg_loss=0.56326]\n",
            "Step 85951   [1.784 sec/step, loss=0.56597, avg_loss=0.56307]\n",
            "Step 85952   [1.784 sec/step, loss=0.47056, avg_loss=0.56270]\n",
            "Step 85953   [1.784 sec/step, loss=0.57585, avg_loss=0.56284]\n",
            "Step 85954   [1.788 sec/step, loss=0.55141, avg_loss=0.56220]\n",
            "Step 85955   [1.785 sec/step, loss=0.55396, avg_loss=0.56207]\n",
            "Step 85956   [1.778 sec/step, loss=0.59109, avg_loss=0.56265]\n",
            "Step 85957   [1.789 sec/step, loss=0.53657, avg_loss=0.56231]\n",
            "Step 85958   [1.797 sec/step, loss=0.55736, avg_loss=0.56187]\n",
            "Generated 16 batches of size 16 in 7.209 sec\n",
            "Step 85959   [1.819 sec/step, loss=0.52353, avg_loss=0.56155]\n",
            "Step 85960   [1.779 sec/step, loss=0.60630, avg_loss=0.56281]\n",
            "Step 85961   [1.777 sec/step, loss=0.59010, avg_loss=0.56270]\n",
            "Step 85962   [1.777 sec/step, loss=0.59068, avg_loss=0.56276]\n",
            "Step 85963   [1.772 sec/step, loss=0.57633, avg_loss=0.56274]\n",
            "Step 85964   [1.764 sec/step, loss=0.61602, avg_loss=0.56305]\n",
            "Step 85965   [1.769 sec/step, loss=0.53975, avg_loss=0.56267]\n",
            "Step 85966   [1.762 sec/step, loss=0.58576, avg_loss=0.56262]\n",
            "Step 85967   [1.762 sec/step, loss=0.59422, avg_loss=0.56272]\n",
            "Step 85968   [1.780 sec/step, loss=0.40123, avg_loss=0.56148]\n",
            "Step 85969   [1.775 sec/step, loss=0.56281, avg_loss=0.56222]\n",
            "Step 85970   [1.788 sec/step, loss=0.59393, avg_loss=0.56272]\n",
            "Step 85971   [1.785 sec/step, loss=0.54894, avg_loss=0.56248]\n",
            "Step 85972   [1.785 sec/step, loss=0.60099, avg_loss=0.56269]\n",
            "Step 85973   [1.786 sec/step, loss=0.55945, avg_loss=0.56245]\n",
            "Step 85974   [1.793 sec/step, loss=0.59019, avg_loss=0.56265]\n",
            "Step 85975   [1.789 sec/step, loss=0.59875, avg_loss=0.56284]\n",
            "Step 85976   [1.804 sec/step, loss=0.59159, avg_loss=0.56293]\n",
            "Generated 16 batches of size 16 in 8.127 sec\n",
            "Step 85977   [1.807 sec/step, loss=0.59257, avg_loss=0.56302]\n",
            "Step 85978   [1.802 sec/step, loss=0.57000, avg_loss=0.56336]\n",
            "Step 85979   [1.797 sec/step, loss=0.54241, avg_loss=0.56286]\n",
            "Step 85980   [1.802 sec/step, loss=0.57130, avg_loss=0.56318]\n",
            "Step 85981   [1.791 sec/step, loss=0.57325, avg_loss=0.56328]\n",
            "Step 85982   [1.798 sec/step, loss=0.56645, avg_loss=0.56331]\n",
            "Step 85983   [1.801 sec/step, loss=0.56329, avg_loss=0.56298]\n",
            "Step 85984   [1.757 sec/step, loss=0.55018, avg_loss=0.56401]\n",
            "Step 85985   [1.754 sec/step, loss=0.53051, avg_loss=0.56397]\n",
            "Step 85986   [1.783 sec/step, loss=0.45114, avg_loss=0.56306]\n",
            "Step 85987   [1.785 sec/step, loss=0.57204, avg_loss=0.56286]\n",
            "Step 85988   [1.794 sec/step, loss=0.54706, avg_loss=0.56246]\n",
            "Step 85989   [1.794 sec/step, loss=0.59497, avg_loss=0.56252]\n",
            "Step 85990   [1.793 sec/step, loss=0.56966, avg_loss=0.56232]\n",
            "Step 85991   [1.802 sec/step, loss=0.60506, avg_loss=0.56266]\n",
            "Step 85992   [1.769 sec/step, loss=0.57428, avg_loss=0.56407]\n",
            "Step 85993   [1.771 sec/step, loss=0.54778, avg_loss=0.56435]\n",
            "Generated 16 batches of size 16 in 8.332 sec\n",
            "Step 85994   [1.758 sec/step, loss=0.57281, avg_loss=0.56438]\n",
            "Step 85995   [1.749 sec/step, loss=0.62136, avg_loss=0.56479]\n",
            "Step 85996   [1.744 sec/step, loss=0.58498, avg_loss=0.56498]\n",
            "Step 85997   [1.724 sec/step, loss=0.56473, avg_loss=0.56512]\n",
            "Step 85998   [1.722 sec/step, loss=0.60838, avg_loss=0.56559]\n",
            "Step 85999   [1.725 sec/step, loss=0.55194, avg_loss=0.56529]\n",
            "Step 86000   [1.738 sec/step, loss=0.55267, avg_loss=0.56487]\n",
            "Writing summary at step: 86000\n",
            "Saving checkpoint to: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/model.ckpt-86000\n",
            "Step 86001   [1.771 sec/step, loss=0.38906, avg_loss=0.56285]\n",
            "Step 86002   [1.777 sec/step, loss=0.57545, avg_loss=0.56298]\n",
            "Step 86003   [1.781 sec/step, loss=0.61991, avg_loss=0.56329]\n",
            "Step 86004   [1.770 sec/step, loss=0.57364, avg_loss=0.56364]\n",
            "Step 86005   [1.775 sec/step, loss=0.61145, avg_loss=0.56384]\n",
            "Step 86006   [1.775 sec/step, loss=0.54862, avg_loss=0.56376]\n",
            "Step 86007   [1.789 sec/step, loss=0.54834, avg_loss=0.56307]\n",
            "Step 86008   [1.785 sec/step, loss=0.56625, avg_loss=0.56321]\n",
            "Generated 16 batches of size 16 in 9.717 sec\n",
            "Step 86009   [1.785 sec/step, loss=0.57934, avg_loss=0.56287]\n",
            "Step 86010   [1.736 sec/step, loss=0.57503, avg_loss=0.56413]\n",
            "Step 86011   [1.745 sec/step, loss=0.54615, avg_loss=0.56388]\n",
            "Step 86012   [1.739 sec/step, loss=0.58344, avg_loss=0.56421]\n",
            "Step 86013   [1.738 sec/step, loss=0.55559, avg_loss=0.56433]\n",
            "Step 86014   [1.742 sec/step, loss=0.59341, avg_loss=0.56424]\n",
            "Step 86015   [1.742 sec/step, loss=0.56250, avg_loss=0.56388]\n",
            "Step 86016   [1.730 sec/step, loss=0.56817, avg_loss=0.56359]\n",
            "Step 86017   [1.734 sec/step, loss=0.54299, avg_loss=0.56292]\n",
            "Step 86018   [1.730 sec/step, loss=0.59332, avg_loss=0.56339]\n",
            "Step 86019   [1.762 sec/step, loss=0.55140, avg_loss=0.56322]\n",
            "Step 86020   [1.777 sec/step, loss=0.52378, avg_loss=0.56308]\n",
            "Step 86021   [1.786 sec/step, loss=0.57741, avg_loss=0.56314]\n",
            "Step 86022   [1.789 sec/step, loss=0.58076, avg_loss=0.56322]\n",
            "Step 86023   [1.790 sec/step, loss=0.56881, avg_loss=0.56291]\n",
            "Step 86024   [1.790 sec/step, loss=0.58092, avg_loss=0.56263]\n",
            "Generated 16 batches of size 16 in 10.334 sec\n",
            "Step 86025   [1.788 sec/step, loss=0.58777, avg_loss=0.56302]\n",
            "Step 86026   [1.793 sec/step, loss=0.54413, avg_loss=0.56275]\n",
            "Step 86027   [1.787 sec/step, loss=0.59241, avg_loss=0.56329]\n",
            "Step 86028   [1.785 sec/step, loss=0.61023, avg_loss=0.56349]\n",
            "Step 86029   [1.781 sec/step, loss=0.57056, avg_loss=0.56349]\n",
            "Step 86030   [1.794 sec/step, loss=0.54029, avg_loss=0.56291]\n",
            "Step 86031   [1.796 sec/step, loss=0.59914, avg_loss=0.56334]\n",
            "Step 86032   [1.792 sec/step, loss=0.54652, avg_loss=0.56353]\n",
            "Step 86033   [1.757 sec/step, loss=0.60189, avg_loss=0.56529]\n",
            "Step 86034   [1.753 sec/step, loss=0.61754, avg_loss=0.56589]\n",
            "Step 86035   [1.749 sec/step, loss=0.59489, avg_loss=0.56606]\n",
            "Step 86036   [1.764 sec/step, loss=0.56705, avg_loss=0.56616]\n",
            "Step 86037   [1.765 sec/step, loss=0.52193, avg_loss=0.56527]\n",
            "Step 86038   [1.808 sec/step, loss=0.49959, avg_loss=0.56424]\n",
            "Generated 16 batches of size 16 in 10.020 sec\n",
            "Step 86039   [1.800 sec/step, loss=0.60695, avg_loss=0.56481]\n",
            "Step 86040   [1.798 sec/step, loss=0.57299, avg_loss=0.56465]\n",
            "Step 86041   [1.784 sec/step, loss=0.55166, avg_loss=0.56498]\n",
            "Step 86042   [1.783 sec/step, loss=0.52394, avg_loss=0.56445]\n",
            "Step 86043   [1.781 sec/step, loss=0.56787, avg_loss=0.56415]\n",
            "Step 86044   [1.789 sec/step, loss=0.53990, avg_loss=0.56373]\n",
            "Step 86045   [1.783 sec/step, loss=0.59650, avg_loss=0.56417]\n",
            "Step 86046   [1.744 sec/step, loss=0.56645, avg_loss=0.56590]\n",
            "Step 86047   [1.745 sec/step, loss=0.56556, avg_loss=0.56593]\n",
            "Step 86048   [1.740 sec/step, loss=0.58796, avg_loss=0.56626]\n",
            "Step 86049   [1.738 sec/step, loss=0.57161, avg_loss=0.56629]\n",
            "Step 86050   [1.740 sec/step, loss=0.59253, avg_loss=0.56606]\n",
            "Step 86051   [1.745 sec/step, loss=0.59357, avg_loss=0.56633]\n",
            "Step 86052   [1.721 sec/step, loss=0.55901, avg_loss=0.56722]\n",
            "Step 86053   [1.729 sec/step, loss=0.49561, avg_loss=0.56642]\n",
            "Generated 16 batches of size 16 in 9.905 sec\n",
            "Step 86054   [1.771 sec/step, loss=0.39042, avg_loss=0.56481]\n",
            "Step 86055   [1.775 sec/step, loss=0.50483, avg_loss=0.56431]\n",
            "Step 86056   [1.777 sec/step, loss=0.58379, avg_loss=0.56424]\n",
            "Step 86057   [1.775 sec/step, loss=0.51711, avg_loss=0.56405]\n",
            "Step 86058   [1.770 sec/step, loss=0.57619, avg_loss=0.56423]\n",
            "Step 86059   [1.746 sec/step, loss=0.61264, avg_loss=0.56513]\n",
            "Step 86060   [1.746 sec/step, loss=0.57865, avg_loss=0.56485]\n",
            "Step 86061   [1.750 sec/step, loss=0.57607, avg_loss=0.56471]\n",
            "Step 86062   [1.755 sec/step, loss=0.55040, avg_loss=0.56431]\n",
            "Step 86063   [1.752 sec/step, loss=0.57612, avg_loss=0.56430]\n",
            "Step 86064   [1.761 sec/step, loss=0.56402, avg_loss=0.56378]\n",
            "Step 86065   [1.756 sec/step, loss=0.58988, avg_loss=0.56429]\n",
            "Step 86066   [1.758 sec/step, loss=0.54851, avg_loss=0.56391]\n",
            "Step 86067   [1.763 sec/step, loss=0.49609, avg_loss=0.56293]\n",
            "Step 86068   [1.729 sec/step, loss=0.59532, avg_loss=0.56487]\n",
            "Step 86069   [1.776 sec/step, loss=0.40128, avg_loss=0.56326]\n",
            "Step 86070   [1.771 sec/step, loss=0.57194, avg_loss=0.56304]\n",
            "Generated 16 batches of size 16 in 10.030 sec\n",
            "Step 86071   [1.770 sec/step, loss=0.59776, avg_loss=0.56353]\n",
            "Step 86072   [1.772 sec/step, loss=0.60164, avg_loss=0.56353]\n",
            "Step 86073   [1.771 sec/step, loss=0.61655, avg_loss=0.56410]\n",
            "Step 86074   [1.774 sec/step, loss=0.55986, avg_loss=0.56380]\n",
            "Step 86075   [1.774 sec/step, loss=0.56152, avg_loss=0.56343]\n",
            "Step 86076   [1.755 sec/step, loss=0.59511, avg_loss=0.56346]\n",
            "Step 86077   [1.751 sec/step, loss=0.55874, avg_loss=0.56312]\n",
            "Step 86078   [1.754 sec/step, loss=0.53440, avg_loss=0.56277]\n",
            "Step 86079   [1.751 sec/step, loss=0.57786, avg_loss=0.56312]\n",
            "Step 86080   [1.742 sec/step, loss=0.55760, avg_loss=0.56299]\n",
            "Step 86081   [1.743 sec/step, loss=0.55563, avg_loss=0.56281]\n",
            "Step 86082   [1.777 sec/step, loss=0.39755, avg_loss=0.56112]\n",
            "Step 86083   [1.776 sec/step, loss=0.59709, avg_loss=0.56146]\n",
            "Step 86084   [1.783 sec/step, loss=0.58095, avg_loss=0.56177]\n",
            "Step 86085   [1.774 sec/step, loss=0.59310, avg_loss=0.56239]\n",
            "Step 86086   [1.734 sec/step, loss=0.58475, avg_loss=0.56373]\n",
            "Step 86087   [1.747 sec/step, loss=0.53354, avg_loss=0.56334]\n",
            "Step 86088   [1.740 sec/step, loss=0.58144, avg_loss=0.56369]\n",
            "Generated 16 batches of size 16 in 10.040 sec\n",
            "Step 86089   [1.745 sec/step, loss=0.59109, avg_loss=0.56365]\n",
            "Step 86090   [1.752 sec/step, loss=0.56087, avg_loss=0.56356]\n",
            "Step 86091   [1.739 sec/step, loss=0.56268, avg_loss=0.56314]\n",
            "Step 86092   [1.735 sec/step, loss=0.53507, avg_loss=0.56274]\n",
            "Step 86093   [1.732 sec/step, loss=0.57270, avg_loss=0.56299]\n",
            "Step 86094   [1.735 sec/step, loss=0.53459, avg_loss=0.56261]\n",
            "Step 86095   [1.733 sec/step, loss=0.61948, avg_loss=0.56259]\n",
            "Step 86096   [1.734 sec/step, loss=0.57175, avg_loss=0.56246]\n",
            "Step 86097   [1.742 sec/step, loss=0.57314, avg_loss=0.56254]\n",
            "Step 86098   [1.741 sec/step, loss=0.59652, avg_loss=0.56243]\n",
            "Step 86099   [1.741 sec/step, loss=0.58131, avg_loss=0.56272]\n",
            "Step 86100   [1.752 sec/step, loss=0.52043, avg_loss=0.56240]\n",
            "Writing summary at step: 86100\n",
            "Step 86101   [1.714 sec/step, loss=0.62185, avg_loss=0.56473]\n",
            "Step 86102   [1.723 sec/step, loss=0.53378, avg_loss=0.56431]\n",
            "Generated 16 batches of size 16 in 10.132 sec\n",
            "Step 86103   [1.758 sec/step, loss=0.44534, avg_loss=0.56256]\n",
            "Step 86104   [1.755 sec/step, loss=0.57203, avg_loss=0.56255]\n",
            "Step 86105   [1.752 sec/step, loss=0.57389, avg_loss=0.56217]\n",
            "Step 86106   [1.746 sec/step, loss=0.58316, avg_loss=0.56252]\n",
            "Step 86107   [1.730 sec/step, loss=0.55691, avg_loss=0.56260]\n",
            "Step 86108   [1.732 sec/step, loss=0.58082, avg_loss=0.56275]\n",
            "Step 86109   [1.728 sec/step, loss=0.60370, avg_loss=0.56299]\n",
            "Step 86110   [1.728 sec/step, loss=0.58640, avg_loss=0.56311]\n",
            "Step 86111   [1.722 sec/step, loss=0.58809, avg_loss=0.56352]\n",
            "Step 86112   [1.718 sec/step, loss=0.57760, avg_loss=0.56347]\n",
            "Step 86113   [1.718 sec/step, loss=0.55670, avg_loss=0.56348]\n",
            "Step 86114   [1.758 sec/step, loss=0.37846, avg_loss=0.56133]\n",
            "Step 86115   [1.758 sec/step, loss=0.54586, avg_loss=0.56116]\n",
            "Step 86116   [1.768 sec/step, loss=0.57717, avg_loss=0.56125]\n",
            "Step 86117   [1.776 sec/step, loss=0.52614, avg_loss=0.56108]\n",
            "Step 86118   [1.778 sec/step, loss=0.59819, avg_loss=0.56113]\n",
            "Step 86119   [1.746 sec/step, loss=0.59545, avg_loss=0.56157]\n",
            "Step 86120   [1.721 sec/step, loss=0.56922, avg_loss=0.56203]\n",
            "Generated 16 batches of size 16 in 10.410 sec\n",
            "Step 86121   [1.720 sec/step, loss=0.57593, avg_loss=0.56201]\n",
            "Step 86122   [1.718 sec/step, loss=0.59809, avg_loss=0.56219]\n",
            "Step 86123   [1.729 sec/step, loss=0.54028, avg_loss=0.56190]\n",
            "Step 86124   [1.730 sec/step, loss=0.59566, avg_loss=0.56205]\n",
            "Step 86125   [1.730 sec/step, loss=0.53388, avg_loss=0.56151]\n",
            "Step 86126   [1.724 sec/step, loss=0.51537, avg_loss=0.56122]\n",
            "Step 86127   [1.724 sec/step, loss=0.59176, avg_loss=0.56121]\n",
            "Step 86128   [1.769 sec/step, loss=0.46520, avg_loss=0.55976]\n",
            "Step 86129   [1.771 sec/step, loss=0.58425, avg_loss=0.55990]\n",
            "Step 86130   [1.754 sec/step, loss=0.59938, avg_loss=0.56049]\n",
            "Step 86131   [1.752 sec/step, loss=0.56962, avg_loss=0.56020]\n",
            "Step 86132   [1.770 sec/step, loss=0.54762, avg_loss=0.56021]\n",
            "Step 86133   [1.772 sec/step, loss=0.57766, avg_loss=0.55997]\n",
            "Step 86134   [1.787 sec/step, loss=0.52996, avg_loss=0.55909]\n",
            "Step 86135   [1.790 sec/step, loss=0.57803, avg_loss=0.55892]\n",
            "Generated 16 batches of size 16 in 10.356 sec\n",
            "Step 86136   [1.787 sec/step, loss=0.58291, avg_loss=0.55908]\n",
            "Step 86137   [1.784 sec/step, loss=0.59555, avg_loss=0.55982]\n",
            "Step 86138   [1.738 sec/step, loss=0.56809, avg_loss=0.56050]\n",
            "Step 86139   [1.739 sec/step, loss=0.60312, avg_loss=0.56046]\n",
            "Step 86140   [1.741 sec/step, loss=0.59868, avg_loss=0.56072]\n",
            "Step 86141   [1.733 sec/step, loss=0.58434, avg_loss=0.56105]\n",
            "Step 86142   [1.728 sec/step, loss=0.58147, avg_loss=0.56162]\n",
            "Step 86143   [1.730 sec/step, loss=0.59770, avg_loss=0.56192]\n",
            "Step 86144   [1.721 sec/step, loss=0.58169, avg_loss=0.56234]\n",
            "Step 86145   [1.724 sec/step, loss=0.59631, avg_loss=0.56234]\n",
            "Step 86146   [1.729 sec/step, loss=0.57309, avg_loss=0.56240]\n",
            "Step 86147   [1.736 sec/step, loss=0.58715, avg_loss=0.56262]\n",
            "Step 86148   [1.737 sec/step, loss=0.58043, avg_loss=0.56254]\n",
            "Step 86149   [1.739 sec/step, loss=0.57951, avg_loss=0.56262]\n",
            "Generated 16 batches of size 16 in 7.375 sec\n",
            "Step 86150   [1.765 sec/step, loss=0.54993, avg_loss=0.56220]\n",
            "Step 86151   [1.795 sec/step, loss=0.47992, avg_loss=0.56106]\n",
            "Step 86152   [1.783 sec/step, loss=0.57238, avg_loss=0.56119]\n",
            "Step 86153   [1.783 sec/step, loss=0.51566, avg_loss=0.56139]\n",
            "Step 86154   [1.743 sec/step, loss=0.51465, avg_loss=0.56264]\n",
            "Step 86155   [1.737 sec/step, loss=0.55449, avg_loss=0.56313]\n",
            "Step 86156   [1.736 sec/step, loss=0.56026, avg_loss=0.56290]\n",
            "Step 86157   [1.724 sec/step, loss=0.56740, avg_loss=0.56340]\n",
            "Step 86158   [1.724 sec/step, loss=0.58509, avg_loss=0.56349]\n",
            "Step 86159   [1.723 sec/step, loss=0.57894, avg_loss=0.56315]\n",
            "Step 86160   [1.721 sec/step, loss=0.59819, avg_loss=0.56335]\n",
            "Step 86161   [1.717 sec/step, loss=0.60186, avg_loss=0.56361]\n",
            "Step 86162   [1.751 sec/step, loss=0.44749, avg_loss=0.56258]\n",
            "Step 86163   [1.768 sec/step, loss=0.56810, avg_loss=0.56250]\n",
            "Step 86164   [1.762 sec/step, loss=0.57728, avg_loss=0.56263]\n",
            "Step 86165   [1.775 sec/step, loss=0.57552, avg_loss=0.56248]\n",
            "Generated 16 batches of size 16 in 7.625 sec\n",
            "Step 86166   [1.795 sec/step, loss=0.53067, avg_loss=0.56231]\n",
            "Step 86167   [1.796 sec/step, loss=0.58631, avg_loss=0.56321]\n",
            "Step 86168   [1.791 sec/step, loss=0.58959, avg_loss=0.56315]\n",
            "Step 86169   [1.737 sec/step, loss=0.53715, avg_loss=0.56451]\n",
            "Step 86170   [1.733 sec/step, loss=0.56173, avg_loss=0.56441]\n",
            "Step 86171   [1.735 sec/step, loss=0.61296, avg_loss=0.56456]\n",
            "Step 86172   [1.734 sec/step, loss=0.61145, avg_loss=0.56466]\n",
            "Step 86173   [1.742 sec/step, loss=0.56774, avg_loss=0.56417]\n",
            "Step 86174   [1.734 sec/step, loss=0.54691, avg_loss=0.56404]\n",
            "Step 86175   [1.738 sec/step, loss=0.57534, avg_loss=0.56418]\n",
            "Step 86176   [1.741 sec/step, loss=0.58845, avg_loss=0.56411]\n",
            "Step 86177   [1.746 sec/step, loss=0.49787, avg_loss=0.56350]\n",
            "Step 86178   [1.750 sec/step, loss=0.57852, avg_loss=0.56394]\n",
            "Step 86179   [1.751 sec/step, loss=0.58240, avg_loss=0.56399]\n",
            "Step 86180   [1.796 sec/step, loss=0.51078, avg_loss=0.56352]\n",
            "Generated 16 batches of size 16 in 7.819 sec\n",
            "Step 86181   [1.795 sec/step, loss=0.62446, avg_loss=0.56421]\n",
            "Step 86182   [1.757 sec/step, loss=0.58699, avg_loss=0.56610]\n",
            "Step 86183   [1.754 sec/step, loss=0.60782, avg_loss=0.56621]\n",
            "Step 86184   [1.749 sec/step, loss=0.55799, avg_loss=0.56598]\n",
            "Step 86185   [1.743 sec/step, loss=0.58207, avg_loss=0.56587]\n",
            "Step 86186   [1.743 sec/step, loss=0.58815, avg_loss=0.56591]\n",
            "Step 86187   [1.741 sec/step, loss=0.53769, avg_loss=0.56595]\n",
            "Step 86188   [1.737 sec/step, loss=0.57749, avg_loss=0.56591]\n",
            "Step 86189   [1.731 sec/step, loss=0.57119, avg_loss=0.56571]\n",
            "Step 86190   [1.724 sec/step, loss=0.52210, avg_loss=0.56532]\n",
            "Step 86191   [1.726 sec/step, loss=0.58151, avg_loss=0.56551]\n",
            "Step 86192   [1.730 sec/step, loss=0.56604, avg_loss=0.56582]\n",
            "Step 86193   [1.731 sec/step, loss=0.59956, avg_loss=0.56609]\n",
            "Step 86194   [1.727 sec/step, loss=0.58594, avg_loss=0.56660]\n",
            "Step 86195   [1.729 sec/step, loss=0.59559, avg_loss=0.56636]\n",
            "Step 86196   [1.734 sec/step, loss=0.59949, avg_loss=0.56664]\n",
            "Step 86197   [1.735 sec/step, loss=0.56423, avg_loss=0.56655]\n",
            "Step 86198   [1.759 sec/step, loss=0.51535, avg_loss=0.56574]\n",
            "Generated 16 batches of size 16 in 8.313 sec\n",
            "Step 86199   [1.754 sec/step, loss=0.59266, avg_loss=0.56585]\n",
            "Step 86200   [1.734 sec/step, loss=0.60967, avg_loss=0.56674]\n",
            "Writing summary at step: 86200\n",
            "Step 86201   [1.740 sec/step, loss=0.52580, avg_loss=0.56578]\n",
            "Step 86202   [1.726 sec/step, loss=0.56630, avg_loss=0.56611]\n",
            "Step 86203   [1.704 sec/step, loss=0.53985, avg_loss=0.56705]\n",
            "Step 86204   [1.705 sec/step, loss=0.57340, avg_loss=0.56707]\n",
            "Step 86205   [1.704 sec/step, loss=0.60375, avg_loss=0.56737]\n",
            "Step 86206   [1.705 sec/step, loss=0.57643, avg_loss=0.56730]\n",
            "Step 86207   [1.714 sec/step, loss=0.59741, avg_loss=0.56770]\n",
            "Step 86208   [1.721 sec/step, loss=0.51699, avg_loss=0.56707]\n",
            "Step 86209   [1.719 sec/step, loss=0.57426, avg_loss=0.56677]\n",
            "Step 86210   [1.721 sec/step, loss=0.58689, avg_loss=0.56678]\n",
            "Step 86211   [1.719 sec/step, loss=0.57712, avg_loss=0.56667]\n",
            "Step 86212   [1.725 sec/step, loss=0.56230, avg_loss=0.56651]\n",
            "Step 86213   [1.762 sec/step, loss=0.53307, avg_loss=0.56628]\n",
            "Generated 16 batches of size 16 in 9.607 sec\n",
            "Step 86214   [1.724 sec/step, loss=0.57629, avg_loss=0.56826]\n",
            "Step 86215   [1.727 sec/step, loss=0.54942, avg_loss=0.56829]\n",
            "Step 86216   [1.720 sec/step, loss=0.59561, avg_loss=0.56848]\n",
            "Step 86217   [1.712 sec/step, loss=0.53960, avg_loss=0.56861]\n",
            "Step 86218   [1.708 sec/step, loss=0.60717, avg_loss=0.56870]\n",
            "Step 86219   [1.713 sec/step, loss=0.58160, avg_loss=0.56856]\n",
            "Step 86220   [1.717 sec/step, loss=0.54228, avg_loss=0.56829]\n",
            "Step 86221   [1.712 sec/step, loss=0.61985, avg_loss=0.56873]\n",
            "Step 86222   [1.713 sec/step, loss=0.53228, avg_loss=0.56807]\n",
            "Step 86223   [1.699 sec/step, loss=0.59962, avg_loss=0.56867]\n",
            "Step 86224   [1.699 sec/step, loss=0.57402, avg_loss=0.56845]\n",
            "Step 86225   [1.699 sec/step, loss=0.58346, avg_loss=0.56895]\n",
            "Step 86226   [1.700 sec/step, loss=0.59630, avg_loss=0.56976]\n",
            "Step 86227   [1.726 sec/step, loss=0.52186, avg_loss=0.56906]\n",
            "Step 86228   [1.684 sec/step, loss=0.59497, avg_loss=0.57035]\n",
            "Step 86229   [1.690 sec/step, loss=0.56807, avg_loss=0.57019]\n",
            "Step 86230   [1.691 sec/step, loss=0.55572, avg_loss=0.56976]\n",
            "Generated 16 batches of size 16 in 10.214 sec\n",
            "Step 86231   [1.692 sec/step, loss=0.59281, avg_loss=0.56999]\n",
            "Step 86232   [1.709 sec/step, loss=0.46765, avg_loss=0.56919]\n",
            "Step 86233   [1.710 sec/step, loss=0.58742, avg_loss=0.56929]\n",
            "Step 86234   [1.694 sec/step, loss=0.56817, avg_loss=0.56967]\n",
            "Step 86235   [1.696 sec/step, loss=0.58001, avg_loss=0.56969]\n",
            "Step 86236   [1.687 sec/step, loss=0.60498, avg_loss=0.56991]\n",
            "Step 86237   [1.692 sec/step, loss=0.55137, avg_loss=0.56947]\n",
            "Step 86238   [1.689 sec/step, loss=0.56633, avg_loss=0.56945]\n",
            "Step 86239   [1.691 sec/step, loss=0.56966, avg_loss=0.56911]\n",
            "Step 86240   [1.691 sec/step, loss=0.60961, avg_loss=0.56922]\n",
            "Step 86241   [1.702 sec/step, loss=0.54922, avg_loss=0.56887]\n",
            "Step 86242   [1.704 sec/step, loss=0.56172, avg_loss=0.56868]\n",
            "Step 86243   [1.721 sec/step, loss=0.53125, avg_loss=0.56801]\n",
            "Step 86244   [1.723 sec/step, loss=0.59921, avg_loss=0.56819]\n",
            "Step 86245   [1.730 sec/step, loss=0.61284, avg_loss=0.56835]\n",
            "Step 86246   [1.729 sec/step, loss=0.56336, avg_loss=0.56825]\n",
            "Generated 16 batches of size 16 in 10.308 sec\n",
            "Step 86247   [1.725 sec/step, loss=0.57461, avg_loss=0.56813]\n",
            "Step 86248   [1.727 sec/step, loss=0.57764, avg_loss=0.56810]\n",
            "Step 86249   [1.725 sec/step, loss=0.59516, avg_loss=0.56826]\n",
            "Step 86250   [1.742 sec/step, loss=0.47543, avg_loss=0.56751]\n",
            "Step 86251   [1.708 sec/step, loss=0.59024, avg_loss=0.56862]\n",
            "Step 86252   [1.709 sec/step, loss=0.54070, avg_loss=0.56830]\n",
            "Step 86253   [1.742 sec/step, loss=0.38806, avg_loss=0.56702]\n",
            "Step 86254   [1.733 sec/step, loss=0.60147, avg_loss=0.56789]\n",
            "Step 86255   [1.733 sec/step, loss=0.56263, avg_loss=0.56797]\n",
            "Step 86256   [1.734 sec/step, loss=0.56850, avg_loss=0.56805]\n",
            "Step 86257   [1.732 sec/step, loss=0.60311, avg_loss=0.56841]\n",
            "Step 86258   [1.738 sec/step, loss=0.58078, avg_loss=0.56837]\n",
            "Step 86259   [1.744 sec/step, loss=0.59517, avg_loss=0.56853]\n",
            "Step 86260   [1.759 sec/step, loss=0.55076, avg_loss=0.56806]\n",
            "Step 86261   [1.770 sec/step, loss=0.56148, avg_loss=0.56765]\n",
            "Generated 16 batches of size 16 in 10.148 sec\n",
            "Step 86262   [1.740 sec/step, loss=0.54046, avg_loss=0.56858]\n",
            "Step 86263   [1.739 sec/step, loss=0.55868, avg_loss=0.56849]\n",
            "Step 86264   [1.739 sec/step, loss=0.54834, avg_loss=0.56820]\n",
            "Step 86265   [1.726 sec/step, loss=0.54166, avg_loss=0.56786]\n",
            "Step 86266   [1.707 sec/step, loss=0.62199, avg_loss=0.56877]\n",
            "Step 86267   [1.702 sec/step, loss=0.61052, avg_loss=0.56902]\n",
            "Step 86268   [1.702 sec/step, loss=0.60439, avg_loss=0.56916]\n",
            "Step 86269   [1.703 sec/step, loss=0.55806, avg_loss=0.56937]\n",
            "Step 86270   [1.703 sec/step, loss=0.55693, avg_loss=0.56932]\n",
            "Step 86271   [1.708 sec/step, loss=0.53156, avg_loss=0.56851]\n",
            "Step 86272   [1.718 sec/step, loss=0.57119, avg_loss=0.56811]\n",
            "Step 86273   [1.715 sec/step, loss=0.55490, avg_loss=0.56798]\n",
            "Step 86274   [1.730 sec/step, loss=0.49061, avg_loss=0.56742]\n",
            "Step 86275   [1.728 sec/step, loss=0.54746, avg_loss=0.56714]\n",
            "Step 86276   [1.732 sec/step, loss=0.62539, avg_loss=0.56751]\n",
            "Step 86277   [1.738 sec/step, loss=0.57245, avg_loss=0.56825]\n",
            "Generated 16 batches of size 16 in 9.906 sec\n",
            "Step 86278   [1.773 sec/step, loss=0.39552, avg_loss=0.56642]\n",
            "Step 86279   [1.771 sec/step, loss=0.56620, avg_loss=0.56626]\n",
            "Step 86280   [1.726 sec/step, loss=0.59590, avg_loss=0.56711]\n",
            "Step 86281   [1.724 sec/step, loss=0.56094, avg_loss=0.56648]\n",
            "Step 86282   [1.719 sec/step, loss=0.58194, avg_loss=0.56643]\n",
            "Step 86283   [1.730 sec/step, loss=0.56637, avg_loss=0.56601]\n",
            "Step 86284   [1.769 sec/step, loss=0.38727, avg_loss=0.56431]\n",
            "Step 86285   [1.772 sec/step, loss=0.54820, avg_loss=0.56397]\n",
            "Step 86286   [1.771 sec/step, loss=0.58905, avg_loss=0.56398]\n",
            "Step 86287   [1.758 sec/step, loss=0.60449, avg_loss=0.56464]\n",
            "Step 86288   [1.757 sec/step, loss=0.59098, avg_loss=0.56478]\n",
            "Step 86289   [1.754 sec/step, loss=0.58140, avg_loss=0.56488]\n",
            "Step 86290   [1.753 sec/step, loss=0.57461, avg_loss=0.56541]\n",
            "Step 86291   [1.762 sec/step, loss=0.55711, avg_loss=0.56516]\n",
            "Step 86292   [1.756 sec/step, loss=0.56562, avg_loss=0.56516]\n",
            "Step 86293   [1.764 sec/step, loss=0.56116, avg_loss=0.56477]\n",
            "Step 86294   [1.769 sec/step, loss=0.55278, avg_loss=0.56444]\n",
            "Generated 16 batches of size 16 in 10.016 sec\n",
            "Step 86295   [1.780 sec/step, loss=0.56018, avg_loss=0.56409]\n",
            "Step 86296   [1.775 sec/step, loss=0.59666, avg_loss=0.56406]\n",
            "Step 86297   [1.767 sec/step, loss=0.57938, avg_loss=0.56421]\n",
            "Step 86298   [1.752 sec/step, loss=0.55267, avg_loss=0.56458]\n",
            "Step 86299   [1.755 sec/step, loss=0.57213, avg_loss=0.56438]\n",
            "Step 86300   [1.757 sec/step, loss=0.57003, avg_loss=0.56398]\n",
            "Writing summary at step: 86300\n",
            "Step 86301   [1.746 sec/step, loss=0.59165, avg_loss=0.56464]\n",
            "Step 86302   [1.756 sec/step, loss=0.51466, avg_loss=0.56412]\n",
            "Step 86303   [1.744 sec/step, loss=0.60383, avg_loss=0.56476]\n",
            "Step 86304   [1.743 sec/step, loss=0.53320, avg_loss=0.56436]\n",
            "Step 86305   [1.746 sec/step, loss=0.58077, avg_loss=0.56413]\n",
            "Step 86306   [1.747 sec/step, loss=0.56012, avg_loss=0.56397]\n",
            "Step 86307   [1.745 sec/step, loss=0.57146, avg_loss=0.56371]\n",
            "Step 86308   [1.738 sec/step, loss=0.59135, avg_loss=0.56445]\n",
            "Generated 16 batches of size 16 in 10.032 sec\n",
            "Step 86309   [1.787 sec/step, loss=0.43509, avg_loss=0.56306]\n",
            "Step 86310   [1.786 sec/step, loss=0.59482, avg_loss=0.56314]\n",
            "Step 86311   [1.783 sec/step, loss=0.56440, avg_loss=0.56301]\n",
            "Step 86312   [1.794 sec/step, loss=0.48230, avg_loss=0.56221]\n",
            "Step 86313   [1.757 sec/step, loss=0.54537, avg_loss=0.56234]\n",
            "Step 86314   [1.765 sec/step, loss=0.54372, avg_loss=0.56201]\n",
            "Step 86315   [1.761 sec/step, loss=0.59361, avg_loss=0.56245]\n",
            "Step 86316   [1.796 sec/step, loss=0.41409, avg_loss=0.56064]\n",
            "Step 86317   [1.796 sec/step, loss=0.56522, avg_loss=0.56089]\n",
            "Step 86318   [1.794 sec/step, loss=0.58498, avg_loss=0.56067]\n",
            "Step 86319   [1.784 sec/step, loss=0.57966, avg_loss=0.56065]\n",
            "Step 86320   [1.776 sec/step, loss=0.56767, avg_loss=0.56091]\n",
            "Step 86321   [1.784 sec/step, loss=0.55167, avg_loss=0.56022]\n",
            "Step 86322   [1.787 sec/step, loss=0.60188, avg_loss=0.56092]\n",
            "Step 86323   [1.791 sec/step, loss=0.58913, avg_loss=0.56082]\n",
            "Step 86324   [1.802 sec/step, loss=0.55549, avg_loss=0.56063]\n",
            "Step 86325   [1.801 sec/step, loss=0.60218, avg_loss=0.56082]\n",
            "Step 86326   [1.796 sec/step, loss=0.56940, avg_loss=0.56055]\n",
            "Generated 16 batches of size 16 in 10.042 sec\n",
            "Step 86327   [1.779 sec/step, loss=0.55220, avg_loss=0.56085]\n",
            "Step 86328   [1.780 sec/step, loss=0.53550, avg_loss=0.56026]\n",
            "Step 86329   [1.772 sec/step, loss=0.58368, avg_loss=0.56041]\n",
            "Step 86330   [1.776 sec/step, loss=0.55797, avg_loss=0.56044]\n",
            "Step 86331   [1.775 sec/step, loss=0.56240, avg_loss=0.56013]\n",
            "Step 86332   [1.737 sec/step, loss=0.57217, avg_loss=0.56118]\n",
            "Step 86333   [1.748 sec/step, loss=0.53276, avg_loss=0.56063]\n",
            "Step 86334   [1.755 sec/step, loss=0.56444, avg_loss=0.56059]\n",
            "Step 86335   [1.749 sec/step, loss=0.61085, avg_loss=0.56090]\n",
            "Step 86336   [1.754 sec/step, loss=0.54796, avg_loss=0.56033]\n",
            "Step 86337   [1.760 sec/step, loss=0.54175, avg_loss=0.56024]\n",
            "Step 86338   [1.769 sec/step, loss=0.54467, avg_loss=0.56002]\n",
            "Generated 16 batches of size 16 in 8.438 sec\n",
            "Step 86339   [1.808 sec/step, loss=0.40700, avg_loss=0.55839]\n",
            "Step 86340   [1.806 sec/step, loss=0.60315, avg_loss=0.55833]\n",
            "Step 86341   [1.797 sec/step, loss=0.58746, avg_loss=0.55871]\n",
            "Step 86342   [1.797 sec/step, loss=0.61282, avg_loss=0.55922]\n",
            "Step 86343   [1.779 sec/step, loss=0.53474, avg_loss=0.55926]\n",
            "Step 86344   [1.775 sec/step, loss=0.58006, avg_loss=0.55906]\n",
            "Step 86345   [1.765 sec/step, loss=0.55678, avg_loss=0.55850]\n",
            "Step 86346   [1.760 sec/step, loss=0.59040, avg_loss=0.55877]\n",
            "Step 86347   [1.767 sec/step, loss=0.58369, avg_loss=0.55886]\n",
            "Step 86348   [1.765 sec/step, loss=0.59636, avg_loss=0.55905]\n",
            "Step 86349   [1.765 sec/step, loss=0.58002, avg_loss=0.55890]\n",
            "Step 86350   [1.728 sec/step, loss=0.54614, avg_loss=0.55961]\n",
            "Step 86351   [1.729 sec/step, loss=0.56858, avg_loss=0.55939]\n",
            "Step 86352   [1.729 sec/step, loss=0.54744, avg_loss=0.55946]\n",
            "Step 86353   [1.691 sec/step, loss=0.56059, avg_loss=0.56118]\n",
            "Step 86354   [1.696 sec/step, loss=0.56600, avg_loss=0.56083]\n",
            "Step 86355   [1.706 sec/step, loss=0.58458, avg_loss=0.56105]\n",
            "Generated 16 batches of size 16 in 6.973 sec\n",
            "Step 86356   [1.722 sec/step, loss=0.55771, avg_loss=0.56094]\n",
            "Step 86357   [1.724 sec/step, loss=0.58109, avg_loss=0.56072]\n",
            "Step 86358   [1.714 sec/step, loss=0.58378, avg_loss=0.56075]\n",
            "Step 86359   [1.751 sec/step, loss=0.48445, avg_loss=0.55964]\n",
            "Step 86360   [1.743 sec/step, loss=0.57150, avg_loss=0.55985]\n",
            "Step 86361   [1.733 sec/step, loss=0.62317, avg_loss=0.56047]\n",
            "Step 86362   [1.727 sec/step, loss=0.56836, avg_loss=0.56075]\n",
            "Step 86363   [1.717 sec/step, loss=0.58286, avg_loss=0.56099]\n",
            "Step 86364   [1.712 sec/step, loss=0.59393, avg_loss=0.56144]\n",
            "Step 86365   [1.715 sec/step, loss=0.59792, avg_loss=0.56201]\n",
            "Step 86366   [1.714 sec/step, loss=0.59425, avg_loss=0.56173]\n",
            "Step 86367   [1.722 sec/step, loss=0.58278, avg_loss=0.56145]\n",
            "Step 86368   [1.719 sec/step, loss=0.55426, avg_loss=0.56095]\n",
            "Step 86369   [1.735 sec/step, loss=0.56834, avg_loss=0.56105]\n",
            "Step 86370   [1.734 sec/step, loss=0.61899, avg_loss=0.56167]\n",
            "Step 86371   [1.739 sec/step, loss=0.59056, avg_loss=0.56226]\n",
            "Generated 16 batches of size 16 in 7.624 sec\n",
            "Step 86372   [1.733 sec/step, loss=0.55360, avg_loss=0.56209]\n",
            "Step 86373   [1.734 sec/step, loss=0.56955, avg_loss=0.56223]\n",
            "Step 86374   [1.731 sec/step, loss=0.52070, avg_loss=0.56254]\n",
            "Step 86375   [1.767 sec/step, loss=0.47638, avg_loss=0.56182]\n",
            "Step 86376   [1.762 sec/step, loss=0.56650, avg_loss=0.56124]\n",
            "Step 86377   [1.756 sec/step, loss=0.57467, avg_loss=0.56126]\n",
            "Step 86378   [1.713 sec/step, loss=0.56536, avg_loss=0.56296]\n",
            "Step 86379   [1.723 sec/step, loss=0.61029, avg_loss=0.56340]\n",
            "Step 86380   [1.720 sec/step, loss=0.58208, avg_loss=0.56326]\n",
            "Step 86381   [1.728 sec/step, loss=0.52298, avg_loss=0.56288]\n",
            "Step 86382   [1.729 sec/step, loss=0.57783, avg_loss=0.56284]\n",
            "Step 86383   [1.719 sec/step, loss=0.60943, avg_loss=0.56327]\n",
            "Step 86384   [1.684 sec/step, loss=0.55957, avg_loss=0.56499]\n",
            "Step 86385   [1.695 sec/step, loss=0.55253, avg_loss=0.56504]\n",
            "Step 86386   [1.699 sec/step, loss=0.50294, avg_loss=0.56417]\n",
            "Step 86387   [1.702 sec/step, loss=0.56800, avg_loss=0.56381]\n",
            "Generated 16 batches of size 16 in 8.208 sec\n",
            "Step 86388   [1.747 sec/step, loss=0.48953, avg_loss=0.56279]\n",
            "Step 86389   [1.762 sec/step, loss=0.52382, avg_loss=0.56222]\n",
            "Step 86390   [1.765 sec/step, loss=0.53692, avg_loss=0.56184]\n",
            "Step 86391   [1.753 sec/step, loss=0.54261, avg_loss=0.56170]\n",
            "Step 86392   [1.755 sec/step, loss=0.59958, avg_loss=0.56204]\n",
            "Step 86393   [1.748 sec/step, loss=0.61563, avg_loss=0.56258]\n",
            "Step 86394   [1.742 sec/step, loss=0.57603, avg_loss=0.56281]\n",
            "Step 86395   [1.734 sec/step, loss=0.57252, avg_loss=0.56294]\n",
            "Step 86396   [1.734 sec/step, loss=0.58766, avg_loss=0.56285]\n",
            "Step 86397   [1.735 sec/step, loss=0.58936, avg_loss=0.56295]\n",
            "Step 86398   [1.734 sec/step, loss=0.56149, avg_loss=0.56304]\n",
            "Step 86399   [1.731 sec/step, loss=0.58216, avg_loss=0.56314]\n",
            "Step 86400   [1.733 sec/step, loss=0.56095, avg_loss=0.56304]\n",
            "Writing summary at step: 86400\n",
            "Generated 16 batches of size 16 in 9.141 sec\n",
            "Step 86401   [1.748 sec/step, loss=0.55969, avg_loss=0.56273]\n",
            "Step 86402   [1.742 sec/step, loss=0.58550, avg_loss=0.56343]\n",
            "Step 86403   [1.738 sec/step, loss=0.60074, avg_loss=0.56340]\n",
            "Step 86404   [1.735 sec/step, loss=0.57608, avg_loss=0.56383]\n",
            "Step 86405   [1.730 sec/step, loss=0.60502, avg_loss=0.56407]\n",
            "Step 86406   [1.743 sec/step, loss=0.48768, avg_loss=0.56335]\n",
            "Step 86407   [1.736 sec/step, loss=0.56472, avg_loss=0.56328]\n",
            "Step 86408   [1.736 sec/step, loss=0.55693, avg_loss=0.56294]\n",
            "Step 86409   [1.691 sec/step, loss=0.52560, avg_loss=0.56384]\n",
            "Step 86410   [1.734 sec/step, loss=0.49822, avg_loss=0.56288]\n",
            "Step 86411   [1.738 sec/step, loss=0.58167, avg_loss=0.56305]\n",
            "Step 86412   [1.722 sec/step, loss=0.58001, avg_loss=0.56403]\n",
            "Step 86413   [1.734 sec/step, loss=0.52493, avg_loss=0.56382]\n",
            "Step 86414   [1.717 sec/step, loss=0.57425, avg_loss=0.56413]\n",
            "Step 86415   [1.724 sec/step, loss=0.55187, avg_loss=0.56371]\n",
            "Step 86416   [1.689 sec/step, loss=0.55934, avg_loss=0.56516]\n",
            "Step 86417   [1.685 sec/step, loss=0.58698, avg_loss=0.56538]\n",
            "Step 86418   [1.687 sec/step, loss=0.59246, avg_loss=0.56546]\n",
            "Step 86419   [1.691 sec/step, loss=0.60827, avg_loss=0.56574]\n",
            "Step 86420   [1.698 sec/step, loss=0.57780, avg_loss=0.56584]\n",
            "Generated 16 batches of size 16 in 9.211 sec\n",
            "Step 86421   [1.704 sec/step, loss=0.53278, avg_loss=0.56565]\n",
            "Step 86422   [1.699 sec/step, loss=0.57937, avg_loss=0.56543]\n",
            "Step 86423   [1.702 sec/step, loss=0.58279, avg_loss=0.56537]\n",
            "Step 86424   [1.694 sec/step, loss=0.60369, avg_loss=0.56585]\n",
            "Step 86425   [1.691 sec/step, loss=0.59095, avg_loss=0.56574]\n",
            "Step 86426   [1.690 sec/step, loss=0.58732, avg_loss=0.56591]\n",
            "Step 86427   [1.691 sec/step, loss=0.54855, avg_loss=0.56588]\n",
            "Step 86428   [1.687 sec/step, loss=0.58960, avg_loss=0.56642]\n",
            "Step 86429   [1.700 sec/step, loss=0.54081, avg_loss=0.56599]\n",
            "Step 86430   [1.695 sec/step, loss=0.59376, avg_loss=0.56635]\n",
            "Step 86431   [1.698 sec/step, loss=0.56208, avg_loss=0.56634]\n",
            "Step 86432   [1.699 sec/step, loss=0.58774, avg_loss=0.56650]\n",
            "Step 86433   [1.739 sec/step, loss=0.40496, avg_loss=0.56522]\n",
            "Step 86434   [1.739 sec/step, loss=0.57467, avg_loss=0.56532]\n",
            "Generated 16 batches of size 16 in 9.509 sec\n",
            "Step 86435   [1.747 sec/step, loss=0.55239, avg_loss=0.56474]\n",
            "Step 86436   [1.741 sec/step, loss=0.58025, avg_loss=0.56506]\n",
            "Step 86437   [1.734 sec/step, loss=0.55048, avg_loss=0.56515]\n",
            "Step 86438   [1.730 sec/step, loss=0.52061, avg_loss=0.56491]\n",
            "Step 86439   [1.689 sec/step, loss=0.56525, avg_loss=0.56649]\n",
            "Step 86440   [1.690 sec/step, loss=0.60217, avg_loss=0.56648]\n",
            "Step 86441   [1.696 sec/step, loss=0.54314, avg_loss=0.56604]\n",
            "Step 86442   [1.704 sec/step, loss=0.53590, avg_loss=0.56527]\n",
            "Step 86443   [1.743 sec/step, loss=0.46124, avg_loss=0.56454]\n",
            "Step 86444   [1.756 sec/step, loss=0.51917, avg_loss=0.56393]\n",
            "Step 86445   [1.760 sec/step, loss=0.55146, avg_loss=0.56387]\n",
            "Step 86446   [1.761 sec/step, loss=0.59875, avg_loss=0.56396]\n",
            "Step 86447   [1.752 sec/step, loss=0.59657, avg_loss=0.56409]\n",
            "Step 86448   [1.751 sec/step, loss=0.55919, avg_loss=0.56371]\n",
            "Step 86449   [1.757 sec/step, loss=0.55983, avg_loss=0.56351]\n",
            "Step 86450   [1.757 sec/step, loss=0.58698, avg_loss=0.56392]\n",
            "Step 86451   [1.756 sec/step, loss=0.55200, avg_loss=0.56375]\n",
            "Step 86452   [1.759 sec/step, loss=0.53845, avg_loss=0.56366]\n",
            "Step 86453   [1.756 sec/step, loss=0.58895, avg_loss=0.56395]\n",
            "Step 86454   [1.753 sec/step, loss=0.59309, avg_loss=0.56422]\n",
            "Generated 16 batches of size 16 in 10.155 sec\n",
            "Step 86455   [1.751 sec/step, loss=0.57041, avg_loss=0.56408]\n",
            "Step 86456   [1.738 sec/step, loss=0.59173, avg_loss=0.56442]\n",
            "Step 86457   [1.742 sec/step, loss=0.53114, avg_loss=0.56392]\n",
            "Step 86458   [1.742 sec/step, loss=0.59460, avg_loss=0.56403]\n",
            "Step 86459   [1.704 sec/step, loss=0.56591, avg_loss=0.56484]\n",
            "Step 86460   [1.696 sec/step, loss=0.58620, avg_loss=0.56499]\n",
            "Step 86461   [1.711 sec/step, loss=0.54171, avg_loss=0.56417]\n",
            "Step 86462   [1.710 sec/step, loss=0.58284, avg_loss=0.56432]\n",
            "Step 86463   [1.714 sec/step, loss=0.54135, avg_loss=0.56390]\n",
            "Step 86464   [1.727 sec/step, loss=0.58642, avg_loss=0.56383]\n",
            "Step 86465   [1.726 sec/step, loss=0.56509, avg_loss=0.56350]\n",
            "Step 86466   [1.730 sec/step, loss=0.58540, avg_loss=0.56341]\n",
            "Step 86467   [1.738 sec/step, loss=0.56069, avg_loss=0.56319]\n",
            "Generated 16 batches of size 16 in 10.210 sec\n",
            "Step 86468   [1.777 sec/step, loss=0.52555, avg_loss=0.56290]\n",
            "Step 86469   [1.763 sec/step, loss=0.55669, avg_loss=0.56279]\n",
            "Step 86470   [1.761 sec/step, loss=0.59111, avg_loss=0.56251]\n",
            "Step 86471   [1.749 sec/step, loss=0.56282, avg_loss=0.56223]\n",
            "Step 86472   [1.751 sec/step, loss=0.57139, avg_loss=0.56241]\n",
            "Step 86473   [1.746 sec/step, loss=0.55780, avg_loss=0.56229]\n",
            "Step 86474   [1.733 sec/step, loss=0.57505, avg_loss=0.56283]\n",
            "Step 86475   [1.707 sec/step, loss=0.56831, avg_loss=0.56375]\n",
            "Step 86476   [1.713 sec/step, loss=0.56169, avg_loss=0.56371]\n",
            "Step 86477   [1.712 sec/step, loss=0.55184, avg_loss=0.56348]\n",
            "Step 86478   [1.713 sec/step, loss=0.55436, avg_loss=0.56337]\n",
            "Step 86479   [1.701 sec/step, loss=0.58744, avg_loss=0.56314]\n",
            "Step 86480   [1.714 sec/step, loss=0.55573, avg_loss=0.56288]\n",
            "Step 86481   [1.714 sec/step, loss=0.56626, avg_loss=0.56331]\n",
            "Step 86482   [1.717 sec/step, loss=0.59632, avg_loss=0.56349]\n",
            "Step 86483   [1.715 sec/step, loss=0.58946, avg_loss=0.56329]\n",
            "Step 86484   [1.718 sec/step, loss=0.57514, avg_loss=0.56345]\n",
            "Generated 16 batches of size 16 in 10.039 sec\n",
            "Step 86485   [1.738 sec/step, loss=0.49929, avg_loss=0.56292]\n",
            "Step 86486   [1.736 sec/step, loss=0.62609, avg_loss=0.56415]\n",
            "Step 86487   [1.742 sec/step, loss=0.55910, avg_loss=0.56406]\n",
            "Step 86488   [1.699 sec/step, loss=0.56433, avg_loss=0.56481]\n",
            "Step 86489   [1.689 sec/step, loss=0.57658, avg_loss=0.56533]\n",
            "Step 86490   [1.688 sec/step, loss=0.58638, avg_loss=0.56583]\n",
            "Step 86491   [1.694 sec/step, loss=0.54269, avg_loss=0.56583]\n",
            "Step 86492   [1.691 sec/step, loss=0.56618, avg_loss=0.56550]\n",
            "Step 86493   [1.697 sec/step, loss=0.56462, avg_loss=0.56499]\n",
            "Step 86494   [1.704 sec/step, loss=0.53036, avg_loss=0.56453]\n",
            "Step 86495   [1.710 sec/step, loss=0.55220, avg_loss=0.56433]\n",
            "Step 86496   [1.713 sec/step, loss=0.60162, avg_loss=0.56447]\n",
            "Step 86497   [1.712 sec/step, loss=0.58627, avg_loss=0.56443]\n",
            "Step 86498   [1.751 sec/step, loss=0.41620, avg_loss=0.56298]\n",
            "Step 86499   [1.754 sec/step, loss=0.58089, avg_loss=0.56297]\n",
            "Generated 16 batches of size 16 in 10.015 sec\n",
            "Step 86500   [1.750 sec/step, loss=0.56282, avg_loss=0.56299]\n",
            "Writing summary at step: 86500\n",
            "Step 86501   [1.734 sec/step, loss=0.58894, avg_loss=0.56328]\n",
            "Step 86502   [1.734 sec/step, loss=0.55375, avg_loss=0.56296]\n",
            "Step 86503   [1.738 sec/step, loss=0.56084, avg_loss=0.56256]\n",
            "Step 86504   [1.741 sec/step, loss=0.59134, avg_loss=0.56272]\n",
            "Step 86505   [1.742 sec/step, loss=0.58586, avg_loss=0.56252]\n",
            "Step 86506   [1.732 sec/step, loss=0.60437, avg_loss=0.56369]\n",
            "Step 86507   [1.739 sec/step, loss=0.57078, avg_loss=0.56375]\n",
            "Step 86508   [1.773 sec/step, loss=0.42844, avg_loss=0.56247]\n",
            "Step 86509   [1.771 sec/step, loss=0.57423, avg_loss=0.56295]\n",
            "Step 86510   [1.725 sec/step, loss=0.58839, avg_loss=0.56386]\n",
            "Step 86511   [1.732 sec/step, loss=0.54585, avg_loss=0.56350]\n",
            "Step 86512   [1.745 sec/step, loss=0.51278, avg_loss=0.56282]\n",
            "Step 86513   [1.731 sec/step, loss=0.56754, avg_loss=0.56325]\n",
            "Step 86514   [1.736 sec/step, loss=0.60997, avg_loss=0.56361]\n",
            "Step 86515   [1.729 sec/step, loss=0.60671, avg_loss=0.56416]\n",
            "Step 86516   [1.733 sec/step, loss=0.53945, avg_loss=0.56396]\n",
            "Generated 16 batches of size 16 in 10.009 sec\n",
            "Step 86517   [1.735 sec/step, loss=0.58093, avg_loss=0.56390]\n",
            "Step 86518   [1.735 sec/step, loss=0.56638, avg_loss=0.56364]\n",
            "Step 86519   [1.744 sec/step, loss=0.54647, avg_loss=0.56302]\n",
            "Step 86520   [1.768 sec/step, loss=0.49217, avg_loss=0.56216]\n",
            "Step 86521   [1.755 sec/step, loss=0.63147, avg_loss=0.56315]\n",
            "Step 86522   [1.761 sec/step, loss=0.57892, avg_loss=0.56314]\n",
            "Step 86523   [1.759 sec/step, loss=0.57145, avg_loss=0.56303]\n",
            "Step 86524   [1.770 sec/step, loss=0.50179, avg_loss=0.56201]\n",
            "Step 86525   [1.768 sec/step, loss=0.59018, avg_loss=0.56200]\n",
            "Step 86526   [1.768 sec/step, loss=0.56630, avg_loss=0.56179]\n",
            "Step 86527   [1.772 sec/step, loss=0.52366, avg_loss=0.56154]\n",
            "Step 86528   [1.781 sec/step, loss=0.60072, avg_loss=0.56166]\n",
            "Step 86529   [1.774 sec/step, loss=0.56705, avg_loss=0.56192]\n",
            "Step 86530   [1.777 sec/step, loss=0.59888, avg_loss=0.56197]\n",
            "Step 86531   [1.777 sec/step, loss=0.58050, avg_loss=0.56215]\n",
            "Step 86532   [1.774 sec/step, loss=0.58689, avg_loss=0.56215]\n",
            "Generated 16 batches of size 16 in 10.042 sec\n",
            "Step 86533   [1.725 sec/step, loss=0.58319, avg_loss=0.56393]\n",
            "Step 86534   [1.717 sec/step, loss=0.58956, avg_loss=0.56408]\n",
            "Step 86535   [1.711 sec/step, loss=0.55414, avg_loss=0.56409]\n",
            "Step 86536   [1.718 sec/step, loss=0.56362, avg_loss=0.56393]\n",
            "Step 86537   [1.709 sec/step, loss=0.58196, avg_loss=0.56424]\n",
            "Step 86538   [1.711 sec/step, loss=0.52689, avg_loss=0.56431]\n",
            "Step 86539   [1.707 sec/step, loss=0.56356, avg_loss=0.56429]\n",
            "Step 86540   [1.713 sec/step, loss=0.58131, avg_loss=0.56408]\n",
            "Step 86541   [1.723 sec/step, loss=0.51757, avg_loss=0.56382]\n",
            "Step 86542   [1.713 sec/step, loss=0.58071, avg_loss=0.56427]\n",
            "Step 86543   [1.676 sec/step, loss=0.57395, avg_loss=0.56540]\n",
            "Step 86544   [1.666 sec/step, loss=0.55539, avg_loss=0.56576]\n",
            "Step 86545   [1.670 sec/step, loss=0.55655, avg_loss=0.56581]\n",
            "Generated 16 batches of size 16 in 7.573 sec\n",
            "Step 86546   [1.713 sec/step, loss=0.43585, avg_loss=0.56418]\n",
            "Step 86547   [1.712 sec/step, loss=0.57017, avg_loss=0.56392]\n",
            "Step 86548   [1.719 sec/step, loss=0.56011, avg_loss=0.56393]\n",
            "Step 86549   [1.716 sec/step, loss=0.58733, avg_loss=0.56420]\n",
            "Step 86550   [1.713 sec/step, loss=0.57040, avg_loss=0.56404]\n",
            "Step 86551   [1.713 sec/step, loss=0.57327, avg_loss=0.56425]\n",
            "Step 86552   [1.708 sec/step, loss=0.58610, avg_loss=0.56473]\n",
            "Step 86553   [1.711 sec/step, loss=0.55145, avg_loss=0.56435]\n",
            "Step 86554   [1.711 sec/step, loss=0.59443, avg_loss=0.56437]\n",
            "Step 86555   [1.714 sec/step, loss=0.55375, avg_loss=0.56420]\n",
            "Step 86556   [1.709 sec/step, loss=0.57226, avg_loss=0.56400]\n",
            "Step 86557   [1.705 sec/step, loss=0.59723, avg_loss=0.56466]\n",
            "Step 86558   [1.710 sec/step, loss=0.56490, avg_loss=0.56437]\n",
            "Step 86559   [1.716 sec/step, loss=0.56374, avg_loss=0.56435]\n",
            "Step 86560   [1.719 sec/step, loss=0.59898, avg_loss=0.56447]\n",
            "Generated 16 batches of size 16 in 7.236 sec\n",
            "Step 86561   [1.754 sec/step, loss=0.43869, avg_loss=0.56344]\n",
            "Step 86562   [1.750 sec/step, loss=0.54567, avg_loss=0.56307]\n",
            "Step 86563   [1.746 sec/step, loss=0.58857, avg_loss=0.56354]\n",
            "Step 86564   [1.732 sec/step, loss=0.56327, avg_loss=0.56331]\n",
            "Step 86565   [1.730 sec/step, loss=0.56365, avg_loss=0.56330]\n",
            "Step 86566   [1.743 sec/step, loss=0.49908, avg_loss=0.56244]\n",
            "Step 86567   [1.731 sec/step, loss=0.59168, avg_loss=0.56275]\n",
            "Step 86568   [1.694 sec/step, loss=0.55928, avg_loss=0.56308]\n",
            "Step 86569   [1.690 sec/step, loss=0.55818, avg_loss=0.56310]\n",
            "Step 86570   [1.691 sec/step, loss=0.55735, avg_loss=0.56276]\n",
            "Step 86571   [1.697 sec/step, loss=0.55271, avg_loss=0.56266]\n",
            "Step 86572   [1.694 sec/step, loss=0.60209, avg_loss=0.56297]\n",
            "Step 86573   [1.699 sec/step, loss=0.56180, avg_loss=0.56301]\n",
            "Step 86574   [1.705 sec/step, loss=0.55527, avg_loss=0.56281]\n",
            "Step 86575   [1.691 sec/step, loss=0.58593, avg_loss=0.56298]\n",
            "Step 86576   [1.687 sec/step, loss=0.62273, avg_loss=0.56359]\n",
            "Step 86577   [1.706 sec/step, loss=0.49830, avg_loss=0.56306]\n",
            "Generated 16 batches of size 16 in 7.431 sec\n",
            "Step 86578   [1.712 sec/step, loss=0.58950, avg_loss=0.56341]\n",
            "Step 86579   [1.751 sec/step, loss=0.43352, avg_loss=0.56187]\n",
            "Step 86580   [1.745 sec/step, loss=0.52032, avg_loss=0.56152]\n",
            "Step 86581   [1.739 sec/step, loss=0.61578, avg_loss=0.56201]\n",
            "Step 86582   [1.737 sec/step, loss=0.57464, avg_loss=0.56180]\n",
            "Step 86583   [1.736 sec/step, loss=0.60174, avg_loss=0.56192]\n",
            "Step 86584   [1.730 sec/step, loss=0.54750, avg_loss=0.56164]\n",
            "Step 86585   [1.702 sec/step, loss=0.54624, avg_loss=0.56211]\n",
            "Step 86586   [1.737 sec/step, loss=0.44170, avg_loss=0.56027]\n",
            "Step 86587   [1.731 sec/step, loss=0.53445, avg_loss=0.56002]\n",
            "Step 86588   [1.729 sec/step, loss=0.55495, avg_loss=0.55993]\n",
            "Step 86589   [1.729 sec/step, loss=0.59103, avg_loss=0.56007]\n",
            "Step 86590   [1.727 sec/step, loss=0.58888, avg_loss=0.56010]\n",
            "Step 86591   [1.723 sec/step, loss=0.60703, avg_loss=0.56074]\n",
            "Step 86592   [1.726 sec/step, loss=0.56478, avg_loss=0.56073]\n",
            "Step 86593   [1.722 sec/step, loss=0.58750, avg_loss=0.56096]\n",
            "Step 86594   [1.718 sec/step, loss=0.58117, avg_loss=0.56146]\n",
            "Step 86595   [1.712 sec/step, loss=0.57238, avg_loss=0.56167]\n",
            "Generated 16 batches of size 16 in 9.048 sec\n",
            "Step 86596   [1.726 sec/step, loss=0.56177, avg_loss=0.56127]\n",
            "Step 86597   [1.731 sec/step, loss=0.52931, avg_loss=0.56070]\n",
            "Step 86598   [1.695 sec/step, loss=0.56063, avg_loss=0.56214]\n",
            "Step 86599   [1.694 sec/step, loss=0.56164, avg_loss=0.56195]\n",
            "Step 86600   [1.702 sec/step, loss=0.59136, avg_loss=0.56223]\n",
            "Writing summary at step: 86600\n",
            "Step 86601   [1.704 sec/step, loss=0.58445, avg_loss=0.56219]\n",
            "Step 86602   [1.706 sec/step, loss=0.57943, avg_loss=0.56245]\n",
            "Step 86603   [1.721 sec/step, loss=0.53031, avg_loss=0.56214]\n",
            "Step 86604   [1.719 sec/step, loss=0.57845, avg_loss=0.56201]\n",
            "Step 86605   [1.718 sec/step, loss=0.62109, avg_loss=0.56236]\n",
            "Step 86606   [1.722 sec/step, loss=0.55756, avg_loss=0.56190]\n",
            "Step 86607   [1.722 sec/step, loss=0.58791, avg_loss=0.56207]\n",
            "Step 86608   [1.687 sec/step, loss=0.58952, avg_loss=0.56368]\n",
            "Step 86609   [1.689 sec/step, loss=0.60029, avg_loss=0.56394]\n",
            "Step 86610   [1.695 sec/step, loss=0.58061, avg_loss=0.56386]\n",
            "Generated 16 batches of size 16 in 8.728 sec\n",
            "Step 86611   [1.730 sec/step, loss=0.49940, avg_loss=0.56340]\n",
            "Step 86612   [1.727 sec/step, loss=0.53783, avg_loss=0.56365]\n",
            "Step 86613   [1.730 sec/step, loss=0.52971, avg_loss=0.56327]\n",
            "Step 86614   [1.729 sec/step, loss=0.55523, avg_loss=0.56272]\n",
            "Step 86615   [1.731 sec/step, loss=0.57707, avg_loss=0.56242]\n",
            "Step 86616   [1.733 sec/step, loss=0.55672, avg_loss=0.56260]\n",
            "Step 86617   [1.735 sec/step, loss=0.60226, avg_loss=0.56281]\n",
            "Step 86618   [1.734 sec/step, loss=0.59714, avg_loss=0.56312]\n",
            "Step 86619   [1.721 sec/step, loss=0.58487, avg_loss=0.56350]\n",
            "Step 86620   [1.696 sec/step, loss=0.58362, avg_loss=0.56442]\n",
            "Step 86621   [1.727 sec/step, loss=0.41542, avg_loss=0.56226]\n",
            "Step 86622   [1.727 sec/step, loss=0.55023, avg_loss=0.56197]\n",
            "Step 86623   [1.738 sec/step, loss=0.58227, avg_loss=0.56208]\n",
            "Step 86624   [1.725 sec/step, loss=0.56524, avg_loss=0.56271]\n",
            "Step 86625   [1.730 sec/step, loss=0.56305, avg_loss=0.56244]\n",
            "Step 86626   [1.740 sec/step, loss=0.53600, avg_loss=0.56214]\n",
            "Generated 16 batches of size 16 in 9.327 sec\n",
            "Step 86627   [1.726 sec/step, loss=0.54727, avg_loss=0.56237]\n",
            "Step 86628   [1.716 sec/step, loss=0.55135, avg_loss=0.56188]\n",
            "Step 86629   [1.711 sec/step, loss=0.57587, avg_loss=0.56197]\n",
            "Step 86630   [1.711 sec/step, loss=0.59211, avg_loss=0.56190]\n",
            "Step 86631   [1.709 sec/step, loss=0.56568, avg_loss=0.56175]\n",
            "Step 86632   [1.717 sec/step, loss=0.54249, avg_loss=0.56131]\n",
            "Step 86633   [1.719 sec/step, loss=0.55760, avg_loss=0.56105]\n",
            "Step 86634   [1.729 sec/step, loss=0.56925, avg_loss=0.56085]\n",
            "Step 86635   [1.731 sec/step, loss=0.58443, avg_loss=0.56115]\n",
            "Step 86636   [1.725 sec/step, loss=0.56789, avg_loss=0.56120]\n",
            "Step 86637   [1.767 sec/step, loss=0.44288, avg_loss=0.55980]\n",
            "Step 86638   [1.765 sec/step, loss=0.59031, avg_loss=0.56044]\n",
            "Step 86639   [1.769 sec/step, loss=0.60869, avg_loss=0.56089]\n",
            "Step 86640   [1.770 sec/step, loss=0.58107, avg_loss=0.56089]\n",
            "Step 86641   [1.774 sec/step, loss=0.51450, avg_loss=0.56086]\n",
            "Step 86642   [1.782 sec/step, loss=0.56800, avg_loss=0.56073]\n",
            "Generated 16 batches of size 16 in 9.933 sec\n",
            "Step 86643   [1.781 sec/step, loss=0.56115, avg_loss=0.56060]\n",
            "Step 86644   [1.782 sec/step, loss=0.57599, avg_loss=0.56081]\n",
            "Step 86645   [1.775 sec/step, loss=0.57649, avg_loss=0.56101]\n",
            "Step 86646   [1.730 sec/step, loss=0.57085, avg_loss=0.56236]\n",
            "Step 86647   [1.730 sec/step, loss=0.60481, avg_loss=0.56270]\n",
            "Step 86648   [1.762 sec/step, loss=0.44474, avg_loss=0.56155]\n",
            "Step 86649   [1.759 sec/step, loss=0.55868, avg_loss=0.56126]\n",
            "Step 86650   [1.756 sec/step, loss=0.58013, avg_loss=0.56136]\n",
            "Step 86651   [1.761 sec/step, loss=0.62115, avg_loss=0.56184]\n",
            "Step 86652   [1.770 sec/step, loss=0.54403, avg_loss=0.56142]\n",
            "Step 86653   [1.771 sec/step, loss=0.56666, avg_loss=0.56157]\n",
            "Step 86654   [1.771 sec/step, loss=0.56141, avg_loss=0.56124]\n",
            "Step 86655   [1.770 sec/step, loss=0.55995, avg_loss=0.56130]\n",
            "Step 86656   [1.775 sec/step, loss=0.52206, avg_loss=0.56080]\n",
            "Step 86657   [1.776 sec/step, loss=0.59766, avg_loss=0.56081]\n",
            "Step 86658   [1.775 sec/step, loss=0.59453, avg_loss=0.56110]\n",
            "Generated 16 batches of size 16 in 10.328 sec\n",
            "Step 86659   [1.785 sec/step, loss=0.50390, avg_loss=0.56050]\n",
            "Step 86660   [1.785 sec/step, loss=0.57550, avg_loss=0.56027]\n",
            "Step 86661   [1.736 sec/step, loss=0.57857, avg_loss=0.56167]\n",
            "Step 86662   [1.738 sec/step, loss=0.59628, avg_loss=0.56217]\n",
            "Step 86663   [1.743 sec/step, loss=0.58003, avg_loss=0.56209]\n",
            "Step 86664   [1.749 sec/step, loss=0.56175, avg_loss=0.56207]\n",
            "Step 86665   [1.753 sec/step, loss=0.58346, avg_loss=0.56227]\n",
            "Step 86666   [1.753 sec/step, loss=0.49385, avg_loss=0.56222]\n",
            "Step 86667   [1.749 sec/step, loss=0.59880, avg_loss=0.56229]\n",
            "Step 86668   [1.752 sec/step, loss=0.59178, avg_loss=0.56261]\n",
            "Step 86669   [1.753 sec/step, loss=0.59795, avg_loss=0.56301]\n",
            "Step 86670   [1.751 sec/step, loss=0.60000, avg_loss=0.56344]\n",
            "Step 86671   [1.746 sec/step, loss=0.56764, avg_loss=0.56359]\n",
            "Step 86672   [1.750 sec/step, loss=0.57778, avg_loss=0.56335]\n",
            "Step 86673   [1.752 sec/step, loss=0.54233, avg_loss=0.56315]\n",
            "Generated 16 batches of size 16 in 10.324 sec\n",
            "Step 86674   [1.792 sec/step, loss=0.40187, avg_loss=0.56162]\n",
            "Step 86675   [1.790 sec/step, loss=0.59382, avg_loss=0.56170]\n",
            "Step 86676   [1.789 sec/step, loss=0.57305, avg_loss=0.56120]\n",
            "Step 86677   [1.769 sec/step, loss=0.57375, avg_loss=0.56195]\n",
            "Step 86678   [1.770 sec/step, loss=0.53295, avg_loss=0.56139]\n",
            "Step 86679   [1.732 sec/step, loss=0.60481, avg_loss=0.56310]\n",
            "Step 86680   [1.726 sec/step, loss=0.54648, avg_loss=0.56336]\n",
            "Step 86681   [1.732 sec/step, loss=0.55514, avg_loss=0.56276]\n",
            "Step 86682   [1.734 sec/step, loss=0.58422, avg_loss=0.56285]\n",
            "Step 86683   [1.735 sec/step, loss=0.58861, avg_loss=0.56272]\n",
            "Step 86684   [1.732 sec/step, loss=0.57803, avg_loss=0.56303]\n",
            "Step 86685   [1.767 sec/step, loss=0.47467, avg_loss=0.56231]\n",
            "Step 86686   [1.751 sec/step, loss=0.54396, avg_loss=0.56333]\n",
            "Step 86687   [1.754 sec/step, loss=0.60906, avg_loss=0.56408]\n",
            "Step 86688   [1.759 sec/step, loss=0.57037, avg_loss=0.56423]\n",
            "Step 86689   [1.753 sec/step, loss=0.59711, avg_loss=0.56429]\n",
            "Step 86690   [1.758 sec/step, loss=0.58024, avg_loss=0.56421]\n",
            "Generated 16 batches of size 16 in 10.118 sec\n",
            "Step 86691   [1.767 sec/step, loss=0.54624, avg_loss=0.56360]\n",
            "Step 86692   [1.763 sec/step, loss=0.60075, avg_loss=0.56396]\n",
            "Step 86693   [1.763 sec/step, loss=0.58604, avg_loss=0.56394]\n",
            "Step 86694   [1.771 sec/step, loss=0.56860, avg_loss=0.56382]\n",
            "Step 86695   [1.766 sec/step, loss=0.57745, avg_loss=0.56387]\n",
            "Step 86696   [1.761 sec/step, loss=0.53365, avg_loss=0.56359]\n",
            "Step 86697   [1.763 sec/step, loss=0.58277, avg_loss=0.56412]\n",
            "Step 86698   [1.763 sec/step, loss=0.54198, avg_loss=0.56394]\n",
            "Step 86699   [1.764 sec/step, loss=0.53450, avg_loss=0.56366]\n",
            "Step 86700   [1.757 sec/step, loss=0.56261, avg_loss=0.56338]\n",
            "Writing summary at step: 86700\n",
            "Step 86701   [1.758 sec/step, loss=0.58236, avg_loss=0.56336]\n",
            "Step 86702   [1.787 sec/step, loss=0.54448, avg_loss=0.56301]\n",
            "Step 86703   [1.770 sec/step, loss=0.59069, avg_loss=0.56361]\n",
            "Step 86704   [1.771 sec/step, loss=0.53964, avg_loss=0.56322]\n",
            "Step 86705   [1.780 sec/step, loss=0.55694, avg_loss=0.56258]\n",
            "Generated 16 batches of size 16 in 10.528 sec\n",
            "Step 86706   [1.772 sec/step, loss=0.61019, avg_loss=0.56311]\n",
            "Step 86707   [1.772 sec/step, loss=0.58130, avg_loss=0.56304]\n",
            "Step 86708   [1.769 sec/step, loss=0.58266, avg_loss=0.56297]\n",
            "Step 86709   [1.769 sec/step, loss=0.57536, avg_loss=0.56272]\n",
            "Step 86710   [1.768 sec/step, loss=0.56575, avg_loss=0.56257]\n",
            "Step 86711   [1.769 sec/step, loss=0.46856, avg_loss=0.56227]\n",
            "Step 86712   [1.776 sec/step, loss=0.51401, avg_loss=0.56203]\n",
            "Step 86713   [1.776 sec/step, loss=0.53964, avg_loss=0.56213]\n",
            "Step 86714   [1.776 sec/step, loss=0.61004, avg_loss=0.56268]\n",
            "Step 86715   [1.776 sec/step, loss=0.58775, avg_loss=0.56278]\n",
            "Step 86716   [1.770 sec/step, loss=0.58497, avg_loss=0.56307]\n",
            "Step 86717   [1.765 sec/step, loss=0.56568, avg_loss=0.56270]\n",
            "Step 86718   [1.774 sec/step, loss=0.58425, avg_loss=0.56257]\n",
            "Step 86719   [1.785 sec/step, loss=0.56968, avg_loss=0.56242]\n",
            "Step 86720   [1.781 sec/step, loss=0.61209, avg_loss=0.56270]\n",
            "Step 86721   [1.752 sec/step, loss=0.55737, avg_loss=0.56412]\n",
            "Generated 16 batches of size 16 in 10.146 sec\n",
            "Step 86722   [1.760 sec/step, loss=0.54406, avg_loss=0.56406]\n",
            "Step 86723   [1.744 sec/step, loss=0.52639, avg_loss=0.56350]\n",
            "Step 86724   [1.740 sec/step, loss=0.58601, avg_loss=0.56371]\n",
            "Step 86725   [1.736 sec/step, loss=0.58845, avg_loss=0.56396]\n",
            "Step 86726   [1.737 sec/step, loss=0.53298, avg_loss=0.56393]\n",
            "Step 86727   [1.743 sec/step, loss=0.60466, avg_loss=0.56451]\n",
            "Step 86728   [1.743 sec/step, loss=0.60199, avg_loss=0.56501]\n",
            "Step 86729   [1.749 sec/step, loss=0.53304, avg_loss=0.56459]\n",
            "Step 86730   [1.763 sec/step, loss=0.50036, avg_loss=0.56367]\n",
            "Step 86731   [1.761 sec/step, loss=0.58001, avg_loss=0.56381]\n",
            "Step 86732   [1.784 sec/step, loss=0.54506, avg_loss=0.56384]\n",
            "Step 86733   [1.784 sec/step, loss=0.56845, avg_loss=0.56395]\n",
            "Step 86734   [1.781 sec/step, loss=0.57007, avg_loss=0.56395]\n",
            "Step 86735   [1.779 sec/step, loss=0.57014, avg_loss=0.56381]\n",
            "Step 86736   [1.783 sec/step, loss=0.59573, avg_loss=0.56409]\n",
            "Step 86737   [1.750 sec/step, loss=0.54891, avg_loss=0.56515]\n",
            "Generated 16 batches of size 16 in 8.821 sec\n",
            "Step 86738   [1.750 sec/step, loss=0.59208, avg_loss=0.56517]\n",
            "Step 86739   [1.750 sec/step, loss=0.57777, avg_loss=0.56486]\n",
            "Step 86740   [1.744 sec/step, loss=0.59111, avg_loss=0.56496]\n",
            "Step 86741   [1.724 sec/step, loss=0.59014, avg_loss=0.56571]\n",
            "Step 86742   [1.715 sec/step, loss=0.57110, avg_loss=0.56575]\n",
            "Step 86743   [1.710 sec/step, loss=0.57080, avg_loss=0.56584]\n",
            "Step 86744   [1.709 sec/step, loss=0.51946, avg_loss=0.56528]\n",
            "Step 86745   [1.712 sec/step, loss=0.56407, avg_loss=0.56515]\n",
            "Step 86746   [1.715 sec/step, loss=0.58179, avg_loss=0.56526]\n",
            "Step 86747   [1.721 sec/step, loss=0.57426, avg_loss=0.56496]\n",
            "Step 86748   [1.681 sec/step, loss=0.59003, avg_loss=0.56641]\n",
            "Step 86749   [1.741 sec/step, loss=0.42169, avg_loss=0.56504]\n",
            "Generated 16 batches of size 16 in 7.455 sec\n",
            "Step 86750   [1.748 sec/step, loss=0.61578, avg_loss=0.56540]\n",
            "Step 86751   [1.741 sec/step, loss=0.60036, avg_loss=0.56519]\n",
            "Step 86752   [1.735 sec/step, loss=0.60297, avg_loss=0.56578]\n",
            "Step 86753   [1.743 sec/step, loss=0.48524, avg_loss=0.56496]\n",
            "Step 86754   [1.742 sec/step, loss=0.58348, avg_loss=0.56518]\n",
            "Step 86755   [1.740 sec/step, loss=0.59746, avg_loss=0.56556]\n",
            "Step 86756   [1.742 sec/step, loss=0.55312, avg_loss=0.56587]\n",
            "Step 86757   [1.740 sec/step, loss=0.62550, avg_loss=0.56615]\n",
            "Step 86758   [1.744 sec/step, loss=0.58606, avg_loss=0.56606]\n",
            "Step 86759   [1.733 sec/step, loss=0.56514, avg_loss=0.56668]\n",
            "Step 86760   [1.746 sec/step, loss=0.53485, avg_loss=0.56627]\n",
            "Step 86761   [1.785 sec/step, loss=0.43339, avg_loss=0.56482]\n",
            "Step 86762   [1.785 sec/step, loss=0.56820, avg_loss=0.56454]\n",
            "Step 86763   [1.784 sec/step, loss=0.57897, avg_loss=0.56453]\n",
            "Step 86764   [1.779 sec/step, loss=0.59258, avg_loss=0.56483]\n",
            "Step 86765   [1.784 sec/step, loss=0.52670, avg_loss=0.56427]\n",
            "Step 86766   [1.773 sec/step, loss=0.56207, avg_loss=0.56495]\n",
            "Step 86767   [1.783 sec/step, loss=0.55213, avg_loss=0.56448]\n",
            "Step 86768   [1.785 sec/step, loss=0.57634, avg_loss=0.56433]\n",
            "Generated 16 batches of size 16 in 7.328 sec\n",
            "Step 86769   [1.789 sec/step, loss=0.56726, avg_loss=0.56402]\n",
            "Step 86770   [1.788 sec/step, loss=0.55445, avg_loss=0.56357]\n",
            "Step 86771   [1.785 sec/step, loss=0.59954, avg_loss=0.56388]\n",
            "Step 86772   [1.777 sec/step, loss=0.62012, avg_loss=0.56431]\n",
            "Step 86773   [1.772 sec/step, loss=0.59964, avg_loss=0.56488]\n",
            "Step 86774   [1.740 sec/step, loss=0.53756, avg_loss=0.56624]\n",
            "Step 86775   [1.749 sec/step, loss=0.57035, avg_loss=0.56600]\n",
            "Step 86776   [1.751 sec/step, loss=0.57902, avg_loss=0.56606]\n",
            "Step 86777   [1.749 sec/step, loss=0.58856, avg_loss=0.56621]\n",
            "Step 86778   [1.747 sec/step, loss=0.53441, avg_loss=0.56623]\n",
            "Step 86779   [1.749 sec/step, loss=0.57549, avg_loss=0.56593]\n",
            "Step 86780   [1.793 sec/step, loss=0.45236, avg_loss=0.56499]\n",
            "Step 86781   [1.792 sec/step, loss=0.60319, avg_loss=0.56547]\n",
            "Step 86782   [1.793 sec/step, loss=0.59912, avg_loss=0.56562]\n",
            "Step 86783   [1.793 sec/step, loss=0.57987, avg_loss=0.56553]\n",
            "Step 86784   [1.805 sec/step, loss=0.58762, avg_loss=0.56563]\n",
            "Step 86785   [1.765 sec/step, loss=0.59432, avg_loss=0.56683]\n",
            "Generated 16 batches of size 16 in 8.506 sec\n",
            "Step 86786   [1.754 sec/step, loss=0.57692, avg_loss=0.56716]\n",
            "Step 86787   [1.752 sec/step, loss=0.49239, avg_loss=0.56599]\n",
            "Step 86788   [1.748 sec/step, loss=0.59058, avg_loss=0.56619]\n",
            "Step 86789   [1.747 sec/step, loss=0.58998, avg_loss=0.56612]\n",
            "Step 86790   [1.751 sec/step, loss=0.57599, avg_loss=0.56608]\n",
            "Step 86791   [1.780 sec/step, loss=0.47685, avg_loss=0.56538]\n",
            "Step 86792   [1.781 sec/step, loss=0.58588, avg_loss=0.56523]\n",
            "Step 86793   [1.793 sec/step, loss=0.50779, avg_loss=0.56445]\n",
            "Step 86794   [1.784 sec/step, loss=0.56713, avg_loss=0.56444]\n",
            "Step 86795   [1.788 sec/step, loss=0.54574, avg_loss=0.56412]\n",
            "Step 86796   [1.774 sec/step, loss=0.60730, avg_loss=0.56486]\n",
            "Step 86797   [1.773 sec/step, loss=0.58310, avg_loss=0.56486]\n",
            "Step 86798   [1.768 sec/step, loss=0.59298, avg_loss=0.56537]\n",
            "Step 86799   [1.771 sec/step, loss=0.60784, avg_loss=0.56610]\n",
            "Step 86800   [1.778 sec/step, loss=0.54268, avg_loss=0.56590]\n",
            "Writing summary at step: 86800\n",
            "Generated 16 batches of size 16 in 8.713 sec\n",
            "Step 86801   [1.776 sec/step, loss=0.60150, avg_loss=0.56610]\n",
            "Step 86802   [1.744 sec/step, loss=0.58020, avg_loss=0.56645]\n",
            "Step 86803   [1.739 sec/step, loss=0.57811, avg_loss=0.56633]\n",
            "Step 86804   [1.739 sec/step, loss=0.57141, avg_loss=0.56664]\n",
            "Step 86805   [1.732 sec/step, loss=0.56271, avg_loss=0.56670]\n",
            "Step 86806   [1.732 sec/step, loss=0.57848, avg_loss=0.56639]\n",
            "Step 86807   [1.728 sec/step, loss=0.55610, avg_loss=0.56613]\n",
            "Step 86808   [1.741 sec/step, loss=0.51529, avg_loss=0.56546]\n",
            "Step 86809   [1.739 sec/step, loss=0.60813, avg_loss=0.56579]\n",
            "Step 86810   [1.733 sec/step, loss=0.56237, avg_loss=0.56575]\n",
            "Step 86811   [1.688 sec/step, loss=0.58077, avg_loss=0.56688]\n",
            "Step 86812   [1.682 sec/step, loss=0.59018, avg_loss=0.56764]\n",
            "Step 86813   [1.681 sec/step, loss=0.59192, avg_loss=0.56816]\n",
            "Step 86814   [1.690 sec/step, loss=0.55292, avg_loss=0.56759]\n",
            "Step 86815   [1.702 sec/step, loss=0.52893, avg_loss=0.56700]\n",
            "Generated 16 batches of size 16 in 8.705 sec\n",
            "Step 86816   [1.704 sec/step, loss=0.57497, avg_loss=0.56690]\n",
            "Step 86817   [1.704 sec/step, loss=0.62181, avg_loss=0.56746]\n",
            "Step 86818   [1.697 sec/step, loss=0.52458, avg_loss=0.56687]\n",
            "Step 86819   [1.723 sec/step, loss=0.47710, avg_loss=0.56594]\n",
            "Step 86820   [1.726 sec/step, loss=0.54320, avg_loss=0.56525]\n",
            "Step 86821   [1.724 sec/step, loss=0.61348, avg_loss=0.56581]\n",
            "Step 86822   [1.708 sec/step, loss=0.60900, avg_loss=0.56646]\n",
            "Step 86823   [1.713 sec/step, loss=0.58856, avg_loss=0.56708]\n",
            "Step 86824   [1.713 sec/step, loss=0.58255, avg_loss=0.56705]\n",
            "Step 86825   [1.724 sec/step, loss=0.51858, avg_loss=0.56635]\n",
            "Step 86826   [1.724 sec/step, loss=0.55261, avg_loss=0.56655]\n",
            "Step 86827   [1.719 sec/step, loss=0.57588, avg_loss=0.56626]\n",
            "Step 86828   [1.720 sec/step, loss=0.57689, avg_loss=0.56601]\n",
            "Step 86829   [1.723 sec/step, loss=0.56942, avg_loss=0.56637]\n",
            "Step 86830   [1.709 sec/step, loss=0.59196, avg_loss=0.56729]\n",
            "Step 86831   [1.718 sec/step, loss=0.54359, avg_loss=0.56692]\n",
            "Step 86832   [1.696 sec/step, loss=0.55338, avg_loss=0.56701]\n",
            "Generated 16 batches of size 16 in 9.514 sec\n",
            "Step 86833   [1.696 sec/step, loss=0.57526, avg_loss=0.56707]\n",
            "Step 86834   [1.690 sec/step, loss=0.58129, avg_loss=0.56719]\n",
            "Step 86835   [1.704 sec/step, loss=0.58354, avg_loss=0.56732]\n",
            "Step 86836   [1.737 sec/step, loss=0.48141, avg_loss=0.56618]\n",
            "Step 86837   [1.743 sec/step, loss=0.53586, avg_loss=0.56605]\n",
            "Step 86838   [1.739 sec/step, loss=0.58571, avg_loss=0.56598]\n",
            "Step 86839   [1.740 sec/step, loss=0.58751, avg_loss=0.56608]\n",
            "Step 86840   [1.741 sec/step, loss=0.55774, avg_loss=0.56575]\n",
            "Step 86841   [1.739 sec/step, loss=0.53071, avg_loss=0.56515]\n",
            "Step 86842   [1.747 sec/step, loss=0.58569, avg_loss=0.56530]\n",
            "Step 86843   [1.748 sec/step, loss=0.58942, avg_loss=0.56548]\n",
            "Step 86844   [1.745 sec/step, loss=0.58001, avg_loss=0.56609]\n",
            "Step 86845   [1.749 sec/step, loss=0.60973, avg_loss=0.56655]\n",
            "Step 86846   [1.753 sec/step, loss=0.61480, avg_loss=0.56688]\n",
            "Step 86847   [1.759 sec/step, loss=0.58143, avg_loss=0.56695]\n",
            "Step 86848   [1.761 sec/step, loss=0.57168, avg_loss=0.56676]\n",
            "Step 86849   [1.711 sec/step, loss=0.53622, avg_loss=0.56791]\n",
            "Generated 16 batches of size 16 in 10.333 sec\n",
            "Step 86850   [1.705 sec/step, loss=0.56721, avg_loss=0.56742]\n",
            "Step 86851   [1.746 sec/step, loss=0.36290, avg_loss=0.56505]\n",
            "Step 86852   [1.754 sec/step, loss=0.56321, avg_loss=0.56465]\n",
            "Step 86853   [1.739 sec/step, loss=0.54772, avg_loss=0.56528]\n",
            "Step 86854   [1.736 sec/step, loss=0.56235, avg_loss=0.56507]\n",
            "Step 86855   [1.733 sec/step, loss=0.56651, avg_loss=0.56476]\n",
            "Step 86856   [1.724 sec/step, loss=0.58401, avg_loss=0.56507]\n",
            "Step 86857   [1.728 sec/step, loss=0.53474, avg_loss=0.56416]\n",
            "Step 86858   [1.721 sec/step, loss=0.58110, avg_loss=0.56411]\n",
            "Step 86859   [1.719 sec/step, loss=0.56549, avg_loss=0.56411]\n",
            "Step 86860   [1.707 sec/step, loss=0.60735, avg_loss=0.56484]\n",
            "Step 86861   [1.671 sec/step, loss=0.56570, avg_loss=0.56616]\n",
            "Step 86862   [1.682 sec/step, loss=0.57503, avg_loss=0.56623]\n",
            "Step 86863   [1.695 sec/step, loss=0.51894, avg_loss=0.56563]\n",
            "Generated 16 batches of size 16 in 10.212 sec\n",
            "Step 86864   [1.709 sec/step, loss=0.54393, avg_loss=0.56514]\n",
            "Step 86865   [1.701 sec/step, loss=0.61140, avg_loss=0.56599]\n",
            "Step 86866   [1.737 sec/step, loss=0.41538, avg_loss=0.56452]\n",
            "Step 86867   [1.731 sec/step, loss=0.57475, avg_loss=0.56475]\n",
            "Step 86868   [1.728 sec/step, loss=0.58226, avg_loss=0.56481]\n",
            "Step 86869   [1.724 sec/step, loss=0.56699, avg_loss=0.56480]\n",
            "Step 86870   [1.726 sec/step, loss=0.59126, avg_loss=0.56517]\n",
            "Step 86871   [1.731 sec/step, loss=0.54210, avg_loss=0.56460]\n",
            "Step 86872   [1.732 sec/step, loss=0.59665, avg_loss=0.56436]\n",
            "Step 86873   [1.728 sec/step, loss=0.58045, avg_loss=0.56417]\n",
            "Step 86874   [1.720 sec/step, loss=0.58864, avg_loss=0.56468]\n",
            "Step 86875   [1.713 sec/step, loss=0.60580, avg_loss=0.56504]\n",
            "Step 86876   [1.720 sec/step, loss=0.53854, avg_loss=0.56463]\n",
            "Step 86877   [1.769 sec/step, loss=0.39870, avg_loss=0.56273]\n",
            "Step 86878   [1.763 sec/step, loss=0.53519, avg_loss=0.56274]\n",
            "Generated 16 batches of size 16 in 10.109 sec\n",
            "Step 86879   [1.775 sec/step, loss=0.55138, avg_loss=0.56250]\n",
            "Step 86880   [1.736 sec/step, loss=0.53933, avg_loss=0.56337]\n",
            "Step 86881   [1.732 sec/step, loss=0.60193, avg_loss=0.56336]\n",
            "Step 86882   [1.731 sec/step, loss=0.60220, avg_loss=0.56339]\n",
            "Step 86883   [1.736 sec/step, loss=0.53867, avg_loss=0.56298]\n",
            "Step 86884   [1.733 sec/step, loss=0.56416, avg_loss=0.56274]\n",
            "Step 86885   [1.737 sec/step, loss=0.58766, avg_loss=0.56267]\n",
            "Step 86886   [1.735 sec/step, loss=0.57870, avg_loss=0.56269]\n",
            "Step 86887   [1.732 sec/step, loss=0.60197, avg_loss=0.56379]\n",
            "Step 86888   [1.733 sec/step, loss=0.56384, avg_loss=0.56352]\n",
            "Step 86889   [1.744 sec/step, loss=0.54527, avg_loss=0.56307]\n",
            "Step 86890   [1.739 sec/step, loss=0.57765, avg_loss=0.56309]\n",
            "Step 86891   [1.700 sec/step, loss=0.59608, avg_loss=0.56428]\n",
            "Step 86892   [1.706 sec/step, loss=0.57100, avg_loss=0.56413]\n",
            "Step 86893   [1.741 sec/step, loss=0.40811, avg_loss=0.56314]\n",
            "Step 86894   [1.748 sec/step, loss=0.55270, avg_loss=0.56299]\n",
            "Generated 16 batches of size 16 in 10.408 sec\n",
            "Step 86895   [1.744 sec/step, loss=0.57644, avg_loss=0.56330]\n",
            "Step 86896   [1.743 sec/step, loss=0.56045, avg_loss=0.56283]\n",
            "Step 86897   [1.745 sec/step, loss=0.54591, avg_loss=0.56246]\n",
            "Step 86898   [1.741 sec/step, loss=0.58899, avg_loss=0.56242]\n",
            "Step 86899   [1.736 sec/step, loss=0.55927, avg_loss=0.56193]\n",
            "Step 86900   [1.728 sec/step, loss=0.55571, avg_loss=0.56206]\n",
            "Writing summary at step: 86900\n",
            "Step 86901   [1.733 sec/step, loss=0.55891, avg_loss=0.56164]\n",
            "Step 86902   [1.736 sec/step, loss=0.59477, avg_loss=0.56178]\n",
            "Step 86903   [1.755 sec/step, loss=0.49161, avg_loss=0.56092]\n",
            "Step 86904   [1.752 sec/step, loss=0.56041, avg_loss=0.56081]\n",
            "Step 86905   [1.752 sec/step, loss=0.58280, avg_loss=0.56101]\n",
            "Step 86906   [1.758 sec/step, loss=0.51340, avg_loss=0.56036]\n",
            "Step 86907   [1.761 sec/step, loss=0.58727, avg_loss=0.56067]\n",
            "Step 86908   [1.760 sec/step, loss=0.53892, avg_loss=0.56091]\n",
            "Step 86909   [1.770 sec/step, loss=0.57392, avg_loss=0.56056]\n",
            "Step 86910   [1.773 sec/step, loss=0.61326, avg_loss=0.56107]\n",
            "Step 86911   [1.779 sec/step, loss=0.54769, avg_loss=0.56074]\n",
            "Generated 16 batches of size 16 in 9.828 sec\n",
            "Step 86912   [1.772 sec/step, loss=0.58337, avg_loss=0.56067]\n",
            "Step 86913   [1.768 sec/step, loss=0.61804, avg_loss=0.56094]\n",
            "Step 86914   [1.757 sec/step, loss=0.58389, avg_loss=0.56125]\n",
            "Step 86915   [1.743 sec/step, loss=0.55507, avg_loss=0.56151]\n",
            "Step 86916   [1.743 sec/step, loss=0.58164, avg_loss=0.56157]\n",
            "Step 86917   [1.782 sec/step, loss=0.42036, avg_loss=0.55956]\n",
            "Step 86918   [1.779 sec/step, loss=0.57516, avg_loss=0.56006]\n",
            "Step 86919   [1.740 sec/step, loss=0.54624, avg_loss=0.56076]\n",
            "Step 86920   [1.738 sec/step, loss=0.57355, avg_loss=0.56106]\n",
            "Step 86921   [1.742 sec/step, loss=0.55064, avg_loss=0.56043]\n",
            "Step 86922   [1.744 sec/step, loss=0.58250, avg_loss=0.56017]\n",
            "Step 86923   [1.744 sec/step, loss=0.57779, avg_loss=0.56006]\n",
            "Step 86924   [1.747 sec/step, loss=0.57234, avg_loss=0.55996]\n",
            "Step 86925   [1.737 sec/step, loss=0.58420, avg_loss=0.56061]\n",
            "Step 86926   [1.732 sec/step, loss=0.57753, avg_loss=0.56086]\n",
            "Step 86927   [1.738 sec/step, loss=0.58094, avg_loss=0.56091]\n",
            "Step 86928   [1.744 sec/step, loss=0.59057, avg_loss=0.56105]\n",
            "Generated 16 batches of size 16 in 10.215 sec\n",
            "Step 86929   [1.745 sec/step, loss=0.52161, avg_loss=0.56057]\n",
            "Step 86930   [1.756 sec/step, loss=0.52139, avg_loss=0.55987]\n",
            "Step 86931   [1.746 sec/step, loss=0.57978, avg_loss=0.56023]\n",
            "Step 86932   [1.743 sec/step, loss=0.52689, avg_loss=0.55996]\n",
            "Step 86933   [1.746 sec/step, loss=0.54504, avg_loss=0.55966]\n",
            "Step 86934   [1.763 sec/step, loss=0.51875, avg_loss=0.55903]\n",
            "Step 86935   [1.749 sec/step, loss=0.56394, avg_loss=0.55884]\n",
            "Step 86936   [1.712 sec/step, loss=0.61689, avg_loss=0.56019]\n",
            "Step 86937   [1.734 sec/step, loss=0.47605, avg_loss=0.55960]\n",
            "Step 86938   [1.735 sec/step, loss=0.61890, avg_loss=0.55993]\n",
            "Step 86939   [1.744 sec/step, loss=0.53645, avg_loss=0.55942]\n",
            "Step 86940   [1.749 sec/step, loss=0.59323, avg_loss=0.55977]\n",
            "Step 86941   [1.751 sec/step, loss=0.50572, avg_loss=0.55952]\n",
            "Step 86942   [1.748 sec/step, loss=0.59287, avg_loss=0.55959]\n",
            "Generated 16 batches of size 16 in 7.499 sec\n",
            "Step 86943   [1.757 sec/step, loss=0.59357, avg_loss=0.55963]\n",
            "Step 86944   [1.756 sec/step, loss=0.57304, avg_loss=0.55957]\n",
            "Step 86945   [1.751 sec/step, loss=0.61199, avg_loss=0.55959]\n",
            "Step 86946   [1.747 sec/step, loss=0.52223, avg_loss=0.55866]\n",
            "Step 86947   [1.735 sec/step, loss=0.57683, avg_loss=0.55862]\n",
            "Step 86948   [1.731 sec/step, loss=0.56924, avg_loss=0.55859]\n",
            "Step 86949   [1.729 sec/step, loss=0.56275, avg_loss=0.55886]\n",
            "Step 86950   [1.731 sec/step, loss=0.56803, avg_loss=0.55887]\n",
            "Step 86951   [1.691 sec/step, loss=0.61067, avg_loss=0.56134]\n",
            "Step 86952   [1.683 sec/step, loss=0.57057, avg_loss=0.56142]\n",
            "Step 86953   [1.686 sec/step, loss=0.54064, avg_loss=0.56135]\n",
            "Step 86954   [1.686 sec/step, loss=0.59172, avg_loss=0.56164]\n",
            "Step 86955   [1.683 sec/step, loss=0.58513, avg_loss=0.56183]\n",
            "Step 86956   [1.687 sec/step, loss=0.54727, avg_loss=0.56146]\n",
            "Step 86957   [1.685 sec/step, loss=0.59906, avg_loss=0.56210]\n",
            "Step 86958   [1.697 sec/step, loss=0.57626, avg_loss=0.56205]\n",
            "Generated 16 batches of size 16 in 6.917 sec\n",
            "Step 86959   [1.703 sec/step, loss=0.55519, avg_loss=0.56195]\n",
            "Step 86960   [1.712 sec/step, loss=0.50705, avg_loss=0.56095]\n",
            "Step 86961   [1.708 sec/step, loss=0.57762, avg_loss=0.56107]\n",
            "Step 86962   [1.737 sec/step, loss=0.45333, avg_loss=0.55985]\n",
            "Step 86963   [1.722 sec/step, loss=0.59974, avg_loss=0.56066]\n",
            "Step 86964   [1.707 sec/step, loss=0.58116, avg_loss=0.56103]\n",
            "Step 86965   [1.707 sec/step, loss=0.58665, avg_loss=0.56078]\n",
            "Step 86966   [1.681 sec/step, loss=0.51996, avg_loss=0.56183]\n",
            "Step 86967   [1.688 sec/step, loss=0.57402, avg_loss=0.56182]\n",
            "Step 86968   [1.686 sec/step, loss=0.58535, avg_loss=0.56185]\n",
            "Step 86969   [1.692 sec/step, loss=0.55064, avg_loss=0.56169]\n",
            "Step 86970   [1.732 sec/step, loss=0.46812, avg_loss=0.56046]\n",
            "Step 86971   [1.730 sec/step, loss=0.56420, avg_loss=0.56068]\n",
            "Step 86972   [1.740 sec/step, loss=0.57566, avg_loss=0.56047]\n",
            "Step 86973   [1.749 sec/step, loss=0.57378, avg_loss=0.56040]\n",
            "Step 86974   [1.741 sec/step, loss=0.50947, avg_loss=0.55961]\n",
            "Generated 16 batches of size 16 in 7.952 sec\n",
            "Step 86975   [1.750 sec/step, loss=0.56675, avg_loss=0.55922]\n",
            "Step 86976   [1.742 sec/step, loss=0.59435, avg_loss=0.55978]\n",
            "Step 86977   [1.695 sec/step, loss=0.59256, avg_loss=0.56172]\n",
            "Step 86978   [1.696 sec/step, loss=0.59050, avg_loss=0.56227]\n",
            "Step 86979   [1.680 sec/step, loss=0.57947, avg_loss=0.56255]\n",
            "Step 86980   [1.679 sec/step, loss=0.61202, avg_loss=0.56328]\n",
            "Step 86981   [1.686 sec/step, loss=0.51467, avg_loss=0.56240]\n",
            "Step 86982   [1.687 sec/step, loss=0.56258, avg_loss=0.56201]\n",
            "Step 86983   [1.681 sec/step, loss=0.58456, avg_loss=0.56247]\n",
            "Step 86984   [1.674 sec/step, loss=0.59539, avg_loss=0.56278]\n",
            "Step 86985   [1.676 sec/step, loss=0.55625, avg_loss=0.56246]\n",
            "Step 86986   [1.673 sec/step, loss=0.54578, avg_loss=0.56214]\n",
            "Step 86987   [1.720 sec/step, loss=0.41726, avg_loss=0.56029]\n",
            "Step 86988   [1.726 sec/step, loss=0.57610, avg_loss=0.56041]\n",
            "Generated 16 batches of size 16 in 8.113 sec\n",
            "Step 86989   [1.714 sec/step, loss=0.55294, avg_loss=0.56049]\n",
            "Step 86990   [1.709 sec/step, loss=0.55709, avg_loss=0.56028]\n",
            "Step 86991   [1.722 sec/step, loss=0.54775, avg_loss=0.55980]\n",
            "Step 86992   [1.717 sec/step, loss=0.58071, avg_loss=0.55990]\n",
            "Step 86993   [1.670 sec/step, loss=0.55576, avg_loss=0.56137]\n",
            "Step 86994   [1.669 sec/step, loss=0.55859, avg_loss=0.56143]\n",
            "Step 86995   [1.668 sec/step, loss=0.60583, avg_loss=0.56172]\n",
            "Step 86996   [1.674 sec/step, loss=0.61211, avg_loss=0.56224]\n",
            "Step 86997   [1.668 sec/step, loss=0.60595, avg_loss=0.56284]\n",
            "Step 86998   [1.674 sec/step, loss=0.58122, avg_loss=0.56276]\n",
            "Step 86999   [1.671 sec/step, loss=0.59416, avg_loss=0.56311]\n",
            "Step 87000   [1.684 sec/step, loss=0.54884, avg_loss=0.56304]\n",
            "Writing summary at step: 87000\n",
            "Saving checkpoint to: /content/drive/My Drive/stt플젝/logdir-tacotron2/kss+inna_2020-10-20_08-04-45/model.ckpt-87000\n",
            "Step 87001   [1.688 sec/step, loss=0.58525, avg_loss=0.56331]\n",
            "Step 87002   [1.735 sec/step, loss=0.49719, avg_loss=0.56233]\n",
            "Step 87003   [1.722 sec/step, loss=0.57609, avg_loss=0.56318]\n",
            "Generated 16 batches of size 16 in 9.543 sec\n",
            "Step 87004   [1.733 sec/step, loss=0.60320, avg_loss=0.56360]\n",
            "Step 87005   [1.744 sec/step, loss=0.52284, avg_loss=0.56300]\n",
            "Step 87006   [1.738 sec/step, loss=0.55230, avg_loss=0.56339]\n",
            "Step 87007   [1.734 sec/step, loss=0.60579, avg_loss=0.56358]\n",
            "Step 87008   [1.718 sec/step, loss=0.56432, avg_loss=0.56383]\n",
            "Step 87009   [1.709 sec/step, loss=0.57270, avg_loss=0.56382]\n",
            "Step 87010   [1.709 sec/step, loss=0.57468, avg_loss=0.56344]\n",
            "Step 87011   [1.703 sec/step, loss=0.58366, avg_loss=0.56379]\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntMQzsXFsljr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8a0f_F9zdAi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}